# 데이터과학 69p ~





## 통계

- 데이터셋 설명

```python
from collections import Counter
import matplotlib.pyplot as plt

num_friends = [100, 49, 41, 40, 25]

friend_counts = Counter(num_friends)
xs = range(101)
ys = [friend_counts[x] for x in xs]
plt.bar(xs, ys)
plt.axis([0, 101, 0, 25])  # y축의 경우 최대값에 걸쳐도 상관이 없으므로 이 형태가 정형적인 형태
plt.title("Histogram of Friends Counts")
plt.xlabel("# of friends")
plt.ylabel("# of people")
plt.show()
```



- 조건부 확률

```python
import enum, random

class Kid(enum.Enum):
    BOY = 0
    GIRL = 1


def random_kid() -> Kid:
    return random.choice([Kid.BOY, Kid.GIRL])


both_girls = 0
older_girl = 0
either_girl = 0

random.seed(0)

for _ in range(10000):
    younger = random_kid()
    older = random_kid()
    if older == Kid.GIRL:
        older_girl += 1
    elif older == Kid.GIRL and younger == Kid.GIRL:
        both_girls += 1
    elif older == Kid.GIRL or younger == Kid.GIRL:
        either_girl += 1

print("P(both | older):", both_girls / older_girl)
print("P(both | either):", both_girls / either_girl)
```





## 기계학습

- 분류 Classification 예측

```python
import matplotlib.pyplot as plt
from sklearn import datasets

digits = datasets.load_digits()

print(type(digits))

for label, img in zip(digits.target[:10], digits.images[:10]):  # target은 digits 안의 속성이기 때문에 그대로 작성
    plt.subplot(2, 5, label+1)  # 2행 5열
    plt.axis('off')
    plt.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')  # 옵션과 옵션값은 자체 작성 필요
    plt.title('Digit:[0]'.format(label))

plt.show()
```



```python
# decision알고리즘을 갖고 있는 모듈 tree
from sklearn import datasets, tree, metrics

digits = datasets.load_digits()

print(type(digits))


flag_3_8 = (digits.target == 3) + (digits.target == 8)
images = digits.images[flag_3_8]
labels = digits.target[flag_3_8]

images = images.reshape(images.shape[0], -1)

# 분류기
n_samples = len(flag_3_8[flag_3_8])
train_size = int(n_samples * 3 / 5)
classifier = tree.DecisionTreeClassifier()  # 분류기 생성
classifier.fit(images[:train_size], labels[:train_size])  # 학습

# 테스트
expected = labels[train_size:]  # 기대값
predicted = classifier.predict(images[train_size:])  # 예측값, 예측

# 출력
print("\nAccuracy:\n", metrics.accuracy_score(expected, predicted))  # 정확도
print("\nConfusion matrix:\n", metrics.confusion_matrix(expected, predicted))  # 정밀도
print("\nPrecision:\n", metrics.precision_score(expected, predicted, pos_label=3))  # 재현율
print("\nRecall:\n", metrics.recall_score(expected, predicted, pos_label=3))
print("\nF-measure:\n", metrics.f1_score(expected, predicted, pos_label=3))
```

```
<class 'sklearn.utils.Bunch'>

Accuracy:
 0.8811188811188811

Confusion matrix:
 [[60 15]
 [ 2 66]]

Precision:
 0.967741935483871

Recall:
 0.8

F-measure:
 0.8759124087591241
```



- 회귀





- 클러스터링 Clustering

```python
from sklearn import datasets, cluster
import matplotlib.pyplot as plt

iris = datasets.load_iris()
data = iris['data']

# K:클러스터 개수, Means:평균 (가까이 있는 데이터셋 끼리 클러스터링)
model = cluster.KMeans(n_clusters=3)
model.fit(data)

labels = model.labels_
ldata = data[labels == 0]  # 산점도
plt.scatter(ldata[: ,2], ldata[: ,3], c='black', alpha=0.3, s=100, marker='o') # 마커를 표시해주는 코드

ldata = data[labels == 1]
plt.scatter(ldata[: ,2], ldata[: ,3], c='black', alpha=0.3, s=100, marker='^')

ldata = data[labels == 2]
plt.scatter(ldata[: ,2], ldata[: ,3], c='black', alpha=0.3, s=100, marker='*')

plt.xlabel(iris['feature_names'][2])
plt.ylabel(iris['feature_names'][3])
plt.show()
```



```python
from sklearn import datasets, cluster, metrics
import matplotlib.pyplot as plt

iris = datasets.load_iris()
data = iris['data']

# K:클러스터 개수, Means:평균 (가까이 있는 데이터셋 끼리 클러스터링)
model = cluster.KMeans(n_clusters=3)
model.fit(data)

labels = model.labels_

# 그래프 그리기
MARKERS = ['v', '^', '+', 'x', 'd', 'p', 's', '1', '2']

def scatter_by_features(feat_idx1, feat_idx2):
    for lbl in range(labels.max() + 1):  # 마지막까지 반복
        clustered = data[labels == lbl]
        plt.scatter(clustered[: ,feat_idx1], clustered[: ,feat_idx2],
                    c='black', alpha=0.3, s=100,
                    marker=MARKERS[lbl], label='label {}'.format(lbl))
    plt.xlabel(iris['feature_names'][feat_idx1], fontsize='xx-large')
    plt.ylabel(iris['feature_names'][feat_idx2], fontsize='xx-large')

plt.figure(figsize=(16, 16))

# feature: sepal length, sepal width
plt.subplot(3, 2, 1)
scatter_by_features(0, 1)

# feature: sepal length, petal length
plt.subplot(3, 2, 2)
scatter_by_features(0, 2)

# feature: sepal length, petal width
plt.subplot(3, 2, 3)
scatter_by_features(0, 3)

# feature: sepal width, petal length
plt.subplot(3, 2, 4)
scatter_by_features(1, 2)

# feature: sepal width, petal width
plt.subplot(3, 2, 5)
scatter_by_features(1, 3)

# feature: petal length, petal width
plt.subplot(3, 2, 6)
scatter_by_features(2, 3)

plt.tight_layout()
plt.show()

print(metrics.confusion_matrix(iris['target'], model.labels_))
print(iris['target_names'])
```

```
[[ 0 50  0]
 [48  0  2]
 [14  0 36]]
['setosa' 'versicolor' 'virginica']
```

> 그래프로 출력