# 데이터 과학

| 시간 | 성인 여부 | 맥주 |
| ---- | --------- | ---- |
| 오전 | 성인      | x    |
| 오전 | 미성      | x    |
| 점심 | 성인      | o    |
| 점심 | 미성      | x    |
| 점심 | 미성      | x    |
| 저녁 | 성인      | o    |
| 저녁 | 성인      | o    |
| 저녁 | 성인      | o    |
| 저녁 | 성인      | x    |
| 저녁 | 미성      | x    |

- 결합확률:

P(주문)



- 저녁때 성인 맥주 주문 확률

```
P(주문|저녁, 성인) = P(저녁|주문)*P(성인|주문)*P(주문)
				= P(저녁 and 주문) / P(주문) * P(성인 and 주문) / P(주문)
				= 3/10 / 4/10 * 4/10 / 4/10 * 4/10
				= 0.3
```



- 저녁 때 성인 맥주 주문하지 않을 확률

```
P(주문안함|저녁, 성인) = P(저녁|주문안함)*P(성인|주문안함)*P(주문안함)
					= P(주문안함 and 저녁)/P(주문안함) * P(성인 and 주문안함)/P(주문안함) * P(주문안함)
					= 2/10 / 6/10 * 2/10 6/10 *6/10
					= 0.066
```

> P(주문|저녁, 성인) 확률이 더 크므로 저녁에 





## 나이브 베이즈 알고리즘 분류

1. 가우시안 나이브 베이즈 분류: iris 데이터셋 분류

>  특징들의 값이 정규 분포(가우시안 분포)되어 있다는 가정하에 조건부 확률을 계산, 연속적인 ㅌ윽징이 있는 데이터를 분류하는데 적합
>
> ex) 꽃받침 길이에 따른 iris 분류

```python
import matplotlib.pyplot as plt
# 시각화를 위해 pandas 임포트
import pandas as pd
# iris 데이터는 sklearn에서 직접 로드
from sklearn.datasets import load_iris
# sklearn의 train_test_split을 사용하면 라인 한 줄로 쉽게 데이터 나눌 수 있음
from sklearn.model_selection import train_test_split
# Gaussian Naive Bayes로 iris 데이터를 분류
from sklearn.naive_bayes import GaussianNB
# 분류 성능을 측정하기 위해 metrics와 accuracy_score 임포트
from sklearn import metrics
from sklearn.metrics import accuracy_score
# sklearn 모델의 동일한 결과 출력을 위해 선언
import numpy as np
np.random.seed(5)


# iris 데이터 시각화
# iris 데이터 불러옴
dataset = load_iris()
# pandas의 데이터프레임으로 데이터를 저장
df = pd.DataFrame(dataset.data, columns=dataset.feature_names)
# 분류값을 데이터 프레임에 저장
df['target'] = dataset.target
# 숫자인 분류값의 이해를 돕기 위해 문자로 변경
df.target = df.target.map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})
# 데이터 확인
print(df.head())
print('---------------------------')


# iris 데이터의 분포도 확인
# 아래 차트에서 볼 수 있듯, iris 데이터의 분포도가 정규분포(Gaussian Distribution)과 유사
# 분류값별로 데이터 프레임을 나누기
setosa_df = df[df.target == 'setosa']
versicolor_df = df[df.target == 'versicolor']
virginica_df = df[df.target == 'virginica']


# sepal length(cm)
ax = setosa_df['sepal length (cm)'].plot(kind='hist')
setosa_df['sepal length (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='setosa sepal length',
                                    figsize=(8, 4))
plt.show()

ax = versicolor_df['sepal length (cm)'].plot(kind='hist')
versicolor_df['sepal length (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='versicolor sepal length',
                                    figsize=(8, 4))
plt.show()

ax = virginica_df['sepal length (cm)'].plot(kind='hist')
virginica_df['sepal length (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='virginica sepal length',
                                    figsize=(8, 4))
plt.show()

# sepal width(cm)
ax = setosa_df['sepal width (cm)'].plot(kind='hist')
setosa_df['sepal width (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='setosa sepal width',
                                    figsize=(8, 4))
plt.show()

ax = versicolor_df['sepal width (cm)'].plot(kind='hist')
versicolor_df['sepal width (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='versicolor sepal width',
                                    figsize=(8, 4))
plt.show()

ax = virginica_df['sepal width (cm)'].plot(kind='hist')
virginica_df['sepal width (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='virginica sepal width',
                                    figsize=(8, 4))
plt.show()


# petal length(cm)
ax = setosa_df['petal length (cm)'].plot(kind='hist')
setosa_df['petal length (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='setosa petal length',
                                    figsize=(8, 4))
plt.show()

ax = versicolor_df['petal length (cm)'].plot(kind='hist')
versicolor_df['petal length (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='versicolor petal length',
                                    figsize=(8, 4))
plt.show()

ax = virginica_df['petal length (cm)'].plot(kind='hist')
virginica_df['petal length (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='virginica petal length',
                                    figsize=(8, 4))
plt.show()


# petal width(cm)
ax = setosa_df['petal width (cm)'].plot(kind='hist')
setosa_df['petal width (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='setosa petal width',
                                    figsize=(8, 4))
plt.show()

ax = versicolor_df['petal width (cm)'].plot(kind='hist')
versicolor_df['petal width (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='versicolor petal width',
                                    figsize=(8, 4))
plt.show()

ax = virginica_df['petal width (cm)'].plot(kind='hist')
virginica_df['petal width (cm)'].plot(kind='kde',
                                    ax=ax,
                                    secondary_y=True,
                                    title='virginica petal width',
                                    figsize=(8, 4))
plt.show()


# 데이터를 학습 데이터와 테스트 데이터로 나누기
# 20%를 테스트 데이터로 분류
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)


# Gaussian Naive Bayes 분류
# 학습데이터로 모델을 학습
model = GaussianNB()
model.fit(X_train, y_train)

# 테스트 데이터로 모델을 테스트
expected = y_test
predicted = model.predict(X_test)
print(metrics.classification_report(y_test, predicted))  # 혼동행렬에서의 값들을 보여줌
print('---------------------------')
print(accuracy_score(y_test, predicted))
print('---------------------------')


# Confusion Matrix
# 1개의 데이터만 제외하고 모든 데이터가 정확히 분류
print(metrics.confusion_matrix(expected, predicted))
print('---------------------------')
```

```
   sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target
0                5.1               3.5  ...               0.2  setosa
1                4.9               3.0  ...               0.2  setosa
2                4.7               3.2  ...               0.2  setosa
3                4.6               3.1  ...               0.2  setosa
4                5.0               3.6  ...               0.2  setosa

[5 rows x 5 columns]
---------------------------
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         8
           1       0.83      0.91      0.87        11
           2       0.90      0.82      0.86        11

    accuracy                           0.90        30
   macro avg       0.91      0.91      0.91        30
weighted avg       0.90      0.90      0.90        30

---------------------------
0.9
---------------------------
[[ 8  0  0]
 [ 0 10  1]
 [ 0  2  9]]
---------------------------

Process finished with exit code 0
```

> KNN과 비교했을 때 정확도가 같게 나옴



2. 다항 분포 나이브 베이즈 분류: 영화 감상평을 토대로 긍정/부정 평가 분류

> 데이터의 특징이 출현 횟수로 표현되었을 때 사용
>
> 주사위를 10번 던졌을 때 1이 한 번, 2가 두 번, 3이 세 번, 4가 네 번 나왔을 경우 10번 던진 결과 데이터를 (1, 2, 3, 4, 0, 0)으로 나타낼 수 있음
>
> 데이터 출현 횟수에 따라 값을 달리하는 데이터에는 이 모델을 사용

```python
import numpy as np
import pandas as pd
# 다항분포 나이브베이즈를 위한 라이브러리
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
# 모델의 정확도 평가를 위한 임포트
from sklearn.metrics import accuracy_score
# sklearn 모델의 동일한 결과 출력을 위해 선언
np.random.seed(5)


# 문제 정의
# 다항분포 나이브베이즈 분류 모델(MultinomialNB)를 사용하여 스팸 메일 분류


# 데이터 수집
# 영화리뷰 분류를 위해 영화에 대한 평가(긍정적/부정적
review_list = [
                {'movie_review': 'this is great great movie. I will watch again', 'type': 'positive'},
                {'movie_review': 'I like this movie', 'type': 'positive'},
                {'movie_review': 'amazing movie in this year', 'type': 'positive'},
                {'movie_review': 'cool my boyfriend also said the movie is cool', 'type': 'positive'},
                {'movie_review': 'awesome of the awesome movie ever', 'type': 'positive'},
                {'movie_review': 'shame I wasted money and time', 'type': 'negative'},
                {'movie_review': 'regret on this move. I will never never what movie from this director', 'type': 'negative'},
                {'movie_review': 'I do not like this movie', 'type': 'negative'},
                {'movie_review': 'I do not like actors in this movie', 'type': 'negative'},
                {'movie_review': 'boring boring sleeping movie', 'type': 'negative'}
             ]
df = pd.DataFrame(review_list)


# sklearn의 다항분포 나이브베이즈 모델은 숫자만을 다루므로 type에 해당하는 데이터를 숫자로
df['label'] = df['type'].map({'positive': 1, 'negative': 0})
print(df)
print('----------------------------------')


# 학습을 위해 학습에 사용 될 특징값과 분류값을 분리
df_x = df['movie_review']
df_y = df['label']


# 다항분포 나이브베이즈의 입력 데이터는 고정된 크기의 벡터로써 각각의 인덱스는 단어의 빈도수로 구분된 데이터여야 함
# sklearn의 CountVectorizer를 사용하여 쉽게 구현 가능
# CountVectorizer는 입력된 데이터(10개의 영화리뷰)에 출현된 모든 단어의 갯수만큼의 크기의 벡터를 만든 후 각각의 리뷰를 그 고정된 벡터로 표현
cv = CountVectorizer()
x_traincv = cv.fit_transform(df_x)
encoded_input = x_traincv.toarray()


# 아래의 행렬에서 볼 수 있 듯, 데이터에서 총 37개의 단어가 발견되어 각각의 영화 리뷰가 37개의 크기를 갖는 벡터로 표현
# 또한 다항분포 나이브베이즈에 사용하기 위해 단어의 빈도수 만큼의 수치로 각 단어의 인덱스에 수치 할당
# this is great movie. I will watch again:
# [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]
print(encoded_input)
print('----------------------------------')


# 벡터로 인코딩된 이메일 제목에 어떤 단어들이 포함되어 있는 지 알고싶을 경우 아래의 명령어로 확인 가능
print(cv.inverse_transform(encoded_input[0]))
print('----------------------------------')


# 벡터의 37개의 인덱스가 각각 무슨 단어를 의미하는 지 알고싶을 경우 아래의 명령어를 통해 확인 가능
print(cv.get_feature_names())
print('----------------------------------')


# 다항 분포 나이브베이즈 분류
# 다항분포 나이브베이즈로 영화 리뷰를 긍정적 평가인 지 부정적 평가인 지 분류
# MultinomialNB는 기본적으로 스무딩을 지원하므로 학습 데이터에 없는 단어가 테스트에 출현해도 분류를 이상없이 진행


# 기존의 데이터로 학습 진행
mnb = MultinomialNB()
y_train = df_y.astype(int)
mnb.fit(x_traincv, y_train)


# 테스트에 사용될 데이터 준비
test_feedback_list = [
                {'movie_review': 'great great great movie ever', 'type': 'positive'},
                {'movie_review': 'I like this amazing movie', 'type': 'positive'},
                {'movie_review': 'my boyfriend said great movie ever', 'type': 'positive'},
                {'movie_review': 'cool cool cool', 'type': 'positive'},
                {'movie_review': 'awesome boyfriend said cool movie ever', 'type': 'positive'},
                {'movie_review': 'shame shame shame', 'type': 'negative'},
                {'movie_review': 'awesome director shame movie boring movie', 'type': 'negative'},
                {'movie_review': 'do not like this movie', 'type': 'negative'},
                {'movie_review': 'I do not like this boring movie', 'type': 'negative'},
                {'movie_review': 'aweful terrible boring movie', 'type': 'negative'}
             ]
test_df = pd.DataFrame(test_feedback_list)
test_df['label'] = test_df['type'].map({'positive': 1, 'negative': 0})
test_x = test_df['movie_review']
test_y = test_df['label']


# 테스트
x_testcv = cv.transform(test_x)
predictions = mnb.predict(x_testcv)


# 정확도
print(accuracy_score(test_y, predictions))
print('----------------------')
```

```
                                        movie_review      type  label
0      this is great great movie. I will watch again  positive      1
1                                  I like this movie  positive      1
2                         amazing movie in this year  positive      1
3      cool my boyfriend also said the movie is cool  positive      1
4                  awesome of the awesome movie ever  positive      1
5                      shame I wasted money and time  negative      0
6  regret on this move. I will never never what m...  negative      0
7                           I do not like this movie  negative      0
8                 I do not like actors in this movie  negative      0
9                       boring boring sleeping movie  negative      0
----------------------------------
[[0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1
  0]
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
  0]
 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
  1]
 [0 0 1 0 0 0 0 1 2 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0
  0]
 [0 0 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0
  0]
 [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0
  0]
 [0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 2 0 0 1 1 0 0 0 0 2 0 0 0 1 1
  0]
 [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0
  0]
 [1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0
  0]
 [0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
  0]]
----------------------------------
[array(['again', 'great', 'is', 'movie', 'this', 'watch', 'will'],
      dtype='<U9')]
----------------------------------
['actors', 'again', 'also', 'amazing', 'and', 'awesome', 'boring', 'boyfriend', 'cool', 'director', 'do', 'ever', 'from', 'great', 'in', 'is', 'like', 'money', 'move', 'movie', 'my', 'never', 'not', 'of', 'on', 'regret', 'said', 'shame', 'sleeping', 'the', 'this', 'time', 'wasted', 'watch', 'what', 'will', 'year']
----------------------------------
1.0
----------------------

Process finished with exit code 0
```



3. 베르누이 나이브 베이즈 분류: 스팸 메일 분류

> 데이터가 0 또는 1로 표현되는 경우에 사용
>
> 주사위를 10번 던졌을 때 1이 한 번, 2가 두 번, 3이 세 번, 4가 네 번 나왔을 경우 10번 던진 결과 데이터를 (1, 1, 1, 1, 0, 0)으로 나타낼 수 있음, 숫자가 출현하면 1, 출현하지 않았을 때는 0으로 구분될 때 사용

```python
import matplotlib.pyplot as plt
# 시각화를 위해 pandas 임포트
import pandas as pd
# 베르누이 나이브 베이즈를 위한 라이브러리 임포트
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import BernoulliNB
# 모델의 정확도 평가를 위한 임포트
from sklearn.metrics import accuracy_score
# sklearn 모델의 동일한 결과 출력을 위한 선언
import numpy as np
np.random.seed(5)


# 문제 정의
# 베르누이 나이브베이즈 분류 모델을 사용하여 스팸 메일 분류


# 데이터 수집
# 이메일 타이틀과 스팸 여부가 있는 데이터 사용(email title, spam 두 가지)
email_list = [
                {'email title': 'free game only today', 'spam': True},
                {'email title': 'cheapest flight deal', 'spam': True},
                {'email title': 'limited time offer only today only today', 'spam': True},
                {'email title': 'today meeting schedule', 'spam': False},
                {'email title': 'your flight schedule attached', 'spam': False},
                {'email title': 'your credit card statement', 'spam': False}
             ]
df = pd.DataFrame(email_list)
print(df)
print('--------------------------')


# 데이터 다듬기(정제)
# sklearn의 베르누이 나이브베이즈 분류기(BernoulliNB)는 숫자만을 다루기 때문에 True와 False를 1과 0으로 치환
df['label'] = df['spam'].map({True:1, False:0})
print(df)
print('--------------------------')

# 학습에 사용될 데이터와 분류값을 나눔
df_x = df['email title']
df_y = df['label']


# 베르누이 나이브베이즈의 입력 데이터는 고정된 크기의 벡터로써 0과 1로 구분된 데이터여야만 함
# sklearn의 CountVertorizer를 사용하여 쉽게 구현 가능
# CountVectorizer는 입력된 데이터(6개의 이메일)에 출현된 모든 단어의 갯수만큼의 크기의 벡터를 만든 후, 각각의 이메일을 그 고정된 벡터로 표현
# bivary=True를 파라미터를 넘겨줌으로써 각각의 이메일 마다 단어가 한 번 이상 출현하면 1, 출현하지 않을 경우 0으로 표시
cv = CountVectorizer(binary=True)
x_traincv = cv.fit_transform(df_x)

# 아래 행렬에서 볼 수 있 듯, 데이터에서 총 17개의 단어가 발견되어 각각의 이멜이 17개의 크기를 갖는 벡터로 표현
# 또한 베르누이 나이브베이즈에 사용하기 위해 중복된 단어가 이메일의 제목에 있더라도 1로 표현되는 것을 확인 가능
# limied time offer only today only today: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0]
encoded_input = x_traincv.toarray()
print(encoded_input)
print('----------------------')


# 벡터로 인코딩된 이메일 제목에 어떤 단어들이 포함되어 있는 지 알고 싶을 경우
cv.inverse_transform(encoded_input[0])

# 벡터의 17개 인덱스가 각각 무슨 단어를 의미하는 지 알고 싶을 경우
cv.get_feature_names()


# 베르누이 나이브베이즈로 스팸 메일을 분류
# BernoulliNB는 기본적으로 스무딩을 지원하므로 학습데이터에 없는 단어가 테스트에 칠현해도 분류를 이상없이 진행
# 학습 데이터로 베르누이 분류기 학습
bnb = BernoulliNB()
y_train = df_y.astype('int')  # 정수형 데이터로 변환
bnb.fit(x_traincv, y_train)


# 테스트 데이터 다듬기
test_email_list = [
                {'email title': 'free flight offer', 'spam': True},
                {'email title': 'hey traveler free flight deal', 'spam': True},
                {'email title': 'limited free game offer', 'spam': True},
                {'email title': 'today flight schedule', 'spam': False},
                {'email title': 'your credit card attached', 'spam': False},
                {'email title': 'free credit card offer only today', 'spam': False}
             ]

test_df =  pd.DataFrame(test_email_list)
test_df['label'] = test_df['spam'].map({True:1, False:0})
test_x = test_df['email title']
test_y = test_df['label']


# 테스트
x_testcv = cv.transform(test_x)
predictions = bnb.predict(x_traincv)


# 정확도
print(accuracy_score(test_y, predictions))
print('----------------------')
```

```
                                email title   spam
0                      free game only today   True
1                      cheapest flight deal   True
2  limited time offer only today only today   True
3                    today meeting schedule  False
4             your flight schedule attached  False
5                your credit card statement  False
--------------------------
                                email title   spam  label
0                      free game only today   True      1
1                      cheapest flight deal   True      1
2  limited time offer only today only today   True      1
3                    today meeting schedule  False      0
4             your flight schedule attached  False      0
5                your credit card statement  False      0
--------------------------
[[0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0]
 [0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0]
 [0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0]
 [1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1]
 [0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1]]
----------------------
1.0
----------------------

Process finished with exit code 0
```

> 다항분포와 비슷함





## 선형 회귀 (Regression Analysis)

1. 회귀란 일반적으로 데이터들을 2차우너 공간에 찍은 후에 이들 데이터들을 가장 잘 설명하는 직선이나 곡선을 찾는 문제라고 할 수 있음
2. y=f(x)에서 출력 y가 실수이고 입력 x도 실수일 때 함수 f(x)를 예측하는 것이 회귀

> 회귀의 종류: 선형, 비선형



- 선형 회귀 소개

1. 직선의 방정식: f(x) = mx+b
2. 선형 회귀는 입력 데이터를 잘 설명하는 기울기와 절편값을 찾는 문제
3. 선형 회귀의 기본식 : f(x) = Wx + b
   - 기울기 -> 가중치
   - 절편 -> 바이어스(편향값)



- 선형 회귀 예제

| 키   | 몸무게 |
| ---- | ------ |
| 174  | 71     |
| 152  | 55     |
| 138  | 46     |
| 128  | 38     |
| 186  | 88     |

```
키(x)=165이면 몸무게는? ->
선형 회귀 모델 ->
몸무게(y)=63으로 예측
```





## K-means 클러스터링

1. 비지도 학습 중에서 가장 대표적인 것
2. K-means 알고리즘은 주어진 n개의 관측값을 k개의 클러스터로 분할하는 알고리즘으로, 관측값들은 거리가 최소인 클러스터로 분류됨



- k-means 알고리즘

```
입력값
1. k: 클러스터 수
2. D: n개의 데이터
3. 출력값: k개의 클러스터
```

```
알고리즘
1. 집합 D에서 k개의 데이터를 임의로 추출하고 이 데이터들을 각 클러스터의 중심(centroid)으로 설정(초기값 설정)
2. 집합 D의 각 데이터에 대해 k개의 클러스터 중심과의 거리를 꼐산하고 각 데이터가 어느 중심점과 가장 유사도가 높은 지 알아냄, 그리고 그렇게 찾아낸 중심점으로 각 데이터들을 할당
3. 클러서터의 중심점을 다시 계산, 즉 2에서 재할당된 클러스터들을 기준으로 중심점을 다시 계산
4. 각 데이터의 소속 클러스터가 바뀌지 않을 때 까지 과정 2, 3을 반복
```

```python
import  pandas as pd
import numpy as np
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
# get_ipython().run_line_magic('matplotlib', 'inline')
# sklearn 모델의 동일한 결과 출력을 위해 선언
np.random.seed(5)


# 데이터 만들기
# 사람들의 키와 몸무게 데이터
df = pd.DataFrame(columns=['height', 'weight'])
df.loc[0] = [185, 60]
df.loc[1] = [180, 60]
df.loc[2] = [185, 70]
df.loc[3] = [165, 63]
df.loc[4] = [155, 68]
df.loc[5] = [170, 75]
df.loc[6] = [175, 80]
print(df.head(7))
print('-------------------')


# 데이터 시각화
# 데이터를 차트에 시각화
sns.lmplot('height', 'weight',
           data=df, fit_reg=False,
           scatter_kws={'s': 200})
plt.show()


# K 평균 군집화
# sklearn의 kmeans 라이브러리에 데이터를 활용하여 데이터를 군집화
data_points = df.values
kmeans = KMeans(n_clusters=3).fit(data_points)


# 해당 명령어로 각 군집의 중심 위치 확인 가능
print(kmeans.cluster_centers_)
print('-------------------')


# 데이터가 어느 군집에 소속되어 있는 지 데이터프레임 cluster_id 행에 저장
df['cluster_id'] = kmeans.labels_


# 데이터 프레임을 조회하여 데이터별 군집 확인 가능
print(df.head(12))
print('-------------------')


# K 평균 군집 시각화
# 군집 결과를 seaborn과 dataframe을 활용하여 손쉽게 시각화 가능
# 키가 작은 그룹, 중간 그룹, 큰 그룹으로 군집된 결과를 각기 다른 색으로 구분된 군집으로 확인 가능
sns.lmplot('height', 'weight', data=df, fit_reg=False,
           scatter_kws={'s': 150}, hue='cluster_id')
plt.show()
```

```
  height weight
0    185     60
1    180     60
2    185     70
3    165     63
4    155     68
5    170     75
6    175     80
-------------------
[[160.          65.5       ]
 [183.33333333  63.33333333]
 [172.5         77.5       ]]
-------------------
  height weight  cluster_id
0    185     60           1
1    180     60           1
2    185     70           1
3    165     63           0
4    155     68           0
5    170     75           2
6    175     80           2
-------------------

Process finished with exit code 0
```

