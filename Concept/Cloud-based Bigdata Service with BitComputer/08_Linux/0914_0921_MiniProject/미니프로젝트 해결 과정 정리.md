# 미니프로젝트 해결 과정 정리

1. raw 데이터 생성

2. hdfs(하둡)에 raw 데이터 저장
3. hive sql에 raw 데이터에 맞는 테이블과 컬럼 생성
4. hdfs에 저장된 raw 데이터를 hive sql로 옮김
5. 윈도우 환경에서 파이썬의  pyhive 모듈로 hive sql 접근
6. 접근된 hive sql의 저장된 raw 데이터를 파이썬 코드를 짜서 데이터 가공
7. 가공된 데이터를 hive sql에 저장
8. MySQL에 테이블 및 컬럼 생성
9. hive sql에 저장된 가공 데이터(테이블)를 sqoop을 사용하여 MySQL에 옮김
10. 윈도우 환경에서 파이썬으로 MySQL 접근
11.  파이썬의 pandas와 matplotlib으로 결과 도출





## 해설 및 코드

> https://github.com/jeonsanggi/TIL/blob/master/Hadoop/13.PyHive_%EC%84%A4%EC%B9%98.md
>
> 전체적으로 전상기님의 깃허브 참조하는 것이 좋음





- raw 데이터 생성(윈도우)

https://github.com/mitchelljy/Trending-YouTube-Scraper 

>  참고 및 파이썬 유튜브  API 코드 활용

> raw 데이터 생성 코드에서 decription 부분 삭제

> raw 데이터에서 헤더 부분 삭제





- hdfs(하둡)에 raw 데이터 저장(리눅스)

```
[bit44@hadoopnode01 ~/hadoop]$ sbin/start-all.sh
.
.
(하둡 실행 코드로 프로세스들 전부 실행)
.
.
bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -put /mnt/share/download/output/*.csv youtube_csv
```

> ```
> [bit44@hadoopnode01 ~/hadoop]$ sbin/stop-all.sh
> [bit44@hadoopnode01 ~/hadoop]$ sbin/mr-jobhistory-daemon.sh stop historyserver
> 
> ```
>
> 하둡 끄는 방법





- hive sql에 raw 데이터에 맞는 테이블과 컬럼 생성

```
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ bin/hive

```

```
hive> CREATE TABLE 200917_am(video_id STRING, title STRING, publishedAt STRING, channelId STRING, channelTitle STRING, categoryId INT, trending_date STRING, tags STRING, view_count INT, likes INT, dislikes INT, comment_count INT, thumbnail_link STRING, comments_disabled STRING, ratings_disabled STRING)

hive> ROW FORMAT DELIMITED
hive> FIELDS TERMINATED BY ','
hive> LINES TERMINATED BY '\n';

```





- hdfs에 저장된 raw 데이터를 hive sql로 옮김(리눅스)

```
bin/hdfs dfs -put /mnt/share/download/output/*.csv hive-data

```

```
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ bin/hive

```

```
hive> load data inpath 'hive-data/20.16.09_KR_videos_PM.csv' overwrite into table 200916_pm;

```



- 윈도우 환경에서 파이썬의  pyhive 모듈로 hive sql 접근(리눅스/윈도우)

https://github.com/jeonsanggi/TIL/blob/master/Hadoop/13.PyHive_%EC%84%A4%EC%B9%98.md

> pyhive 패키지 설치 참조

> 인터프리터가 파이썬 3.8 이상인 경우 sasl 패키지가 설치 안됨
>
> 따라서 파이썬 3.7.9 버전 설치 후 프로젝트 인터프리터를 해당 버전으로 바꾸고 sasl 설치 실행



```
[bit44@hadoopnode01 ~]$ cd ~/apache-hive-2.3.7-bin/
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ bin/hiveserver2
.
.
which: no hbase in (/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/lib/j                  vm/java-1.8.0-openjdk-1.8.0.262.b10-0.el7_8.x86_64/bin:/home/bit44/.local/bin:/h                  ome/bit44/bin)
2020-09-17 13:50:17: Starting HiveServer2
.
.

```

> 하이브 서버 실행



```python
from pyhive import hive
conn = hive.Connection(host="127.0.0.1", port=10000, username='bit44', password='1234', auth='CUSTOM')

print(conn)
```

> 윈도우에서의 파이썬 코드
>
> 해당 코드 실행 후 오류 발생하면 연결 안 되는 것





- 접근된 hive sql의 저장된 raw 데이터를 파이썬 코드를 짜서 데이터 가공(윈도우)

```python
from pyhive import hive
# 하이브 연결
conn = hive.Connection(host="127.0.0.1", port=10000, username='bit44', password='1234', auth='CUSTOM')
print(conn)


# hive sql의 raw 데이터 가공
import pandas as pd
df = pd.read_sql("SELECT * FROM 200915_pm", conn)
# print(df)


# 사용하지 않는 컬럼 드랍
df.drop(['200915_pm.thumbnail_link', '200915_pm.comments_disabled', '200915_pm.ratings_disabled'], axis='columns', inplace=True)
# print(df)


# 데이터 확인을 위한 출력 컬럼 수 확장
pd.set_option('display.max_columns', 100)


# 컬럼명 변경
df = df.rename({'200915_pm.categoryid':'200915_pm.category'}, axis='columns')
# print(df)


# 숫자로 되어있던 카테고리id를 실제 카테고리명으로 교체
def function(row):
    if row['200915_pm.category'] == 2:
        return 'Autos & Vehicles'
    elif row['200915_pm.category'] == 1:
        return 'Film & Animation'
    elif row['200915_pm.category'] == 10:
        return 'Music'
    elif row['200915_pm.category'] == 15:
        return 'Pets & Animals'
    elif row['200915_pm.category'] == 17:
        return 'Sports'
    elif row['200915_pm.category'] == 18:
        return 'Short Movies'
    elif row['200915_pm.category'] == 19:
        return 'Travel & Events'
    elif row['200915_pm.category'] == 20:
        return 'Gaming'
    elif row['200915_pm.category'] == 21:
        return 'Videoblogging'
    elif row['200915_pm.category'] == 22:
        return 'People & Blogs'
    elif row['200915_pm.category'] == 23:
        return 'Comedy'
    elif row['200915_pm.category'] == 24:
        return 'Entertainment'
    elif row['200915_pm.category'] == 25:
        return 'News & Politics'
    elif row['200915_pm.category'] == 26:
        return 'Howto & Style'
    elif row['200915_pm.category'] == 27:
        return 'Education'
    elif row['200915_pm.category'] == 28:
        return 'Science & Technology'
    elif row['200915_pm.category'] == 29:
        return 'Nonprofits & Activism'
    elif row['200915_pm.category'] == 30:
        return 'Movies'
    elif row['200915_pm.category'] == 31:
        return 'Anime/Animation'
    elif row['200915_pm.category'] == 32:
        return 'Action/Adventure'
    elif row['200915_pm.category'] == 33:
        return 'Classics'
    elif row['200915_pm.category'] == 34:
        return 'Comedy'
    elif row['200915_pm.category'] == 35:
        return 'Documentary'
    elif row['200915_pm.category'] == 36:
        return 'Drama'
    elif row['200915_pm.category'] == 37:
        return 'Family'
    elif row['200915_pm.category'] == 38:
        return 'Foreign'
    elif row['200915_pm.category'] == 39:
        return 'Horror'
    elif row['200915_pm.category'] == 40:
        return 'Sci-Fi/Fantasy'
    elif row['200915_pm.category'] == 41:
        return 'Thriller'
    elif row['200915_pm.category'] == 42:
        return 'Shorts'
    elif row['200915_pm.category'] == 43:
        return 'Shows'
    elif row['200915_pm.category'] == 44:
        return 'Trailers'
    else:
        return 'etc'

df['200915_pm.category'] = df.apply(function, axis = 1)
# print(df)


# 임시로 저장(buffer) 되어있던 가공된 데이터 프레임을 복사 및 컬럼명 간결화
df_cutcol = df.copy()


# 컬럼명 간결화
new_columns = [col_name.split(".")[1] for col_name in df_cutcol.columns]
# print(new_columns)


# 간결화 확인
new_column_dict = {before_col : new_col for before_col, new_col in zip(df_cutcol.columns, new_columns)}
# print(new_column_dict)


# 간결화 된 컬럼명을 가공된 데이터 프레임에 적용
df_cutcol.rename(columns=new_column_dict, inplace=True)
print(df_cutcol)


# hive sql에 데이터 프레임을 저장
from sqlalchemy import create_engine
from sqlalchemy.types import Float

engine = create_engine('hive://bit44:1234@127.0.0.1:10000/default', connect_args={'auth':'CUSTOM'}, echo=True)

df_cutcol.to_sql("200915_pm_hive", con=engine, if_exists="replace", index=None, method='multi', chunksize=5000)

```





- 가공된 데이터를 hive sql에 저장(리눅스/윈도우)

```
위 파이썬 코드의 "# hive sql에 데이터 프레임을 저장" 참조,
각 날짜&시각 마다 해당 코드 부분만 바꿔서 실행
```

```
hive> show tables;
OK
tab_name
200915_am
200915_am_hive
200915_pm
200915_pm_hive
200916_am
200916_am_hive
200916_pm
200916_pm_hive
200917_am
200917_am_hive

```

> hive sql에서 새로 생성된 테이블 확인 가능





- MySQL에 테이블 및 컬럼 생성(리눅스)

```
[bit44@hadoopnode01 ~]$ mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 48
Server version: 8.0.21 MySQL Community Server - GPL

Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> create database youtube;
Query OK, 1 row affected (0.01 sec)

mysql> grant all privileges on youtube.* to hiveuser@'%';
Query OK, 0 rows affected (0.00 sec)

mysql> flush privileges;
Query OK, 0 rows affected (0.00 sec)

mysql> quit

```

> youtube 데이터베이스 생성
>
> 데이터베이스는 테이블 보다 상위 개념



```
[bit44@hadoopnode01 ~]$ mysql -u hiveuser -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 49
Server version: 8.0.21 MySQL Community Server - GPL

Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql>
mysql>
mysql> use youtube;
Database changed

```

> hiveuser의 MySQL 기존 데이터베이스에서 youtube 데이터 베이스로 전환



```
mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| hive_db            |
| information_schema |
| tpch_db            |
| youtube            |
+--------------------+
4 rows in set (0.00 sec)

```

> hiveuser의 데이터베이스들 확인 가능(youtube 데이터베이스 존재 확인)



```
mysql> create table 200915_pm(video_id VARCHAR(20), title VARCHAR(50), publishedAt DATE, channelI                               d VARCHAR(20), channeltitle VARCHAR(30), category VARCHAR(50), trending_date DATE, tags VARCHAR(2                               00), view_count INT(30), likes INT(20), dislikes INT(20), comment_count INT(20));

```

> 테이블 및 컬럼 생성



```
mysql> create table 200915_pm(video_id TEXT, title TEXT, publishedAt DATETIME, channelId TEXT, channeltitle TEXT, category TEXT, trending_date DATE, tags TEXT, view_count INT, likes INT, dislikes INT, comment_count INT);
```

> 프라이머리키 컬럼을 만드는 지 안만드는 지는 잘 모름



```
mysql> show tables;
+-------------------+
| Tables_in_youtube |
+-------------------+
| 200915_am         |
| 200915_pm         |
| 200916_am         |
| 200916_pm         |
| 200917_am         |
+-------------------+
5 rows in set (0.00 sec)

```

> hiveuser의 데이터베이스들 중 youtube 데이터베이스로 전환했으므로 테이블 보기를 하면 youtube 데이터베이스의 테이블만 나옴





- hive sql에 저장된 가공 데이터(테이블)를 sqoop을 사용하여 MySQL에 옮김(리눅스)

```
[bit44@hadoopnode01 ~]$ cd ~/sqoop-1.4.7.bin__hadoop-2.6.0/
[bit44@hadoopnode01 ~/sqoop-1.4.7.bin__hadoop-2.6.0]$
[bit44@hadoopnode01 ~/sqoop-1.4.7.bin__hadoop-2.6.0]$
[bit44@hadoopnode01 ~/sqoop-1.4.7.bin__hadoop-2.6.0]$
[bit44@hadoopnode01 ~/sqoop-1.4.7.bin__hadoop-2.6.0]$
[bit44@hadoopnode01 ~/sqoop-1.4.7.bin__hadoop-2.6.0]$ vi scripts/youtube_export.sh

```

> sqoop의 실행 코드가 길기 때문에 youtube_export.sh를 만들어 실행 코드 간단히 하기 위함



```
--username
hiveuser
--password
1234
--connect
jdbc:mysql://localhost:3306/youtube
--table
200915_pm
--columns
video_id, title, publishedAt, channelId, channelTitle, category, trending_date, tags, view_count, likes, dislikes, comment_count
--input-fields-terminated-by
,
--export-dir
/user/hive/warehouse
-m
1

```

> youtube_export.sh 내용

> --export-dir에 hive sql의 warehouse 경로가 들어가야 함
>
> hivesql의 warehouse는 테이블 생성 시 자동적으로 테이블 내용을 해당 경로로 저장



```
[bit44@hadoopnode01 ~/sqoop-1.4.7.bin__hadoop-2.6.0]$ bin/sqoop export --options-file scripts/youtube_export.sh

```

> sqoop으로 hive sql에 있던 데이터를 MySQL로 복사 및 전환

> 해당 부분부터 실행되지 않음



- 윈도우 환경에서 파이썬으로 MySQL 접근(윈도우)





-  파이썬의 pandas와 matplotlib으로 결과 도출(윈도우)

