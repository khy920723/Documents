## 빅데이터 실습

- 예제를 위한 데이터셋

http://stat-computing.org/dataexpo/2009/the-data.html

https://packages.revolutionanalytics.com/datasets/

AirOnTimeCSV.zip 다운로드



- 예제 해설 사이트

https://docs.microsoft.com/en-us/machine-learning-server/r-reference/revoscaler/airontime87to12



- 준비

airOT201201~12까지 D:\KHY_OracleVBox\VBoxShare\download에 복사

헤더(칼럼: DEPT.. 등의 워드들) 삭제

> 노트패드++를 사용하면 훨씬 빠르게 읽기/쓰기 가능



- 

```
login as: bit44
bit44@127.0.0.1's password:
Last login: Fri Aug 21 09:16:31 2020
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$ cd hadoop
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ sbin/start-dfs.sh
Starting namenodes on [hadoopnode01]
hadoopnode01: starting namenode, logging to /home/bit44/hadoop-2.7.2/logs/hadoop-bit44-namenode-hadoopnode01.out
hadoopnode01: starting datanode, logging to /home/bit44/hadoop-2.7.2/logs/hadoop-bit44-datanode-hadoopnode01.out
Starting secondary namenodes [hadoopnode01]
hadoopnode01: starting secondarynamenode, logging to /home/bit44/hadoop-2.7.2/logs/hadoop-bit44-secondarynamenode-hadoopnode01.out
[bit44@hadoopnode01 ~/hadoop]$ sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /home/bit44/hadoop-2.7.2/logs/yarn-bit44-resourcemanager-hadoopnode01.out
hadoopnode01: starting nodemanager, logging to /home/bit44/hadoop-2.7.2/logs/yarn-bit44-nodemanager-hadoopnode01.out
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ sbin/yarn-daemon.sh start proxyserver
starting proxyserver, logging to /home/bit44/hadoop-2.7.2/logs/yarn-bit44-proxyserver-hadoopnode01.out
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ jps
4497 ResourceManager
4962 WebAppProxyServer
3971 NameNode
4613 NodeManager
4105 DataNode
4314 SecondaryNameNode
5006 Jps

```

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -mkdir airline_dep_delay_input
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -put /mnt/share/download/*csv airline_dep_delay_input
.
.
.
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls airline_dep_delay_input

```

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -rm airline_dep_delay_input/ill*.csv

```

> 기존에 있던 일러스트~.csv 때문에 지워줌

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls airline_dep_delay_input
Found 12 items
-rw-r--r--   1 bit44 supergroup  108490517 2020-08-21 10:23 airline_dep_delay_input/airOT201201.csv
-rw-r--r--   1 bit44 supergroup  103649811 2020-08-21 10:23 airline_dep_delay_input/airOT201202.csv
-rw-r--r--   1 bit44 supergroup  116714828 2020-08-21 10:23 airline_dep_delay_input/airOT201203.csv
-rw-r--r--   1 bit44 supergroup  112620850 2020-08-21 10:23 airline_dep_delay_input/airOT201204.csv
-rw-r--r--   1 bit44 supergroup  115919743 2020-08-21 10:23 airline_dep_delay_input/airOT201205.csv
-rw-r--r--   1 bit44 supergroup  117982135 2020-08-21 10:24 airline_dep_delay_input/airOT201206.csv
-rw-r--r--   1 bit44 supergroup  122492345 2020-08-21 10:24 airline_dep_delay_input/airOT201207.csv
-rw-r--r--   1 bit44 supergroup  121168354 2020-08-21 10:24 airline_dep_delay_input/airOT201208.csv
-rw-r--r--   1 bit44 supergroup  109647621 2020-08-21 10:24 airline_dep_delay_input/airOT201209.csv
-rw-r--r--   1 bit44 supergroup  115129089 2020-08-21 10:24 airline_dep_delay_input/airOT201210.csv
-rw-r--r--   1 bit44 supergroup  109297700 2020-08-21 10:24 airline_dep_delay_input/airOT201211.csv
-rw-r--r--   1 bit44 supergroup  111451449 2020-08-21 10:24 airline_dep_delay_input/airOT201212.csv

```



- 이클립스(자바)

> 자바를 쓰는 이유? 파이썬의 pandas의 경우 너무 큰 파일의 경우 열리지가 않기 때문에 자바로 먼저 데이터를 가공 후 pandas로 씀

20일 자바 부분 확인

> 파일 - 뉴 - 아더 - Maven - 메이븐 프로젝트 - 넥스트 - create simple project 체크 - 넥스트 - 그룹 아이디: com.adacho - 아티팩트아이디(프로젝트이름): com.adacho - 버전: 0.0.1 - 피니쉬
>
> 폼.xml 수정
>
> ```
>   <build>
>   	<plugins>
>   		<plugin>
>   		<groupId>org.apache.maven.plugins</groupId>
>   		<artifactId>maven-compiler-plugin</artifactId>
>   		<version>3.6.1</version>
>   		<configuration>
>   			<source>1.8</source>
>   			<target>1.8</target>
>   		</configuration>
>   		</plugin>
>   	</plugins>
>   </build>
>   <dependencies>
>   <!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common -->
> 	<dependency>
> 	    <groupId>org.apache.hadoop</groupId>
> 	    <artifactId>hadoop-common</artifactId>
> 	    <version>2.7.2</version>
> 	</dependency>
> 	<!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core -->
> 	<dependency>
> 	    <groupId>org.apache.hadoop</groupId>
> 	    <artifactId>hadoop-mapreduce-client-core</artifactId>
> 	    <version>2.7.2</version>
> 	</dependency>
>   </dependencies>
> ```
>
> 프로젝트 파일 우측 클릭 -> 메이븐 -> 업데이트 프로젝트-> 오케이

프로젝트 이름 airline

패키지1 com.adacho.hadoop.common

패키지2 com.adacho.hadoop.driver

패키지3 com.adacho.hadoop.mapper

패키지4 com.adacho.hadoop.reducer

```java
package com.adacho.hadoop.common;

import org.apache.hadoop.io.Text;

public class AirlinePerformanceParser {
	
	private int year;
	private int month;
	private int day;
	
	private int arriveDelayTime = 0;
	private int departureDelayTime = 0;
	private int distance = 0;
	
	private boolean arriveDelayAvailable = true;
	private boolean departureDelayAvailable = true;
	private boolean distanceAvailable = true;
	
	private String uniqueCarrier;
	
	
	public AirlinePerformanceParser(Text text) {
		try {
			String[] columns = text.toString().split(",");
			
			year = Integer.parseInt(columns[0]);
			month = Integer.parseInt(columns[1]);
			day = Integer.parseInt(columns[2]);
			uniqueCarrier = columns[5];
			
			if(!columns[17].equals("")) {
				departureDelayTime = (int)Float.parseFloat(columns[17]);
			} else {
				departureDelayAvailable = false;
			}
			
			if(!columns[27].equals("")) {
				arriveDelayTime = (int)Float.parseFloat(columns[27]);
			} else {
				arriveDelayAvailable = false;
			}
			
			if(!columns[37].equals("")) {
				distance = (int)Float.parseFloat(columns[37]);
			} else {
				distanceAvailable = false;
			}
			
		} catch (Exception e) {
			// TODO: handle exception
			System.out.println("Error parsing a record : " + e.getMessage());
		}
	}


	/**
	 * @return the year
	 */
	public int getYear() {
		return year;
	}


	/**
	 * @return the month
	 */
	public int getMonth() {
		return month;
	}


	/**
	 * @return the arriveDelayTime
	 */
	public int getArriveDelayTime() {
		return arriveDelayTime;
	}


	/**
	 * @return the departureDelayTime
	 */
	public int getDepartureDelayTime() {
		return departureDelayTime;
	}


	/**
	 * @return the distance
	 */
	public int getDistance() {
		return distance;
	}


	/**
	 * @return the arriveDelayAvailable
	 */
	public boolean isArriveDelayAvailable() {
		return arriveDelayAvailable;
	}


	/**
	 * @return the departureDelayAvailable
	 */
	public boolean isDepartureDelayAvailable() {
		return departureDelayAvailable;
	}


	/**
	 * @return the distanceAvailable
	 */
	public boolean isDistanceAvailable() {
		return distanceAvailable;
	}


	/**
	 * @return the uniqueCarrier
	 */
	public String getUniqueCarrier() {
		return uniqueCarrier;
	}
	
}

```

```java
package com.adacho.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.adacho.hadoop.common.AirlinePerformanceParser;

public class DepartureDelayCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
	
	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputKey = new Text();
	
	
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.map(key, value, context);
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		
		outputKey.set(parser.getYear() + "," + parser.getMonth());
		
		if(parser.getDepartureDelayTime() > 0) {
			context.write(outputKey, outputValue);
		}
		
	}
		
}

```

```java
package com.adacho.hadoop.reducer;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class DelayCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
	
	private IntWritable result = new IntWritable();

	@Override
	protected void reduce(Text key, Iterable<IntWritable> values,
			Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.reduce(arg0, arg1, arg2);
		int sum = 0;
		
		for(IntWritable value : values) {
			sum += value.get();
		}
		result.set(sum);
		context.write(key, result);
	}
	
}

```

```java
package com.adacho.hadoop.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import com.adacho.hadoop.mapper.DepartureDelayCountMapper;
import com.adacho.hadoop.reducer.DelayCountReducer;

public class DepartureDelayCount {
	public static void main(String[] args) throws Exception{
	
		Configuration conf = new Configuration();
		
		if(args.length != 2) {
			System.out.println("Usage: DepartureDelayCount <input> <output>");
			System.exit(2);
		}
		
		
		Job job = Job.getInstance(conf, "DepartureDelayCount");
		
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		job.setJarByClass(DepartureDelayCount.class);
		job.setMapperClass(DepartureDelayCountMapper.class);
		job.setReducerClass(DelayCountReducer.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		job.waitForCompletion(true);
	}
}

```



- 메이븐 실행

프로젝트파일 우클릭 -> 런애즈 -> 메이븐 빌드 ... -> 골: clean install -> 런 -> BUILD SUCCESS 확인



- 하둡

D:\KHY\airline\target 확인

> 프로젝트파일 우클릭 - 프로퍼티 - 리소스 - 경로의 .jar를 share의 java 폴더에 복붙

D:\KHY_OracleVBox\VBoxShare\download에 txt 파일 넣기

```
[bit44@hadoopnode01 ~/hadoop]$ bin/yarn jar /mnt/share/java/airline-0.0.1.jar com.adacho.hadoop.driver.DepartureDelayCount airline_dep_delay_input airline_dep_delay_output



20/08/21 13:27:45 INFO client.RMProxy: Connecting to ResourceManager at hadoopnode01/10.0.2.15:8032
20/08/21 13:27:46 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
20/08/21 13:27:46 INFO input.FileInputFormat: Total input paths to process : 12
20/08/21 13:27:47 INFO mapreduce.JobSubmitter: number of splits:12
20/08/21 13:27:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1597972299825_0001
20/08/21 13:27:47 INFO impl.YarnClientImpl: Submitted application application_1597972299825_0001
20/08/21 13:27:48 INFO mapreduce.Job: The url to track the job: http://0.0.0.0:8090/proxy/application_1597972299825_0001/
20/08/21 13:27:48 INFO mapreduce.Job: Running job: job_1597972299825_0001
20/08/21 13:28:00 INFO mapreduce.Job: Job job_1597972299825_0001 running in uber mode : false
20/08/21 13:28:00 INFO mapreduce.Job:  map 0% reduce 0%
20/08/21 13:28:38 INFO mapreduce.Job:  map 4% reduce 0%
20/08/21 13:28:41 INFO mapreduce.Job:  map 6% reduce 0%
20/08/21 13:28:42 INFO mapreduce.Job:  map 8% reduce 0%
20/08/21 13:28:44 INFO mapreduce.Job:  map 9% reduce 0%
20/08/21 13:28:45 INFO mapreduce.Job:  map 10% reduce 0%
20/08/21 13:28:46 INFO mapreduce.Job:  map 14% reduce 0%
20/08/21 13:28:47 INFO mapreduce.Job:  map 15% reduce 0%
20/08/21 13:28:48 INFO mapreduce.Job:  map 16% reduce 0%

```

> 위에서 sbin으로 ssh를 실행 시켜줬기 때문에 바로 이렇게 실행

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -cat airline_dep_delay_output/part-r-00000
2012,1  151762
2012,10 183397
2012,11 161643
2012,12 207041
2012,2  136280
2012,3  187828
2012,4  153453
2012,5  175546
2012,6  205523
2012,7  232461
2012,8  215957
2012,9  158063

```

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls airline_dep_delay_output
Found 2 items
-rw-r--r--   1 bit44 supergroup          0 2020-08-21 13:29 airline_dep_delay_output/_SUCCESS
-rw-r--r--   1 bit44 supergroup        171 2020-08-21 13:29 airline_dep_delay_output/part-r-00000

```

