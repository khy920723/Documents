## 년도별/월별로 Sorting 시키기

- 기존 항공 운항 데이터 분석 결과를 보면 월 순서대로 출력되지 않았음

- 출력 데이터의 키 값 자체가 연도와 월이 합쳐진 하나의 문자열로 인식

- 보조정렬(Secondary Sort)은 키의 값들을 그룹핑하고 그루핑된 레코드에 순서를 부여하는 방식

- 보조정렬 구현순서

1. 기존 키의 값들을 조합한 복합키(Composite Key)를 정의, 이 때 키의 값 중에서 어떤 키를 그루핑 키로 사용할 지 결정
2. 복합키의 레코드를 정렬하기 위한 비교기(Comparator)를 정의
3. 그룹핑 키를 파티셔닝 할 파티셔너(Partitioner)를 정의
4. 그룹핑 키를 정렬하기 위한 비교기(Comparator)를 정의

```
login as: bit44
bit44@127.0.0.1's password:
Last login: Mon Aug 24 09:08:20 2020
[bit44@hadoopnode01 ~]$ cd hadoop
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ sbin/start-dfs.sh
.
.
.
[bit44@hadoopnode01 ~/hadoop]$ jps
3457 DataNode
3331 NameNode
3667 SecondaryNameNode
3811 Jps
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ sbin/start-yarn.sh
.
.
.
[bit44@hadoopnode01 ~/hadoop]$ jps
3457 DataNode
3331 NameNode
3667 SecondaryNameNode
4295 Jps
3865 ResourceManager
3981 NodeManager
[bit44@hadoopnode01 ~/hadoop]$ sbin/yarn-daemon.sh start proxyserver
.
.
.
[bit44@hadoopnode01 ~/hadoop]$ sbin/mr-jobhistory-daemon.sh start historyserver
.
.
.
[bit44@hadoopnode01 ~/hadoop]$ jps
3457 DataNode
3331 NameNode
3667 SecondaryNameNode
4339 WebAppProxyServer
3865 ResourceManager
3981 NodeManager
4398 JobHistoryServer
4463 Jps
[bit44@hadoopnode01 ~/hadoop]$
```

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -mv airline_dep_delay_input airline_input
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls
Found 10 items
drwxr-xr-x   - bit44 supergroup          0 2020-08-24 10:15 airline_arr_delay_output
drwxr-xr-x   - bit44 supergroup          0 2020-08-24 09:28 airline_dep_delay_output
drwxr-xr-x   - bit44 supergroup          0 2020-08-21 10:26 airline_input
drwxr-xr-x   - bit44 supergroup          0 2020-08-24 16:52 delay_counts_mos
drwxr-xr-x   - bit44 supergroup          0 2020-08-24 18:47 dep_delay_counter_output
drwxr-xr-x   - bit44 supergroup          0 2020-08-24 18:32 dep_delay_output
drwxr-xr-x   - bit44 supergroup          0 2020-08-20 09:31 word-input
drwxr-xr-x   - bit44 supergroup          0 2020-08-20 14:39 word-input2
drwxr-xr-x   - bit44 supergroup          0 2020-08-20 09:32 word-output
drwxr-xr-x   - bit44 supergroup          0 2020-08-20 14:42 word-output2

```

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -rm airline_input/*


20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201201.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201202.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201203.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201204.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201205.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201206.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201207.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201208.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201209.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201210.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201211.csv
20/08/25 09:30:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
Deleted airline_input/airOT201212.csv
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls airline_input

```



- 준비

D:\KHY_OracleVBox\VBoxShare\download에 D:\AirOnTimeCSV 파일 중 년도,월이 다른 것 6개 추가 후 기존에 있던 12개 csv파일을 download 폴더에서 6개만 지움

> 소팅을 시키기 위한 데이터 세팅 작업

> 헤더부분 지우기



- 하둡

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -put /mnt/share/download/*csv airline_input

```

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls airline_input


Found 12 items
-rw-r--r--   1 bit44 supergroup  116443192 2020-08-25 09:41 airline_input/airOT201001.csv
-rw-r--r--   1 bit44 supergroup  106921008 2020-08-25 09:41 airline_input/airOT201002.csv
-rw-r--r--   1 bit44 supergroup  123736245 2020-08-25 09:42 airline_input/airOT201006.csv
-rw-r--r--   1 bit44 supergroup  117353958 2020-08-25 09:42 airline_input/airOT201009.csv
-rw-r--r--   1 bit44 supergroup  117039438 2020-08-25 09:42 airline_input/airOT201011.csv
-rw-r--r--   1 bit44 supergroup  121086455 2020-08-25 09:42 airline_input/airOT201012.csv
-rw-r--r--   1 bit44 supergroup  103649811 2020-08-25 09:42 airline_input/airOT201202.csv
-rw-r--r--   1 bit44 supergroup  115919743 2020-08-25 09:42 airline_input/airOT201205.csv
-rw-r--r--   1 bit44 supergroup  117982135 2020-08-25 09:42 airline_input/airOT201206.csv
-rw-r--r--   1 bit44 supergroup  122492345 2020-08-25 09:42 airline_input/airOT201207.csv
-rw-r--r--   1 bit44 supergroup  121168354 2020-08-25 09:42 airline_input/airOT201208.csv
-rw-r--r--   1 bit44 supergroup  111451449 2020-08-25 09:42 airline_input/airOT201212.csv

```



- 자바

```java
package com.adacho.hadoop.common;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableUtils;

public class DateKey implements WritableComparable<DateKey>{

	private String year;
	private Integer month;
	
	public DateKey() {
		
	}
	
	public DateKey(String year, Integer month) {
		this.year = year;
		this.month = month;
	}

	
	
	@Override
	public void write(DataOutput out) throws IOException {
		// TODO Auto-generated method stub
		WritableUtils.writeString(out, year);
		out.writeInt(month);
	}

	
	
	@Override
	public void readFields(DataInput in) throws IOException {
		// TODO Auto-generated method stub
		year = WritableUtils.readString(in);
		month = in.readInt();
	}


	// 복합키 끼리 비교
	@Override
	public int compareTo(DateKey key) {
		// TODO Auto-generated method stub
		int result = year.compareTo(key.year);
		
		if(result == 0) {
			result = month.compareTo(key.month);
		}
		return result;
	}
	

	@Override
	public String toString() {
		// TODO Auto-generated method stub
		return new StringBuilder().append(year).append(",").append(month).toString();
	}

	
	
	/**
	 * @return the year
	 */
	public String getYear() {
		return year;
	}

	/**
	 * @param year the year to set
	 */
	public void setYear(String year) {
		this.year = year;
	}

	/**
	 * @return the month
	 */
	public Integer getMonth() {
		return month;
	}

	/**
	 * @param month the month to set
	 */
	public void setMonth(Integer month) {
		this.month = month;
	}
	
	
	
}

```

```java
package com.adacho.hadoop.common;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

public class DateKeyComparator extends WritableComparator{

	protected DateKeyComparator() {
		super(DateKey.class, true);
	}

	
	@SuppressWarnings("rawtypes") // 주석으로 노란 줄 막기
	@Override
	public int compare(WritableComparable a, WritableComparable b) {
		// TODO Auto-generated method stub
		DateKey k1 = (DateKey)a;
		DateKey k2 = (DateKey)b;
		
		int cmp = k1.getYear().compareTo(k2.getYear());
		
		if(cmp != 0) {
			return cmp;
		}
		
		return k1.getMonth() == k2.getMonth() ? 0 : (k1.getMonth() < k2.getMonth() ? -1 : 1); // 월 비교 부분
	}
	
	
	
}

```

```java
package com.adacho.hadoop.common;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Partitioner;

public class GroupKeyPartitioner extends Partitioner<DateKey, IntWritable>{

	@Override
	public int getPartition(DateKey key, IntWritable value, int numPartitions) {
		// TODO Auto-generated method stub
		int hash = key.getYear().hashCode();
		int partition = hash % numPartitions; // 파티션으로 나머지 연산 하면 파티션 갯수가 나옴
		return partition;
	}

	
}

```

```java
package com.adacho.hadoop.common;

import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;

public class GroupKeyComparator extends WritableComparator{

	protected GroupKeyComparator() {
		super(DateKey.class, true);
	}

	
	@SuppressWarnings("rawtypes")
	@Override
	public int compare(WritableComparable a, WritableComparable b) {
		// TODO Auto-generated method stub
		DateKey k1 = (DateKey)a;
		DateKey k2 = (DateKey)b;
		
		return k1.getYear().compareTo(k2.getYear());
	}
	
	
}

```



- 리듀서를 작성하기 전 봐야할 부분

1. 리듀서에는 그루핑 파티셔너와 그루핑 comparator에 의해 같은 연도로 그루핑된 데이터가 전달된 상태
2. 복합키 comparator로 인해 그룹핑 된 값은 월의 순서대로 오름차순으로 정렬되어 있음
3. 하지만 리듀서 메서드에서 지연 횟수를 합산할 경우 데이터에 오류가 발생
4. 예를들어 2008년도 항공 출발 지연 데이터를 처리할 경우 다음과 같은 결과가 나타남

> 2008 12 2647363

5. 2008년도 12월만 출력되고 지연 횟수도 2008년도의 모든 지연 횟수가 합산되어 출력됨
6. 이러한 현상이 나타나는 이유는 리듀서는 2008년이라는 그룹키를 기준으로 연산을 수행하기 때문
7. 월별로 지연 횟수를 계산하려면 복합키를 구분해서 처리하는 코드를 구현해야함
8. 입력 데이터의 값에 해당하는 Iterable 객체를 순회할 때 월에 해당하는 값을 bMonth라는 변수에 백업
9. 순회를 하면서 백업된 월과 현재 데이터의 월이 일치하지 않을 때는 리듀서의 출력 데이터에 백업된 월의 지연 횟수를 출력
10. 이 때 다음 순서에 있는 월의 지연 횟수를 합산할 수 있게 지연 횟수 합계 변수를 0으로 초기화

```java
package com.adacho.hadoop.reducer;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;

import com.adacho.hadoop.common.DateKey;

public class DelayCountReducerWithDateKey extends Reducer<DateKey, IntWritable, DateKey, IntWritable>{

	private MultipleOutputs<DateKey, IntWritable> mos;
	private DateKey outputKey = new DateKey();
	private IntWritable result = new IntWritable();
	
	
	@Override
	protected void setup(Reducer<DateKey, IntWritable, DateKey, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.setup(context);
		mos = new MultipleOutputs<DateKey, IntWritable>(context);
	}


	@Override
	protected void reduce(DateKey key, Iterable<IntWritable> values,
			Reducer<DateKey, IntWritable, DateKey, IntWritable>.Context context) throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.reduce(arg0, arg1, arg2);
		String[] columns = key.getYear().split(",");
		
		int sum = 0;
		Integer bMonth = key.getMonth();
		
		// departure 부분
		if(columns[0].equals("D")) {
			for(IntWritable value : values) {
				if(bMonth != key.getMonth()) {
					result.set(sum);
					outputKey.setYear(key.getYear().substring(2));
					outputKey.setMonth(bMonth);
					mos.write("departure", outputKey, result);
					sum = 0;
				}
				sum += value.get();
				bMonth = key.getMonth();
			}
			if(key.getMonth() == bMonth) {
				outputKey.setYear(key.getYear().substring(2));
				outputKey.setMonth(key.getMonth());
				result.set(sum);
				mos.write("departure", outputKey, result);
			}
		} 
		// arrival 부분
		else {
			for(IntWritable value : values) {
				if(bMonth != key.getMonth()) {
					result.set(sum);
					outputKey.setYear(key.getYear().substring(2));
					outputKey.setMonth(bMonth);
					mos.write("arrival", outputKey, result);
					sum = 0;
				}
				sum += value.get();
				bMonth = key.getMonth();
			}
			if(key.getMonth() == bMonth) {
				outputKey.setYear(key.getYear().substring(2));
				outputKey.setMonth(key.getMonth());
				result.set(sum);
				mos.write("arrival", outputKey, result);
			}
		}
	}


	// 리소스 해제
	@Override
	protected void cleanup(Reducer<DateKey, IntWritable, DateKey, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.cleanup(context);
		mos.close();
	}

	
}

```

```java
package com.adacho.hadoop.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import com.adacho.hadoop.common.DateKey;
import com.adacho.hadoop.common.DateKeyComparator;
import com.adacho.hadoop.common.GroupKeyComparator;
import com.adacho.hadoop.common.GroupKeyPartitioner;
import com.adacho.hadoop.mapper.DelayCountMapperWithDateKey;
import com.adacho.hadoop.reducer.DelayCountReducerWithDateKey;

public class DelayCountWithDateKey extends Configured implements Tool{

	public static void main(String[] args) throws Exception{
		int res = ToolRunner.run(new Configuration(), new DelayCountWithDateKey(), args);
		System.out.println("MR-Job Result:" + res);
	}

	
	@Override
	public int run(String[] args) throws Exception {
		// TODO Auto-generated method stub
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();
		
		// 입출력 데이터 경로 확인
		if(otherArgs.length != 2) {
			System.err.println("Usage: DelayCountWithDateKey <in> <out>");
			System.exit(2);
		}
		
		// Job 이름 설정
		Job job = Job.getInstance(getConf(), "DelayCountWithDateKey");
		
		// 입출력 데이터 경로 설정
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		
		// Job 클래스 설정
		job.setJarByClass(DelayCountWithDateKey.class);
		job.setPartitionerClass(GroupKeyPartitioner.class);
		job.setGroupingComparatorClass(GroupKeyComparator.class);
		job.setSortComparatorClass(DateKeyComparator.class);
		
		// Mapper 클래스 설정
		job.setMapperClass(DelayCountMapperWithDateKey.class);
		
		// 리듀서 클래스 설정
		job.setReducerClass(DelayCountReducerWithDateKey.class);
		
		job.setMapOutputKeyClass(DateKey.class);
		job.setMapOutputValueClass(IntWritable.class);
		
		// 입출력 데이터 포맷 설정
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		// 출력키 및 출력값 유형 설정
		job.setOutputKeyClass(DateKey.class);
		job.setOutputValueClass(IntWritable.class);
		
		// MultipleOutputs 설정
		MultipleOutputs.addNamedOutput(job, "departure", TextOutputFormat.class, DateKey.class, IntWritable.class);
		MultipleOutputs.addNamedOutput(job, "arrival", TextOutputFormat.class, DateKey.class, IntWritable.class);
		
		job.waitForCompletion(true);
		return 0;
	}
	
	
	
}

```

```java
package com.adacho.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.adacho.hadoop.common.AirlinePerformanceParser;
import com.adacho.hadoop.common.DateKey;
import com.adacho.hadoop.counter.DelayCounters;

public class DelayCountMapperWithDateKey extends Mapper<LongWritable, Text, DateKey, IntWritable>{
	
	private final static IntWritable outputValue = new IntWritable(1);
	private DateKey outputKey = new DateKey();
	
	
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, DateKey, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.map(key, value, context);
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		
		if(parser.isDepartureDelayAvailable()) {
			if(parser.getDepartureDelayTime() > 0) {
				outputKey.setYear("D,"+ parser.getYear());
				outputKey.setMonth(parser.getMonth());
				context.write(outputKey, outputValue);
			}else if(parser.getDepartureDelayTime() == 0) {
				context.getCounter(DelayCounters.scheduled_departure).increment(1);
			}else if(parser.getDepartureDelayTime() < 0) {
				context.getCounter(DelayCounters.early_departure).increment(1);
			}
		}else {
			context.getCounter(DelayCounters.not_available_departure).increment(1);
		}
		
		if(parser.isArriveDelayAvailable()) {
			if(parser.getArriveDelayTime() > 0) {
				outputKey.setYear("A," + parser.getYear());
				outputKey.setMonth(parser.getMonth());
				context.write(outputKey, outputValue);
			} else if(parser.getArriveDelayTime() == 0) {
				context.getCounter(DelayCounters.scheduled_arrival).increment(1);
			}else if(parser.getArriveDelayTime() < 0) {
				context.getCounter(DelayCounters.scheduled_arrival).increment(1);
			}
		}else {
			context.getCounter(DelayCounters.not_available_arrival).increment(1);
		}
	}
	
	
}

```



- jar 파일 만들어서 java 폴더에 덮어쓰기



- 하둡

```
[bit44@hadoopnode01 ~/hadoop]$ bin/yarn jar /mnt/share/java/airline-0.0.1.jar com.adacho.hadoop.driver.DelayCountWithDateKey airline_input delay_count_sort_output
.
.
.
        com.adacho.hadoop.counter.DelayCounters
                early_departure=3363088
                not_available_arrival=128309
                not_available_departure=109084
                scheduled_arrival=3754343
                scheduled_departure=410947
        File Input Format Counters
                Bytes Read=1395244133
        File Output Format Counters
                Bytes Written=0
MR-Job Result:0

```

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls delay_count_sort_output


Found 4 items
-rw-r--r--   1 bit44 supergroup          0 2020-08-25 14:15 delay_count_sort_output/_SUCCESS
-rw-r--r--   1 bit44 supergroup        171 2020-08-25 14:15 delay_count_sort_output/arrival-r-00000
-rw-r--r--   1 bit44 supergroup        171 2020-08-25 14:15 delay_count_sort_output/departure-r-00000
-rw-r--r--   1 bit44 supergroup          0 2020-08-25 14:15 delay_count_sort_output/part-r-00000

```

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -cat delay_count_sort_output/arrival-r-00000
2010,1  193759
2010,2  186804
2010,6  231109
2010,9  169708
2010,11 187232
2010,12 240259
2012,2  138349
2012,5  171789
2012,6  197091
2012,7  224610
2012,8  208734
2012,12 201482
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -cat delay_count_sort_output/departure-r-00000
2010,1  186222
2010,2  181897
2010,6  233892
2010,9  156332
2010,11 179047
2010,12 240261
2012,2  136280
2012,5  175546
2012,6  205523
2012,7  232461
2012,8  215957
2012,12 207041

```





# Data warehouse(하이브)

https://www.mysql.com/ 접속 - 다운로드 - [MySQL Community (GPL) Downloads » -> yum 레퍼지토리 ->  mysql80-community-release-el7-3.noarch.rpm 다운 -> D:\KHY_OracleVBox\VBoxShare\download에 복붙



```
[bit44@hadoopnode01 ~/hadoop]$ su -
암호:
마지막 로그인: 화  8월 18 14:39:51 KST 2020 일시 pts/0
[root@hadoopnode01 ~]#
[root@hadoopnode01 ~]#
[root@hadoopnode01 ~]#
[root@hadoopnode01 ~]#
[root@hadoopnode01 ~]# pwd
/root
[root@hadoopnode01 ~]#
[root@hadoopnode01 ~]#
[root@hadoopnode01 ~]#
[root@hadoopnode01 ~]#
[root@hadoopnode01 ~]# cd /usr/local
[root@hadoopnode01 local]#
[root@hadoopnode01 local]#
[root@hadoopnode01 local]#
[root@hadoopnode01 local]#
[root@hadoopnode01 local]# ls -l
합계 8
drwxr-xr-x.  2 root   root   20  8월 18 15:19 bin
drwxr-xr-x.  2 root   root    6  4월 11  2018 etc
drwxr-xr-x.  2 root   root    6  4월 11  2018 games
drwxr-xr-x.  3 root   root   20  8월 18 15:19 include
drwxr-xr-x.  3 root   root 4096  8월 18 15:19 lib
drwxr-xr-x.  2 root   root    6  4월 11  2018 lib64
drwxr-xr-x.  2 root   root    6  4월 11  2018 libexec
drwxr-xr-x. 10 109965 5000 4096  8월 18 15:10 protobuf-2.5.0
drwxr-xr-x.  2 root   root    6  4월 11  2018 sbin
drwxr-xr-x.  5 root   root   49  8월 12 11:31 share
drwxr-xr-x.  2 root   root    6  4월 11  2018 src

```

> 관리자 권한으로 설치

```
[root@hadoopnode01 local]# cp /mnt/share/download/mysql80-community-release-el7-3.noarch.rpm .
[root@hadoopnode01 local]# ls -l
합계 36
drwxr-xr-x.  2 root   root    20  8월 18 15:19 bin
drwxr-xr-x.  2 root   root     6  4월 11  2018 etc
drwxr-xr-x.  2 root   root     6  4월 11  2018 games
drwxr-xr-x.  3 root   root    20  8월 18 15:19 include
drwxr-xr-x.  3 root   root  4096  8월 18 15:19 lib
drwxr-xr-x.  2 root   root     6  4월 11  2018 lib64
drwxr-xr-x.  2 root   root     6  4월 11  2018 libexec
-rwxr-x---.  1 root   root 26024  8월 25 15:20 mysql80-community-release-el7-3.noarch.rpm
drwxr-xr-x. 10 109965 5000  4096  8월 18 15:10 protobuf-2.5.0
drwxr-xr-x.  2 root   root     6  4월 11  2018 sbin
drwxr-xr-x.  5 root   root    49  8월 12 11:31 share
drwxr-xr-x.  2 root   root     6  4월 11  2018 src

```

```
[root@hadoopnode01 local]# vi /etc/yum.conf

```

```
[main]
cachedir=/var/cache/yum/$basearch/$releasever
keepcache=0
debuglevel=2
logfile=/var/log/yum.log
exactarch=1
obsoletes=1
gpgcheck=0
plugins=1
installonly_limit=5


```

> gpgcheck=0 : 전자 서명 인증을 하지 않음으로 바꿈

```
[root@hadoopnode01 local]# rpm -ivh mysql80-community-release-el7-3.noarch.rpm
.
.
.

[root@hadoopnode01 local]# yum install -y mysql-server
.
.
.

```

```
[root@hadoopnode01 local]# vi /etc/yum.conf

```

```
cachedir=/var/cache/yum/$basearch/$releasever
keepcache=0
debuglevel=2
logfile=/var/log/yum.log
exactarch=1
obsoletes=1
gpgcheck=1
plugins=1

```

> 1로 다시 원래 상태로 돌려놓기



- mysql 설정

```
[root@hadoopnode01 local]# systemctl enable mysqld

```

> 마이에스큐엘 데몬을 자동으로 올려줌

```
[root@hadoopnode01 local]# systemctl start mysqld

```

```
[root@hadoopnode01 local]# mysql root@localhost
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)

```

> 마이에스큐엘을 설치하면 루트 계정에 저장되는데 임시 패스워드로 접근해야함

```
[root@hadoopnode01 local]# cat /var/log/mysqld.log


2020-08-25T06:46:51.058561Z 0 [System] [MY-013169] [Server] /usr/sbin/mysqld (mysqld 8.0.21) initializing of server in progress as process 11279
2020-08-25T06:46:51.070828Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2020-08-25T06:46:53.219241Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
2020-08-25T06:46:55.423292Z 6 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: 8SerbM1J?*q,
2020-08-25T06:47:00.915584Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.21) starting as process 11325
2020-08-25T06:47:00.930998Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2020-08-25T06:47:01.597775Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
2020-08-25T06:47:01.770525Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: '::' port: 33060, socket: /var/run/mysqld/mysqlx.sock
2020-08-25T06:47:02.033524Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
2020-08-25T06:47:02.033821Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
2020-08-25T06:47:02.175835Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: '8.0.21'  socket: '/var/lib/mysql/mysql.sock'  port: 3306  MySQL Community Server - GPL.

```

> 8SerbM1J?*q, 이 부분이 임시 패스워드

> ```
> [root@hadoopnode01 local]# rm -f mysql80-community-release-el7-3.noarch.rpm
> ```

```
login as: bit44
bit44@127.0.0.1's password:
Last login: Tue Aug 25 09:07:13 2020 from gateway
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$ ls -l
합계 4
lrwxrwxrwx.  1 bit44 bit44   12  8월 20 09:09 hadoop -> hadoop-2.7.2
drwxr-xr-x. 11 bit44 bit44  173  8월 20 09:25 hadoop-2.7.2
drwxrwxr-x.  3 bit44 bit44   20  8월 20 09:11 hdfs-data
drwxrwxr-x.  2 bit44 bit44   49  8월 14 09:39 source
drwxrwxr-x.  3 bit44 bit44 4096  8월 24 09:29 test01
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 공개
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 다운로드
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 문서
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 바탕화면
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 비디오
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 사진
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 서식
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 음악
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$ mysql -u root -p
Enter password:
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 9
Server version: 8.0.21

Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.


```

> 비밀번호 입력 후의 모습
>
> (대소문자 구분)

> 새로운 세션 모습

```
mysql> alter user 'root'@'localhost' identified by '1234';
ERROR 1819 (HY000): Your password does not satisfy the current policy requirements
mysql>
mysql>
mysql>
mysql> show variables like 'validate_password%';
ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.

```

> 이렇게 하면 안 들어가짐

```
mysql> alter user 'root'@'localhost' identified by 'Thisishadoop10!';
Query OK, 0 rows affected (0.01 sec)

mysql>

```

> 설정을 바꿔줌(대소문자, 특수문자, 숫자 필요)

```
mysql> show variables like 'validate_password%';
+--------------------------------------+--------+
| Variable_name                        | Value  |
+--------------------------------------+--------+
| validate_password.check_user_name    | ON     |
| validate_password.dictionary_file    |        |
| validate_password.length             | 8      |
| validate_password.mixed_case_count   | 1      |
| validate_password.number_count       | 1      |
| validate_password.policy             | MEDIUM |
| validate_password.special_char_count | 1      |
+--------------------------------------+--------+
7 rows in set (0.01 sec)

```

> 계정 관련 정보 형태

```
mysql> set GLOBAL validate_password.length=4;
Query OK, 0 rows affected (0.00 sec)

mysql> set GLOBAL validate_password.mixed_case_count=0;
Query OK, 0 rows affected (0.00 sec)

mysql> set GLOBAL validate_password.policy=LOW;
Query OK, 0 rows affected (0.00 sec)

mysql> set GLOBAL validate_password.special_char_count=0;
Query OK, 0 rows affected (0.00 sec)

mysql> alter user 'root'@'localhost' identified by '1234';
Query OK, 0 rows affected (0.01 sec)


```

> 비밀번호(글로벌 변수값) 1회성으로 변경



- 하이브에서 메타데이터를 저장할 데이터 만들기

```
mysql> create database hive_db default character set utf8;
Query OK, 1 row affected, 1 warning (0.01 sec)
mysql>
mysql>
mysql>
mysql> show databases
    -> ;
+--------------------+
| Database           |
+--------------------+
| hive_db            |
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.00 sec)

mysql>

```

```
mysql> flush privileges;
Query OK, 0 rows affected (0.01 sec)

mysql>
mysql> create user 'hiveuser'@'%' identified by '1234';
Query OK, 0 rows affected (0.01 sec)

mysql>
mysql> grant all privileges on hive_db.* to hiveuser@'%';
Query OK, 0 rows affected (0.00 sec)


```

> 하이브가 쓸 유저 만들기

```
mysql> quit
Bye

```

> SQL에서 빠져나가기

> root@hadoopnode01 local 에서 exit 입력하면 하둡으로 빠져나옴



- 아파치 하이브

https://hive.apache.org/ -> 다운로드 -> http://mirror.apache-kr.org/hive/hive-2.3.7/ -> bin.tar.gz 링크 복사

```
[bit44@hadoopnode01 ~/hadoop]$ cd
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$
[bit44@hadoopnode01 ~]$ wget http://mirror.apache-kr.org/hive/hive-2.3.7/apache-hive-2.3.7-bin.tar.gz

```

```
[bit44@hadoopnode01 ~]$ tar -zxcf apache-hive-2.3.7-bin.tar.gz
.
.
.

```

```
[bit44@hadoopnode01 ~]$ rm -f apache-hive-2.3.7-bin.tar.gz
[bit44@hadoopnode01 ~]$ ls -l
합계 4
drwxrwxr-x. 10 bit44 bit44  184  8월 25 16:42 apache-hive-2.3.7-bin
lrwxrwxrwx.  1 bit44 bit44   12  8월 20 09:09 hadoop -> hadoop-2.7.2
drwxr-xr-x. 11 bit44 bit44  209  8월 25 16:39 hadoop-2.7.2
drwxrwxr-x.  3 bit44 bit44   20  8월 20 09:11 hdfs-data
drwxrwxr-x.  2 bit44 bit44   49  8월 14 09:39 source
drwxrwxr-x.  3 bit44 bit44 4096  8월 24 09:29 test01
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 공개
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 다운로드
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 문서
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 바탕화면
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 비디오
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 사진
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 서식
drwxr-xr-x.  2 bit44 bit44    6  8월 12 12:18 음악
[bit44@hadoopnode01 ~]$

```

```
[bit44@hadoopnode01 ~]$ cd apache-hive-2.3.7-bin/
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ ls -l
합계 56
-rw-r--r--. 1 bit44 bit44 20798  3월 10 01:09 LICENSE
-rw-r--r--. 1 bit44 bit44   230  4월  8 03:57 NOTICE
-rw-r--r--. 1 bit44 bit44   361  4월  8 04:37 RELEASE_NOTES.txt
drwxrwxr-x. 3 bit44 bit44   133  8월 25 16:42 bin
drwxrwxr-x. 2 bit44 bit44  4096  8월 25 16:42 binary-package-licenses
drwxrwxr-x. 2 bit44 bit44  4096  8월 25 16:42 conf
drwxrwxr-x. 4 bit44 bit44    34  8월 25 16:42 examples
drwxrwxr-x. 7 bit44 bit44    68  8월 25 16:42 hcatalog
drwxrwxr-x. 2 bit44 bit44    44  8월 25 16:42 jdbc
drwxrwxr-x. 4 bit44 bit44 12288  8월 25 16:42 lib
drwxrwxr-x. 4 bit44 bit44    35  8월 25 16:42 scripts
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ ls -l conf
합계 288
-rw-r--r--. 1 bit44 bit44   1596 12월 17  2016 beeline-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44 257573  4월  8 04:42 hive-default.xml.template
-rw-r--r--. 1 bit44 bit44   2365  5월  3  2017 hive-env.sh.template
-rw-r--r--. 1 bit44 bit44   2274  3월  9  2017 hive-exec-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   2925  3월 10 01:09 hive-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   2060  3월  9  2017 ivysettings.xml
-rw-r--r--. 1 bit44 bit44   2719  3월 10 01:09 llap-cli-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   7041  3월 10 01:09 llap-daemon-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   2662  3월  9  2017 parquet-logging.properties
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$

```

> 하이브에 필요한 설정파일들

```
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ mv conf/hive-env.sh.template conf/hive-env.sh
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ ls -l conf
합계 288
-rw-r--r--. 1 bit44 bit44   1596 12월 17  2016 beeline-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44 257573  4월  8 04:42 hive-default.xml.template
-rw-r--r--. 1 bit44 bit44   2365  5월  3  2017 hive-env.sh
-rw-r--r--. 1 bit44 bit44   2274  3월  9  2017 hive-exec-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   2925  3월 10 01:09 hive-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   2060  3월  9  2017 ivysettings.xml
-rw-r--r--. 1 bit44 bit44   2719  3월 10 01:09 llap-cli-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   7041  3월 10 01:09 llap-daemon-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   2662  3월  9  2017 parquet-logging.properties
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$

```

> 사용하기 위해 디렉토리 옮기기

```
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ vi conf/hive-env.sh

```

```
​```
# Set HADOOP_HOME to point to a specific hadoop install directory
HADOOP_HOME=/home/bit44/hadoop

```

> 환경변수 변경

```
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ mv conf/hive-default.xml.template conf/hive-default.xml
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ ls -l conf


합계 288
-rw-r--r--. 1 bit44 bit44   1596 12월 17  2016 beeline-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44 257573  4월  8 04:42 hive-default.xml
-rw-r--r--. 1 bit44 bit44   2362  8월 25 17:03 hive-env.sh
-rw-r--r--. 1 bit44 bit44   2274  3월  9  2017 hive-exec-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   2925  3월 10 01:09 hive-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   2060  3월  9  2017 ivysettings.xml
-rw-r--r--. 1 bit44 bit44   2719  3월 10 01:09 llap-cli-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   7041  3월 10 01:09 llap-daemon-log4j2.properties.template
-rw-r--r--. 1 bit44 bit44   2662  3월  9  2017 parquet-logging.properties
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$

```

> 템플릿을 xml로 바꿔줌

```
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ vi conf/hive-default.xml

```

```
  <property>
    <name>hive.metastore.local</name>
    <value>false</value>
    <description>location of default database for the warehouse</description>
 </property>

```

> 해당 내용 끼워넣기

> /hive.metastore 로 찾기, n으로 다음꺼
>
> yy o p로 붙여넣기 후 수정

```
    <description>Name of the hook to use for retrieving the JDO connection URL. If empty, the value in javax.jdo.option.ConnectionURL is used</description>
  </property>
  <property>
    <name>javax.jdo.option.Multithreaded</name>
    <value>true</value>
    <description>Set this to true if multiple threads access metastore through JDO concurrently.</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/hive_db?createDatabaseIfNotExist=true</value>
    <description>
      JDBC connect string for a JDBC metastore.
      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.

```

> /javax.jdo.option.ConnectionURL

> 해당부분에서 바뀐 곳 변경
>
> 로컬호스트는 3306 포트를 사용함

```
escription>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
    <description>Driver class name for a JDBC metastore</description>
  </property>
  <property>
    <name>javax.jdo.PersistenceManagerFactoryClass</name>
    <value>org.datanucleus.api.jdo.JDOPersistenceManagerFactory</value>
    <description>class implementing the jdo persistence</description>
  </property>

```

> 해당부분에서 바뀐 곳 변경

> /javax.jdo.option.ConnectionDriverName

```
  <value>true</value>
    <description>Reads outside of transactions</description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hiveuser</value>
    <description>Username to use against metastore database</description>
  </property>
  <property>
    <name>hive.metastore.end.function.listeners</name>
    <value/>
    <description>List of comma separated listeners for the end of metastore functions.</description>
  </property>
  <property>
    <name>hive.metastore.partition.inherit.table.properties</name>

```

> /javax.jdo.option.ConnectionUserName

```
      Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec), which is sec if not specified.
      MetaStore Client socket lifetime in seconds. After this time is exceeded, client
      reconnects on the next MetaStore operation. A value of 0s means the connection
      has an infinite lifetime.
    </description>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>1234</value>
    <description>password to use against metastore database</description>
  </property>
  <property>
    <name>hive.metastore.ds.connection.url.hook</name>
    <value/>
    <description>Name of the hook to use for retrieving the JDO connection URL. If empty, the value in javax.jdo.option.ConnectionURL is used</description>
  </property>

```

> /javax.jdo.option.ConnectionPassword

```
     set by SQL standard authorization instead of replacing it entirely.
    </description>
  </property>
  <property>
    <name>hive.cli.print.header</name>
    <value>true</value>
    <description>Whether to print the names of the columns in query output.</description>
  </property>
  <property>
    <name>hive.cli.tez.session.async</name>
    <value>true</value>
    <description>

```

> /hive.cli.print.header

```
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$ sudo netstat -tlpn | grep mysqld
[sudo] bit44의 암호:
tcp6       0      0 :::33060                :::*                    LISTEN      11325/mysqld
tcp6       0      0 :::3306                 :::*                    LISTEN      11325/mysqld
[bit44@hadoopnode01 ~/apache-hive-2.3.7-bin]$

```







```
[bit44@hadoopnode01 ~]$ cd hadoop
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ sbin/mr-jobhistory-daemon.sh stop historyserver
stopping historyserver
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ sbin/stop-yarn.sh
stopping yarn daemons
stopping resourcemanager
hadoopnode01: stopping nodemanager
stopping proxyserver
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ sbin/stop-dfs.sh
Stopping namenodes on [hadoopnode01]
hadoopnode01: stopping namenode
hadoopnode01: stopping datanode
Stopping secondary namenodes [hadoopnode01]
hadoopnode01: stopping secondarynamenode
[bit44@hadoopnode01 ~/hadoop]$

```

