## CNN Advanced

```python
# Advanced한 내용
# DataGenerator를 이용한 데이터 관리
# @tf.session을 이용한 훈련
import tensorflow as tf
import numpy as np

from tensorflow.keras import layers
from tensorflow.keras import datasets

import matplotlib.pyplot as plt
%matplotlib inline

```

```python
## DataGenerator
# 1. 사이킷런의 train_test_split의 업그레이드 된 버전
# 2. 셔플의 기능이 추가, 배치의 기능까지 추가(모든 데이터를 병렬로 처리)
# 3. label의 관리가 필요 없음

# from_tensor_slices() -> 데이터 세트 만들기
# shuffle() -> 섞기
# batch() -> 미니배치 만들기

mnist = tf.keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train[..., tf.newaxis]
X_test = X_test[..., tf.newaxis]

X_train, X_test = X_train / 255.0, X_test / 255.0

```

```python
# tf.data 사용하기

# 데이터를 병렬로 불러올 수 있게 해줌
train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))

# 훈련 데이터 generator 만들기
train_ds = train_ds.shuffle(1000) # 버퍼 사이즈 지정, 1000 정도가 제일 적당
train_ds = train_ds.batch(32) # 미니 배치 사이즈

# 테스트 데이터 generator 만들기
test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))
test_ds = test_ds.batch(32)
```

```python
# 시각화
# take() 함수를 이용하면 배치 사이즈 만큼 가져옴

# 현재 각 배치의 형상(32, 28, 28, 1)
for image_batch, label in train_ds.take(2):
  print(image_batch.shape, label.shape)
  plt.title(str(label[0]))
  plt.imshow(image_batch[0, ..., 0], 'gray')
  plt.show()
```

```
(32, 28, 28, 1) (32,)

(32, 28, 28, 1) (32,)

```

> 노트설정 GPU로 변경



```python
# 케라스를 이용해서 CNN 레이어 만들기
from tensorflow.keras import layers

# 레이어 쌓기
# 1. Input 형상, 분류할 클래스 개수 준비
imput_shape = (28, 28, 1) # 이렇게 하기 싫으면 X_train[0].shape으로 사용, 하지만 데이터 모양은 일정하기 때문에 상수로 넣어줌
num_classes = 10

# 2. input 레이어 만들기
inputs = layers.Input(shape=input_shape)

# 3, 필요한 레이어 쌓기
# Feature Extraction을 할 수 있는 CNN 레이어 구성
# 레이어가 많아질 수록 많은 특징 파악 가능
net = layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='SAME')(inputs) # 데이터를 투과시켜 net이 결과물을 받음
net = layers.Activation('relu')(net)
net = layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.MaxPool2D((2, 2))(net)
net = layers.Dropout(0.25)(net)

net = layers.Conv2D(filters=64, kernel_size=3, strides=1, padding='SAME')(net) # 데이터를 투과시켜 net이 결과물을 받음
net = layers.Activation('relu')(net)
net = layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='SAME')(net)
net = layers.Activation('relu')(net)
net = layers.MaxPool2D((2, 2))(net)
net = layers.Dropout(0.25)(net)

# 분류를 위한 Fully Connected(Dense, Affine)
net = layers.Flatten()(net) # 데이터를 펴줌
net = layers.Dense(512)(net)
net = layers.Activation('relu')(net)
net = layers.Dropout(0.25)(net)

# 출력층 설계
net = layers.Dense(num_classes)(net)
net = layers.Activation('softmax')(net)

# 최종 모델 만들기
model = tf.keras.Model(inputs=inputs, outputs=net, name='Basic_CNN')

model.summury() # 모델 서머리 확인
model.complie(optimier'adam', loss='sparse_categoryical_crossentropy') # 모델 컴파일
model.fit(train_ds, epochs=1) # 학습
```

```python
"""
Tensorflow를 중급자 수준으로 사용하기

최적화 함수 따로 빼기
- Loss Function
- Optimizer
"""
loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()

"""
배치를 활용하고 있기 때문에 각 배치마다의 loss, accuracy에 대한 평균을 구해줌
즉 평가 방법을 각 배치마다의 평균을 냄
"""
train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

```

```python
""""
훈련 시키기
* compile하고 fit 했던 부분 주석처리 필요

@tf.function - 그래프만 미리 만들어 놓고 실제 학습이 시작되면 실행되는 함수 만들기
"""

@tf.function # Tensorflow session에서 작동되는 함수 만들기(훈련을 할 때만 실행 됨)
def train_step(images, labels):
  # 자동 미분(오차 역전파)을 수행하는 객체 생성
  with tf.GradientTape() as tape: # tape이 미분을 실행해주는 시스템(무조건 사용해야 함)
    predictions = model(images) # model이 훈련 모드로 작동
    loss = loss_object(labels, predictions)

  # 미분 적용하기
  gradients = tape.gradient(loss, model.trainable_variables) # 오차역전파를 통한 기울기 구하기
  optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # 구해진 기울기를 이용한 최적화

  # 배치 평균 손실 구하기
  train_loss(loss)

  # 배치 평균 정확도 구하기
  train_accuracy(labels, predictions)

```

```python
"""
테스트 구현하기
"""
@tf.function
def test_step(images, labels):
  predictions = model(images)
  t_loss = loss_object(labels, predictions)
  test_loss(t_loss)

  test_loss(t_loss)
  test_accuracy(labels, predictions)

epochs = 5

for epoch in range(epochs):
  # 전체 데이터 훈련
  for images, labels in train_ds:
    train_step(images, labels)

  # 이어서 테스트
  for test_images, test_labels in test_ds:
    test_step(test_images, test_labels)

  template = "Epoch {}, Train Loss : {} / Train Accuracy: {} --- Test Loss : {}  Test Accuracy : {}"
  print(template.format(epoch + 1, 
                        train_loss.result(), 
                        train_accuracy.result() * 100, 
                        test_loss.result(),
                        test_accuracy.result() * 100))
```

```python
"""
모델 평가와 히스토리 보기
"""

"""
Evaluating
- 학습된 모델 확인
"""
model.complie(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])
hist = model.fit(tarin_ds, epochs=10) # 훈련 데이터에 대한 에폭 당 히스토리 확인 가능, 실제 업계에선 만 단위로 보기도 함

hist.history # 히스토리 보기

# 테스트 데이터에 대한 확인
model.evaluate(test_ds) # 출시하기 전에 evaluate 해보는 것
```





## DNN

```python
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
import numpy as np

x = np.arange(-1.0, 5.0)
y = np.arange(5.0, 11.0)

print(x)
print(y)

# 시퀀셜 모델
model = Sequential([
                    Dense(1),
]) # 10개를 한 번에 받음

# 모델 생성
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, epochs=1200)
```

```
...
Epoch 1199/1200
1/1 [==============================] - 0s 2ms/step - loss: 2.7198e-10
Epoch 1200/1200
1/1 [==============================] - 0s 3ms/step - loss: 2.7198e-10
<tensorflow.python.keras.callbacks.History at 0x7f079ca51f28>
```



```python
print(model.predict([10.0]))

weights = model.get_weights()
print(weights[0], weights[1])
```

```
[[16.000046]]
[[1.0000069]] [5.999978]
```



```python
"""
TF-2 Fashion MNIST
"""
fashion_mnist = tf.keras.datasets.fashion_mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
```

```python
print(X_train.shape)
import matplotlib.pyplot as plt
%matplotlib inline

plt.imshow(X_train[0], 'gray')
plt.show()
```

```
(60000, 28, 28)

```



```python
# 리스케일링
X_train, X_test = X_train / 255.0, X_test / 255.0

"""
DNN으로 이미지 문제 풀기
"""
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.callbacks import ModelCheckpoint # 좋았던 부분만 저장하는 클래스

model = Sequential([
                    Flatten(input_shape=(28, 28)),
                    Dense(128, activation='relu'),
                    Dense(128, activation='relu'),
                    Dense(10, activation='softmax')
]) # 입력층, 중간층, 출력층

print(y_train[0])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])
```

```
9
```



```python
"""
ModelCheckpoint 만들기
1 애폭 당 훈련을 하게 되면 loss, acc 등이 나옴
1 에폭 훈련 시 제일 좋았던 가중치와 편향 등을 저장 할 수 있게 해줌
"""
# 체크 포인트 파일 이름 지정
checkpoint_path = 'my_checkpoint.ckpt'
checkpoint = ModelCheckpoint(
    filepath = checkpoint_path,
    save_weights_only=True,
    save_best_only=True,
    monitor='val_loss', # 검증 세트에 대한 정확도만 살핌
    verbose=1 # 로깅 설정
)
```

```python
model.fit(X_train, y_train, validation_data=(X_test, y_test),
          epochs=10,
          callbacks=[checkpoint])

model.load_weights(checkpoint_path)
```

```
Epoch 10/10
1874/1875 [============================>.] - ETA: 0s - loss: 0.3647 - acc: 0.8685
Epoch 00010: val_loss improved from 0.41046 to 0.40474, saving model to my_checkpoint.ckpt
1875/1875 [==============================] - 4s 2ms/step - loss: 0.3646 - acc: 0.8686 - val_loss: 0.4047 - val_acc: 0.8530
<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0797d88f28>
```



```python
"""
테스트 세트로 evaluate 할 것!
하지만 우리는 테스트 세트를 검증 세트로 사용함(실제로는 그러면 안됨)
"""

"""
cat vs dog 문제
"""
import tensorflow_datasets as tfds
import tensorflow as tf

from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint

dataset_name = 'cats_vs_dogs'

train_dataset, info = tfds.load(name=dataset_name, split='train[:80%]', with_info=True)
valid_dataset, info = tfds.load(name=dataset_name, split='train[-20%:]', with_info=True)
```

```python
"""
이미지 리스케일링 및 전처리
"""
def preprocess(features):
  # 이미지 리스케일링
  img, lbl = tf.cast(features['image'], tf.float32) / 255.0, features['label']

  # 이미지 사이즈 조절 tf.image 기능을 사용
  img = tf.image.resize(img, size=(224, 224))

  return img, lbl
```

```python
"""
데이터 전처리
"""
batch_size = 32

# map: 자료구조에서 데이터를 하니씩 꺼내어 특정 함수를 적용
train_data = train_dataset.map(preprocess).batch(batch_size) # 전처리 후 배치로 묶기
valid_data = valid_dataset.map(preprocess).batch(batch_size) # 전처리 후 배치로 묶기
```

```python
total_image_size = info.splits['train'].num_examples
steps_per_epoch = int(total_image_size * 0.8) // batch_size + 1
validation_steps = int(total_image_size * 0.2) // batch_size + 1
```

```python
model = Sequential([
                    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)),
                    MaxPooling2D(2, 2), 
                    Conv2D(32, (3, 3), activation='relu'),
                    MaxPooling2D(2, 2), 
                    Conv2D(16, (3, 3), activation='relu'),
                    MaxPooling2D(2, 2), 
                    Flatten(), 
                    Dropout(0.3), 
                    Dense(512, activation='relu'),
                    Dense(128, activation='relu'),
                    Dense(2, activation='softmax')
])
```

```python
"""
RMSProp 사용하기
- 학습률을 학습 도중에 바꿀 수 있음
"""
# 현재 학습률에 대해 val_loss가 epoch를 3번 도는데 나아지지 않으면 학습률을 조정(RMSProp과 같이 사용됨)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=3,
    min_lr=0.00001,)

optimizer = tf.keras.optimizers.RMSprop(0.001)
```

```python
"""
모델 컴파일
"""
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
```

```python
"""
모델 훈련
"""
checkpoint_path = 'cat_dog_checkpoint.ckpt'

checkpoint = ModelCheckpoint(
    checkpoint_path,
    save_best_only=True,
    save_weights_only=True,
    monitor='val_loss',
    verbose=1
)

"""
# 원래 되는건데 갑자기 안 돼서 코드 변경
model.fit(
    train_data,
    validation_data=(valid_data),
    steps_per_epoch=steps_per_epoch,
    validation_steps=validation_steps,
    epochs=40,
    callbacks=[checkpoint, reduce_lr]
)
"""
# 따라서 수동으로 사이즈 지정
model.fit(
    train_data,
    validation_data=(valid_data),
    batch_size=32,
    epochs=40,
    callbacks=[checkpoint, reduce_lr]
)

model.load_weights(checkpoint_path)
```





## 오타 혹은 오류 찾는 방법

- 변수: 값 저장

1. 상수일 경우

> 대부분 상수(변경이 잘 되지 않음)이기 때문에 오타가 날 수 있는 문자열 같은 경우 특히 변수에 저장해서 쓰는 것이 좋음

1. 변수일 경우



- 오류가 났을 경우

1. 자바: 예외 발생 시 위에서 부터 아래 순서로 오류 확인(위 쪽에 에러 원인이 출력되어 있음), 그 후 자신이 만든 클래스만 확인

2. 파이썬: 오류 발생 시 아래에서 위 순서로 확인(아래쪽에 에러 원인이 출력되어 있음)