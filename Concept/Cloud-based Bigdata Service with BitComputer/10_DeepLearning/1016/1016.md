## 텍스트 전처리



```python
"""
Tokeniztion
- 어디까지가 문장이고 어디까지가 단어인 지 컴퓨터에게 알려주는 것
- 문장을 토큰화 시키기 위해 .을 기준으로 자르면 안 됨
- 단어를 단순하게 띄어쓰기로만 자르면  문제가 생김
"""
"""
Parsing
문자열을 내가 원하는 타입으로 바꾸는 것
- "100" -> 100
- {'hello':'hi'}
"""
sample_text = "Can you pay my bills? Can you pay my telephone bills? Do you pay my automo' bills? If you did then maybe we could chill. I don't think you do. So, you and me are through"

tokenized_sentence = sample_text.split(". ")
print(tokenized_sentence)
```

```
["Can you pay my bills? Can you pay my telephone bills? Do you pay my automo' bills? If you did then maybe we could chill", "I don't think you do", 'So, you and me are through']
```



```python
"""
한글/영어 둘 다 온점으로 구분되어 있는 문장이면 무리없이 토크나이징 가능
"""
tokenized_word = sample_text.split(" ")
print(tokenized_word)
```

```
['Can', 'you', 'pay', 'my', 'bills?', 'Can', 'you', 'pay', 'my', 'telephone', 'bills?', 'Do', 'you', 'pay', 'my', "automo'", 'bills?', 'If', 'you', 'did', 'then', 'maybe', 'we', 'could', 'chill.', 'I', "don't", 'think', 'you', 'do.', 'So,', 'you', 'and', 'me', 'are', 'through']
```



```python
"""
띄어쓰기로 영어 문장 내 단어 구분?
- We're Genius!!
- We are Genius!!
- We are Genius

특수문자 제거를 이용하여 단어 구분?
- $12.57
- Mr.So
- 192.168.56.101

형태소 분석기 사용
TreebankWordTokenizer - Penn Tree Tokenization 규칙
- 하이푼으로 구성 된 단어는 하나로 유지
- don't와 같이 어퍼스트로피로 접어가 함께 하는 단어는 분리
- I don't care -> I, do, n't, care
- I'm Iron-man -> I, 'm, Iron-man 
"""
import nltk
nltk.download('punkt')
```

```
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
True
```



```python
sentence = "Ain't nothin' sweeter, you want this sugar, don't ya?"

"""
기본 토크나이저
"""
from nltk.tokenize import word_tokenize
print(word_tokenize(sentence)) # 함수

"""
WordPunctTokenizer
- 어퍼스트로피를 따로 분리하지 않는 토크나이저
"""
from nltk.tokenize import WordPunctTokenizer
print(WordPunctTokenizer().tokenize(sentence)) # 클래스

"""
TreebankWordTokenizer
- 하이푼으로 연결 된 단어는 하나로 유지
- 어퍼스트로피로 접어가 함께하는 단어는 따로 분리
"""
from nltk.tokenize import TreebankWordTokenizer
tokenizer = TreebankWordTokenizer()
print(tokenizer.tokenize(sentence))

sample_text = "I'm Iron-man"
print(tokenizer.tokenize(sample_text))
```

```
['Ai', "n't", 'nothin', "'", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'do', "n't", 'ya', '?']
['Ain', "'", 't', 'nothin', "'", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'don', "'", 't', 'ya', '?']
['Ai', "n't", 'nothin', "'", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'do', "n't", 'ya', '?']
['I', "'m", 'Iron-man']
```



```python
"""
한국어 토큰화가 어려운 이유
1. 한국어는 교착어
- "커피를" 마시는 것 보다 "커피가" 키보드에 쏟아지는 것이 더 잠이 잘 깬다

2. 한국어는 띄어쓰기가 잘 지켜지지 않음
- 띄어쓰기를 쓰지 않아도 사람이 읽을 땐 잘 읽힘
- 영어는 띄어쓰기를 쓰지 않으면 이해를 하기 힘듦(때문에 잘 지키는 편)

3. 한국어는 주어 생략은 물론이고 어순도 중요하지 않음
- 나는 운동을 했어. 체육관에서
- 나는 체육관에서 운동을 했어.
- 나는 운동을 체육관에서 했어.
- 체육관에서 운동 했어.

4. 한자어라는 특성상 하나의 음절도 각자 다른 의미를 갖음
- 한국
"""
! pip install konlpy
```

```
...
Successfully installed JPype1-1.0.2 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0
```



```python
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
%cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh
```

```
...
Successfully Installed
Now you can use Mecab
from konlpy.tag import Mecab
mecab = Mecab()
```

> macab이라는 형태소 분석기



```python
from konlpy.tag import Hannanum, Kkma, Komoran, Okt, Mecab

hannanum = Hannanum()
kkma = Kkma()
komoran = Komoran()
okt = Okt()
mecab = Mecab()

sentence = "좋으니 그 사람 솔직히 견디기 버거워"
```

```python
"""
- nonus: 명사 추출
- morphs: 형태소 분리
- pos: 형태소 분리 및 설명
"""
# 트위터(okt)
print(okt.nouns(sentence))
print(okt.morphs(sentence))
print(okt.pos(sentence))

# 꼬꼬마(Kkma)
print(kkma.nouns(sentence))
print(kkma.morphs(sentence))
print(kkma.pos(sentence))

# 코모란(Komoran)
print(komoran.nouns(sentence))
print(komoran.morphs(sentence))
print(komoran.pos(sentence))

# 한나눔(Hannanum)
print(hannanum.nouns(sentence))
print(hannanum.morphs(sentence))
print(hannanum.pos(sentence))

# Mecab
print(mecab.nouns(sentence))
print(mecab.morphs(sentence))
print(mecab.pos(sentence))

```

```
['그', '사람']
['좋으니', '그', '사람', '솔직히', '견디기', '버거워']
[('좋으니', 'Adjective'), ('그', 'Noun'), ('사람', 'Noun'), ('솔직히', 'Adjective'), ('견디기', 'Verb'), ('버거워', 'Adjective')]
['사람']
['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버겁', '어']
[('좋', 'VA'), ('으니', 'ECD'), ('그', 'MDT'), ('사람', 'NNG'), ('솔직히', 'MAG'), ('견디', 'VV'), ('기', 'ETN'), ('버겁', 'VA'), ('어', 'ECS')]
['사람']
['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버거워']
[('좋', 'VA'), ('으니', 'EC'), ('그', 'MM'), ('사람', 'NNG'), ('솔직히', 'MAG'), ('견디', 'VV'), ('기', 'ETN'), ('버거워', 'NA')]
['사람', '버거워']
['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버거워']
[('좋', 'P'), ('으니', 'E'), ('그', 'M'), ('사람', 'N'), ('솔직히', 'M'), ('견디', 'P'), ('기', 'E'), ('버거워', 'N')]
['사람']
['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버거워']
[('좋', 'VA'), ('으니', 'EC'), ('그', 'MM'), ('사람', 'NNG'), ('솔직히', 'MAG'), ('견디', 'VV'), ('기', 'ETN'), ('버거워', 'VA+EC')]
```



```python
"""
정제(cleaning), 정규화(Normalization)

1. 정제: 불필요한 데이터를 제거하는 작업
- 한국어 분석에 집중하기 위해 숫자, 영어, 특수 기호들은 제거
- 은, 는, 이, 가와 같은 것들 제거 
- 띄어쓰기나 맞춤법을 확인해서 데이터를 깨끗하게 만드는 과정

- 정규표현식을 이용한 노이즈 데이터 제거
- 인코딩 문제 해결
- 등장 빈도가 적은 단어 제거(단어에 대한 중요도 판단)
    등장 빈도가 2회 이하면 해당 단어 제거
- 길이가 짧은 단어 제거
    영어의 경우 I, by, at ...
- 불용어 제거
    영어의 the는 정말 많이 나오지만 실제 의미가 없기 때문에 제거
    한글은 그럼, 윙하, 때, 있, 그것, 사실, 경우 ,어떤, 은, 는, 를 등등..



2. 정규화: 문장의 복잡도를 줄여주는 과정
- 보통 사람이 직접 규칙을 정의
- lemmatization
    am, are, were, was -> be
    has, had -> have
- 10, 158, 122 -> num(숫자가 별로 중요하지 않은 경우)
- ㅋ, ㅋㅋㅋ -> ㅋㅋ
- wow, wooow, Hmm, Hmmmm -> wow, hmm
- 대소문자 통합 등
"""

"""
영어 정규화 - Stemming
- 영문법의 규칙에 의한 단어를 단순화 시키는 과정

- 포터 스테머 규칙
    Serialize -> serial
    Allowance -> allow
    Medical -> medic
    This -> thi
"""
from nltk.stem import PorterStemmer

porterStemmer = PorterStemmer()
sentence = 'What time is it now? It is a quarter past nine by my watch.'

words = word_tokenize(sentence)
print(words)

print([porterStemmer.stem(w) for w in words]) # 다른 문장으로 하면 word_tokenize 보다 몇 개 변경 사항이 있을 수 있음
```

```
['What', 'time', 'is', 'it', 'now', '?', 'It', 'is', 'a', 'quarter', 'past', 'nine', 'by', 'my', 'watch', '.']
['what', 'time', 'is', 'it', 'now', '?', 'It', 'is', 'a', 'quarter', 'past', 'nine', 'by', 'my', 'watch', '.']
```



```python
"""
영어 정규화 - Lemmatization
뿌리 단어를 찾아서 바꿔 줌 -> 단어의 개수를 줄일 수 있음
"""
nltk.download('wordnet')
```

```
[nltk_data] Downloading package wordnet to /root/nltk_data...
[nltk_data]   Unzipping corpora/wordnet.zip.
True
```



```python
from nltk.stem import WordNetLemmatizer
wordnetlemmatizer = WordNetLemmatizer()

words = [
         'policy',
         'doing',
         'organization',
         'have',
         'are',
         'is',
         'am',
         'loves',
         'flew',
         'dies',
         'dead',
         'watched'
]

print([wordnetlemmatizer.lemmatize(w) for w in words])
```

```
['policy', 'doing', 'organization', 'have', 'are', 'is', 'am', 'love', 'flew', 'dy', 'dead', 'watched']
```



```python
"""
한국어 정규화 - Stemming, Normalization
"""
text = '4번 놀고있지. 4번은 팀워크가 없어. 4번은 개인주의야. 4번은 혼자밖에 생각하지 않아.'
print(okt.morphs(text)) # 형태소 분류
print(okt.morphs(text, stem=True)) # stemming: 어근 찾기

text = "웃기는 소리하넼ㅋㅋㅋㅋㅋ"
text2 = "웃기는 소리 하지마랔ㅋㅋㅋ"
print(okt.morphs(text))
print(okt.morphs(text, norm=True))
print(okt.morphs(text, norm=True, stem=True))
print(okt.morphs(text2))
print(okt.morphs(text2, norm=True))
print(okt.morphs(text2, norm=True, stem=True)) # 하지말라는 것을 "웃기는 소리 하다"로 변형시키기 때문에 조심해야 함
```

```
['4', '번', '놀고있지', '.', '4', '번은', '팀워크', '가', '없어', '.', '4', '번은', '개인주의', '야', '.', '4', '번은', '혼자', '밖에', '생각', '하지', '않아', '.']
['4', '번', '놀다', '.', '4', '번은', '팀워크', '가', '없다', '.', '4', '번은', '개인주의', '야', '.', '4', '번은', '혼자', '밖에', '생각', '하다', '않다', '.']
['웃기는', '소리', '하', '넼', 'ㅋㅋㅋㅋㅋ']
['웃기는', '소리', '하네', 'ㅋㅋㅋ']
['웃기다', '소리', '하다', 'ㅋㅋㅋ']
['웃기는', '소리', '하지마', '랔', 'ㅋㅋㅋ']
['웃기는', '소리', '하지마라', 'ㅋㅋㅋ']
['웃기다', '소리', '하다', 'ㅋㅋㅋ']
```



```python
# soynlp를 이용하여 반복 분자 정제
!pip install soynlp
```

```
...
Successfully installed soynlp-0.0.493
```



```python
from soynlp.normalizer import emoticon_normalize

print(emoticon_normalize("앜ㅋㅋㅋㅋ 이 영화 쥰내 재미쓰뮤ㅠㅠㅠ", num_repeats=2))
print(emoticon_normalize("앜ㅋㅋㅋㅋ 이 영화 쥰내 재미씀ㅠㅠㅠ", num_repeats=2))
print(emoticon_normalize("앜ㅋㅋㅋㅋ 이 영화 쥰내 재미써ㅠㅠㅠ", num_repeats=2))

from soynlp.normalizer import repeat_normalize
print(repeat_normalize("호에에에에에에에에엥", num_repeats=2))
print(repeat_normalize("호에에에에엥", num_repeats=2))
print(repeat_normalize("호에에에엥", num_repeats=2)) # 에가 3개 나오는데 아마 반복이 부족해서 그런 듯
```

```
아ㅋㅋ 이 영화 쥰내 재미쓰뮤ㅠㅠㅠ
아ㅋㅋ 이 영화 쥰내 재미씀ㅠㅠㅠ
아ㅋㅋ 이 영화 쥰내 재미써ㅠㅠㅠ
호에에엥
호에에엥
호에에에엥
```



```
!pip install git+https://github.com/haven-jeon/PyKoSpacing.git
!pip install git+https://github.com/ssut/py-hanspell.git
```

```
Successfully built py-hanspell
Installing collected packages: py-hanspell
Successfully installed py-hanspell-1.1
```



```python
# KoSpacing 활용하기
from pykospacing import spacing
text = '4번 놀고있지. 4번은 팀워크가 없어. 4번은 개인주의야. 4번은 혼자밖에 생각하지 않아.'
spacing_text = spacing(text)
print(spacing_text)

from hanspell import spell_checker

sentence = '마춤뻡 틀리면 외 안되'
spelled_sent = spell_checker.check(sentence)

hansell_sentence = spelled_sent.checked
print(hansell_sentence)

print(text)

kospacing_text = spacing(text) # kospacing 활용하여 띄어쓰기 교정
hanspell_text = spell_checker.check(text).checked
print(kospacing_text)
print(hanspell_text)
```

```
4번 놀고 있지. 4번은 팀워크가 없어. 4번은 개인주의야. 4번은 혼자 밖에 생각하지 않아.
맞춤법 틀리면 외 안돼
4번 놀고있지. 4번은 팀워크가 없어. 4번은 개인주의야. 4번은 혼자밖에 생각하지 않아.
4번 놀고 있지. 4번은 팀워크가 없어. 4번은 개인주의야. 4번은 혼자 밖에 생각하지 않아.
4번 놀고 있지. 4번은 팀워크가 없어. 4번은 개인주의야. 4번은 혼자밖에 생각하지 않아.
```



```python
"""
텍스트 정제

정규 표현식 사용하기
- 영어만 추출하는 정규식 표현 
    [a-zA-Z]
- 한글 추출 정규식 
    [ㄱ-ㅎㅏ-ㅣ가-힣]
    [ㄱ-ㅎ가-힣]
    [가-힣]
"""
import re
eng_sent = 'hi 1 2 3 !!! hello bye FAQ'
print(eng_sent)

eng_sent = re.sub("[^a-zA-Z]", " ", eng_sent)
print(eng_sent)

eng_sent = ' '.join([w for w in eng_sent.split() if len(w) > 2])
print(eng_sent)
```

```
hi 1 2 3 !!! hello bye FAQ
hi           hello bye FAQ
hello bye FAQ
```



```python
"""
Stopwords 설정하기
불용어 - 자주 등장하는데 실제 의미 분석을 하는데 있어 별 필요 없는 단어들

영어 stopwords
"""
nltk.download('stopwords')
```

```
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
True
```



```python
from nltk.corpus import stopwords

print(stopwords.words('english'))

example_text = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words('english'))

word_tokens = word_tokenize(example_text)
print(word_tokens)

result = []

for w in word_tokens:
  if w not in stop_words:
    result.append(w)

  print("원문 : ", word_tokens)
  print("불용어 제거 후 : ", result)
```

```
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]
['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family']
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family']
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family']
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family']
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family', 'important']
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family', 'important', 'thing']
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family', 'important', 'thing', '.']
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family', 'important', 'thing', '.', 'It']
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family', 'important', 'thing', '.', 'It', "'s"]
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family', 'important', 'thing', '.', 'It', "'s", 'everything']
원문 :  ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 :  ['Family', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
```



```python
"""
한국어 stopwords
"""
example = "와 이런 것도 영화라고 차라리 뮤직비디오를 만드는게 나을 뻔"
word_tokens = okt.morphs(example)
print(word_tokens)

stop_words = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', '것']
result = [word for word in word_tokens if not word in stop_words]
print("원문: ", word_tokens)
print("불용어 제거 후 : ", result)
```

```
['와', '이런', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만드는게', '나을', '뻔']
원문:  ['와', '이런', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만드는게', '나을', '뻔']
불용어 제거 후 :  ['이런', '영화', '라고', '차라리', '뮤직비디오', '만드는게', '나을', '뻔']
```





## 텍스트 분석을 위한 인코딩

```python
"""
# 텍스트의 수치화
1. Integer Encoding
- 단어 토큰화 이후에 각 단어에 고유한 정수를 부여
- 중복이 허용되지 않는 모든 단어들의 집합을 만들기

예시)
문장1 - 오늘 점심 수제비
문장2 - 수제비가 오늘
문장3 - 내일은 점심에 짬뽕
문장4 - 저녁은 소고기
오늘(1) 점심(2) 수제비(3) 내일(4) 짬뽕(5) 저녁(6) 소고기(7)

문장1 [1, 2, 3]
문장2 [3, 1]
문장3 [4, 6]
문장4 [7, 8]

패딩처리 후(부족한 부분 0으로 채우기)
문장1 [1, 2, 3]
문장2 [3, 1, 0]
문장3 [4, 0, 6]
문장4 [7, 8, 0]

padding
- 모든 문장에 대해 정수 인코딩을 수행했을 때 문장마다의 길이가 다를 수 있음
- 가상의 단어를 만들어 줌(보통 <PAD>로 만들고 0 추가)

# 텍스트의 벡터화(vectorization)
one-hot encoding
DTM (document term matrix)
  문장1 - 오늘 수제비를 먹었는데 어제 먹은 수제비 보다 짬뽕맛이 낫다
  오늘(1회) 점심(0회) 수제비(2) 내일(0) 짬뽕(1) 저녁(0) 소고기(0) OOV(단어 집합이 없는 개수 4개)
TF-IDF
  단어빈도(term frequency) - 역(Inverse) 문서 빈도(Document Frequency)
  TF값과 IDF를 곱한 것
  문서의 유사도와 검색 시스템에서 결과의 순위를 구하는 일 등에 사용 됨
  인공 신경망의 입력으로 자주 사용 됨
"""
import nltk
nltk.download('punkt')
nltk.download('stopwords')
```

```
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
True
```



```python
text = "A barber is a person. a barber is good person. a barber is huge person. \
        he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. \
        His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. \
        the barber went up a huge mountain."

from nltk.tokenize import sent_tokenize

text_token = sent_tokenize(text)
print(text_token)

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

sentences = []
stop_words = set(stopwords.words('english')) # NLTK 불용어 불러오기
for i in text_token:
  sentence = word_tokenize(i) # 단어 토큰화
  result = []

  for word in sentence:
    word = word.lower()
    if word not in stop_words: # 불용어 제거
      if len(word) > 2: # 단어가 2 이하인 경우 추가적으로 제거
        result.append(word)
  sentences.append(result)
print(sentences)
```

```
['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']
[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]
```



```python
# 가장 기초적인 방법으로 단어 집합 만들기
from collections import Counter

words = sum(sentences, [])
print(words)

"""
단어 집합을 만들 때 여러가지 기준 존재
1. 단어의 빈도수(많이 나올 수록 앞쪽에 등장)
2. 가나다 순으로 저장
3. 그냥 들어가는 대로 넣어도 상관은 없음(자주 사용되는 방법은 아님)
"""
vocab = Counter(words)
print(vocab)
print(vocab['barber'])
```

```
['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']
Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})
8
```



```python
"""
Integer Encoding
1. 빈도수가 높은 순서대로 정렬
2. 높을 수록 낮은 번호 부여
3. 빈도수가 낮은 단어들을 제외시키면서 단어 집합의 크기를 조절하기 편함
"""
# 빈도수가 높은 순서대로 정렬하기
vocab_sorted = sorted(vocab.items(), key=lambda x : x[1], reverse=True)
print(vocab_sorted)

# 높은 빈도수를 가진 단어일 수록 낮은 정수 인덱스 부여하기
word2idx = {}
i = 0

# 각 단어의 빈도수 불러오기
for (word, frequency) in vocab_sorted:
  # 단어의 빈도가 2개 이상인 것만 단어집합에 추가
  if frequency > 1:
    i = i + 1
    word2idx[word] = i
print(word2idx)
```

```
[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]
{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}
```



```python
"""
만들어진 단어 집합에서 상위 5개의 단어만 사용하도록 하기
"""
vocab_size = 5 # 딥러닝 모델링을 할 때 Input Data의 형상이 됨
words_frequency = [w for w, c in word2idx.items() if c >= vocab_size + 1] # 인덱스가 5 초과인 단어 제거
for w in words_frequency:
  del word2idx[w] # 위에서 선택된 단어 제거
print(word2idx)

"""
OOV 추가
Out Of Vocabulary

단어 사전 제일 뒤에 OOV 또는 UNK 토큰을 추가해 줌
"""
word2idx['UNK'] = 6
print(word2idx)

print(sentences)
encoded = [] # 정수 인코딩 된 리스트를 저장
for s in sentences:
  temp = []
  for w in s:
    try:
      temp.append(word2idx[w])
    except KeyError:
      temp.append(word2idx['UNK'])
  encoded.append(temp)
print(encoded)
```

```
{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}
{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'UNK': 6}
[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]
[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]
```



```python
"""
단어 집합 생성 및 정수 인코딩을 Tensorflow로
- keras.preprocessing.text.Tokenizer를 이용
"""
from tensorflow.keras.preprocessing.text import Tokenizer

print(sentences)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences) # 빈도수를 기준으로 단어 집합 생성
print(tokenizer.word_index)

print(tokenizer.word_counts)
print(tokenizer.texts_to_sequences(sentences))

vocab_size = 5
tokenizer = Tokenizer(num_words=vocab_size + 1) # 상위 5개의 단어만 사용, 0은 패딩을 위한 자리이기 때문에 1을 더해 줌
tokenizer.fit_on_texts(sentences)
tokenizer.texts_to_sequences(sentences)

vocab_size = 5
# 빈도수 상위 5개 단어만 사용
# 숫자 0과 OOV를 고려해야 하기 때문에 단어 집합의 크기는 vocab_size + 2가 되어야 함
tokenizer = Tokenizer(num_words=vocab_size + 2, oov_token='OOV')
tokenizer.fit_on_texts
print(tokenizer.texts_to_sequences(sentences))
print(tokenizer.word_index)
print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))
```



```python
"""한국어 Tokeniztion & Integer Encoding까지 Tensorflow로"""
!pip install konlpy
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
%cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh
```

```
Successfully Installed
Now you can use Mecab
from konlpy.tag import Mecab
mecab = Mecab()
```



```python
import pandas as pd
import numpy as np
import urllib.request
from tensorflow.keras.preprocessing.text import Tokenizer
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt", filename="ratings_test.txt")
```

```
('ratings_test.txt', <http.client.HTTPMessage at 0x7f496bdeba90>)
```



```python
train_data = pd.read_table('ratings_test.txt')
train_data.info()
train_data.head()
train_data['document'].nunique()
# 중복 데이터 제거 및 null 값 제거
train_data.drop_duplicates(subset=['document'], inplace=True) # 중복 제거
train_data = train_data.dropna(how='any') # Null값이 존재하는 행 제거

len(train_data)

train_data['document'] = train_data['document'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]", "")
train_data.head()

train_data['document'].replace('', np.nan, inplace=True)
print(train_data.isnull().sum())

# document가 Nan값인 행 날리기
train_data = train_data.dropna(how='any')
print(len(train_data))

stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']
```



```python
from konlpy.tag import Okt

okt = Okt()
okt.morphs("와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔", stem=True)

"""
형태소 분리르 이용한 토큰화 후 불용어 처리까지
"""
X_train = []
for sentence in train_data['document']:
  temp_X = []
  temp_X = okt.morphs(sentence, stem=True) # 토큰화
  temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거
  X_train.append(temp_X)
```

```python
X_train[:10]
```

```
...
  '이다'],
 ['이별', '아픔', '뒤', '찾아오다', '새롭다', '인연', '기쁨', '모든', '사람', '그렇다', '않다'],
 ['괜찮다', '오랜', '만', '포켓몬스터', '잼밌', '어', '요']]
```



```python
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
print(tokenizer.word_index)
print(tokenizer.word_counts)
print(tokenizer.word_counts['영화'])
print(tokenizer.word_counts['보다'])
```

```
...
17231
13796
```



```python
"""
인코딩 수행하기
인코딩: 딥러닝에서의 데이터 변환(내가 원하는 형태의 숫자로 바꾸는 것)
디코딩: 원본으로 복구(변환)
"""
encoded = tokenizer.texts_to_sequences(X_train)
X_train[:5]

print(encoded[:5])

print(tokenizer.word_index['굳다'])
print(tokenizer.word_index['지루하다'])
```

```
[[613, 91], [64, 167, 23, 394, 19, 20, 279, 775, 44, 860, 17], [66, 19, 89, 379, 109, 111, 60, 150, 245], [13, 17, 126, 1783, 110, 60, 36, 18, 24, 175, 14705, 796], [225, 3158, 11, 28, 225, 1]]
613
66
```





## 시계열

1. 순서가 바뀌면 의미가 아예 바뀌는 것

> 유전자 염기서열 테스트

2. 연속적인 시가에 관측된 값
3. 특정 시간에 관측한 값