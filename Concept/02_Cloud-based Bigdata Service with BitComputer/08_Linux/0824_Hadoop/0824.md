

## ariline_dep_delay_output 실습 이어서

> 프로젝트가 임포트 안 되는 등의 프로세스가 꼬일 경우
>
> 프로젝트 - 클린을 해줌

```
login as: bit44
bit44@127.0.0.1's password:
Last login: Mon Aug 24 09:08:20 2020
[bit44@hadoopnode01 ~]$ cd hadoop
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ sbin/start-dfs.sh
.
.
.
[bit44@hadoopnode01 ~/hadoop]$ jps
3457 DataNode
3331 NameNode
3667 SecondaryNameNode
3811 Jps
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ sbin/start-yarn.sh
.
.
.
[bit44@hadoopnode01 ~/hadoop]$ jps
3457 DataNode
3331 NameNode
3667 SecondaryNameNode
4295 Jps
3865 ResourceManager
3981 NodeManager
[bit44@hadoopnode01 ~/hadoop]$ sbin/yarn-daemon.sh start proxyserver
.
.
.
[bit44@hadoopnode01 ~/hadoop]$ sbin/mr-jobhistory-daemon.sh start historyserver
.
.
.
[bit44@hadoopnode01 ~/hadoop]$ jps
3457 DataNode
3331 NameNode
3667 SecondaryNameNode
4339 WebAppProxyServer
3865 ResourceManager
3981 NodeManager
4398 JobHistoryServer
4463 Jps
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls airline_dep_delay_output
Found 2 items
-rw-r--r--   1 bit44 supergroup          0 2020-08-21 13:29 airline_dep_delay_output/_SUCCESS
-rw-r--r--   1 bit44 supergroup        171 2020-08-21 13:29 airline_dep_delay_output/part-r-00000
[bit44@hadoopnode01 ~/hadoop]$



[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -cat airline_dep_delay_output/part-r-00000
2012,1  151762
2012,10 183397
2012,11 161643
2012,12 207041
2012,2  136280
2012,3  187828
2012,4  153453
2012,5  175546
2012,6  205523
2012,7  232461
2012,8  215957
2012,9  158063
[bit44@hadoopnode01 ~/hadoop]$



여기로 결과를 아웃풋 하겠다
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -get airline_dep_delay_output/part-r-00000 ../test01/



[bit44@hadoopnode01 ~/hadoop]$ ls -l ../test01
합계 30100
-rw-rw-r--. 1 bit44 bit44      201  8월 18 13:31 Makefile
-rw-rw-r--. 1 bit44 bit44       28  8월 18 10:39 awk-file
-rw-rw-r--. 1 bit44 bit44      163  8월 18 09:16 awkfile
-rwxrwxr-x. 1 bit44 bit44     8432  8월 18 13:31 cal_main
-rw-rw-r--. 1 bit44 bit44      174  8월 18 13:31 cal_main.c
-rw-rw-r--. 1 bit44 bit44     1608  8월 18 13:31 cal_main.o
-rw-rw-r--. 1 bit44 bit44       23  8월 14 10:22 hfile
-rw-rw-r--. 1 bit44 bit44       48  8월 18 11:41 int_add.c
-rw-rw-r--. 1 bit44 bit44     1248  8월 18 11:41 int_add.o
drwxrwxr-x. 2 bit44 bit44       43  8월 14 11:18 nest01
-rw-rw-r--. 1 bit44 bit44 24770560  8월 14 11:07 nest01.tar
-rw-rw-r--. 1 bit44 bit44  5962746  8월 14 11:20 nest01.tar.gz
-rw-r--r--. 1 bit44 bit44      171  8월 24 09:27 part-r-00000
-rwxrwxr-x. 1 bit44 bit44       62  8월 14 14:01 test01.sh
-rwxrwxr-x. 1 bit44 bit44       50  8월 14 14:16 test02.sh
-rwxrwxr-x. 1 bit44 bit44       48  8월 14 14:19 test03.sh
-rwxrwxr-x. 1 bit44 bit44       49  8월 14 14:21 test04.sh
-rwxrwxr-x. 1 bit44 bit44       52  8월 14 14:26 test05.sh
-rwxrwxr-x. 1 bit44 bit44      213  8월 14 14:54 test06.sh
-rwxrwxr-x. 1 bit44 bit44       55  8월 14 15:07 test07.sh
-rwxrwxr-x. 1 bit44 bit44      212  8월 14 15:26 test08.sh
-rw-rw-r--. 1 bit44 bit44       18  8월 18 10:10 testdd




[bit44@hadoopnode01 ~/hadoop]$ cat ../test01/part*
2012,1  151762
2012,10 183397
2012,11 161643
2012,12 207041
2012,2  136280
2012,3  187828
2012,4  153453
2012,5  175546
2012,6  205523
2012,7  232461
2012,8  215957
2012,9  158063




128메가를 오바하게 되면 파트를 쪼개서 결과를 내놓음
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls airline_dep_delay_output
Found 2 items
-rw-r--r--   1 bit44 supergroup          0 2020-08-21 13:29 airline_dep_delay_output/_SUCCESS
-rw-r--r--   1 bit44 supergroup        171 2020-08-21 13:29 airline_dep_delay_output/part-r-00000


[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -cp airline_dep_delay_output/part-r-00000 airline_dep_delay_output/part-r-00001



*(와일드카드)로 모든 것을 다 가져옴(지금은 파일이 있으므로 지우고 나서 확인하면 됨)
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -get airline_dep_out airline_dep_delay_output/part-r-* ../test01/
get: `airline_dep_out': No such file or directory
get: `../test01/part-r-00000': File exists




두 개의 내용을 합쳐서 하나의 내용으로 출력(파일명을 줘야함)
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -getmerge airline_dep_delay_output ../test01/part-merge

```





## 도착 지연 산출 실습

매퍼와 드라이버 새로 짜기

> 출발을 도착으로 바꾸는 것과 동일

> 파서는 재사용 가능하고 리듀스는 지연 시간이므로 그대로 사용 가능



- 이클립스

> 기존 airline에서 재사용

> 겹치는 클래스(다른 이름/유사 작업 작업)는 딜리트함
>
> *****하지만 실습 내용을 확인하니 딜리트 할 필요가 없음*****

```java
package com.adacho.hadoop.common;

import org.apache.hadoop.io.Text;

public class AirlinePerformanceParser {
	
	private int year;
	private int month;
	private int day;
	
	private int arriveDelayTime = 0;
	private int departureDelayTime = 0;
	private int distance = 0;
	
	private boolean arriveDelayAvailable = true;
	private boolean departureDelayAvailable = true;
	private boolean distanceAvailable = true;
	
	private String uniqueCarrier;
	
	
	public AirlinePerformanceParser(Text text) {
		try {
			String[] columns = text.toString().split(",");
			
			year = Integer.parseInt(columns[0]);
			month = Integer.parseInt(columns[1]);
			day = Integer.parseInt(columns[2]);
			uniqueCarrier = columns[5];
			
			if(!columns[17].equals("")) {
				departureDelayTime = (int)Float.parseFloat(columns[17]);
			} else {
				departureDelayAvailable = false;
			}
			
			if(!columns[27].equals("")) {
				arriveDelayTime = (int)Float.parseFloat(columns[27]);
			} else {
				arriveDelayAvailable = false;
			}
			
			if(!columns[37].equals("")) {
				distance = (int)Float.parseFloat(columns[37]);
			} else {
				distanceAvailable = false;
			}
			
		} catch (Exception e) {
			// TODO: handle exception
			System.out.println("Error parsing a record : " + e.getMessage());
		}
	}


	/**
	 * @return the year
	 */
	public int getYear() {
		return year;
	}


	/**
	 * @return the month
	 */
	public int getMonth() {
		return month;
	}


	/**
	 * @return the arriveDelayTime
	 */
	public int getArriveDelayTime() {
		return arriveDelayTime;
	}


	/**
	 * @return the departureDelayTime
	 */
	public int getDepartureDelayTime() {
		return departureDelayTime;
	}


	/**
	 * @return the distance
	 */
	public int getDistance() {
		return distance;
	}


	/**
	 * @return the arriveDelayAvailable
	 */
	public boolean isArriveDelayAvailable() {
		return arriveDelayAvailable;
	}


	/**
	 * @return the departureDelayAvailable
	 */
	public boolean isDepartureDelayAvailable() {
		return departureDelayAvailable;
	}


	/**
	 * @return the distanceAvailable
	 */
	public boolean isDistanceAvailable() {
		return distanceAvailable;
	}


	/**
	 * @return the uniqueCarrier
	 */
	public String getUniqueCarrier() {
		return uniqueCarrier;
	}
	
}

```

```java
package com.adacho.hadoop.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import com.adacho.hadoop.mapper.ArrivalDelayCountMapper;
import com.adacho.hadoop.reducer.DelayCountReducer;

public class ArrivalDelayCount {
	public static void main(String[] args) throws Exception{
		
		Configuration conf = new Configuration();
		
		if(args.length != 2) {
			System.out.println("Usage: ArrivalDelayCount <input> <output>");
			System.exit(2);
		}
		
		
		Job job = Job.getInstance(conf, "ArrivalDelayCount");
		
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		
		job.setJarByClass(ArrivalDelayCount.class);
		job.setMapperClass(ArrivalDelayCountMapper.class);
		job.setReducerClass(DelayCountReducer.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		job.waitForCompletion(true);
	}
}

```

```java
package com.adacho.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.adacho.hadoop.common.AirlinePerformanceParser;

public class ArrivalDelayCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
	
	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputKey = new Text();
	
	
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.map(key, value, context);
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		
		outputKey.set(parser.getYear() + "," + parser.getMonth());
		
		if(parser.getArriveDelayTime() > 0) {
			context.write(outputKey, outputValue);
		}
		
	}
	
}

```

```java
package com.adacho.hadoop.reducer;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class DelayCountReducer extends Reducer<Text, IntWritable, Text, IntWritable>{
	
	private IntWritable result = new IntWritable();

	@Override
	protected void reduce(Text key, Iterable<IntWritable> values,
			Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.reduce(arg0, arg1, arg2);
		int sum = 0;
		
		for(IntWritable value : values) {
			sum += value.get();
		}
		result.set(sum);
		context.write(key, result);
	}
}

```



- 메이븐 실행

프로젝트파일 우클릭 -> 런애즈 -> 메이븐 빌드 ... -> 골: clean install -> 런 -> BUILD SUCCESS 확인



- 하둡

D:\KHY\airline\target 확인

> 프로젝트파일 우클릭 - 프로퍼티 - 리소스 - 경로의 .jar를 share의 java 폴더에 복붙(기존에 있었으므로 덮어쓰기)

D:\KHY_OracleVBox\VBoxShare\download에 airOT201201~12.csv 파일 넣기

> 이미 되어있으므로 패스

```
[bit44@hadoopnode01 ~/hadoop]$ bin/yarn jar /mnt/share/java/airline-0.0.1.jar com.adacho.hadoop.driver.ArrivalDelayCount airline_dep_delay_input airline_arr_delay_output



20/08/24 10:13:31 INFO client.RMProxy: Connecting to ResourceManager at hadoopnode01/10.0.2.15:8032
20/08/24 10:13:31 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
20/08/24 10:13:32 INFO input.FileInputFormat: Total input paths to process : 12
20/08/24 10:13:32 INFO mapreduce.JobSubmitter: number of splits:12
20/08/24 10:13:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1598228097388_0001
20/08/24 10:13:33 INFO impl.YarnClientImpl: Submitted application application_1598228097388_0001
20/08/24 10:13:33 INFO mapreduce.Job: The url to track the job: http://0.0.0.0:8090/proxy/application_1598228097388_0001/
20/08/24 10:13:33 INFO mapreduce.Job: Running job: job_1598228097388_0001
20/08/24 10:13:47 INFO mapreduce.Job: Job job_1598228097388_0001 running in uber mode : false
20/08/24 10:13:47 INFO mapreduce.Job:  map 0% reduce 0%
20/08/24 10:14:27 INFO mapreduce.Job:  map 1% reduce 0%
20/08/24 10:14:28 INFO mapreduce.Job:  map 2% reduce 0%
20/08/24 10:14:29 INFO mapreduce.Job:  map 4% reduce 0%
20/08/24 10:14:31 INFO mapreduce.Job:  map 6% reduce 0%
20/08/24 10:14:32 INFO mapreduce.Job:  map 7% reduce 0%
20/08/24 10:14:34 INFO mapreduce.Job:  map 8% reduce 0%
20/08/24 10:14:36 INFO mapreduce.Job:  map 12% reduce 0%
20/08/24 10:14:37 INFO mapreduce.Job:  map 14% reduce 0%
20/08/24 10:14:39 INFO mapreduce.Job:  map 16% reduce 0%
20/08/24 10:14:40 INFO mapreduce.Job:  map 19% reduce 0%
20/08/24 10:14:42 INFO mapreduce.Job:  map 21% reduce 0%
20/08/24 10:14:43 INFO mapreduce.Job:  map 23% reduce 0%
20/08/24 10:14:44 INFO mapreduce.Job:  map 24% reduce 0%

```

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -cat airline_arr_delay_output/part-r-00000
2012,1  149036
2012,10 188527
2012,11 150009
2012,12 201482
2012,2  138349
2012,3  183040
2012,4  150954
2012,5  171789
2012,6  197091
2012,7  224610
2012,8  208734
2012,9  162147
[bit44@hadoopnode01 ~/hadoop]$

```





## 사용자 정의 옵션

- 사용자 정의옵션 pdf 참조



- 자바

```java
package com.adacho.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.adacho.hadoop.common.AirlinePerformanceParser;

public class DelayCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{

	private String workType;
	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputkey = new Text();
	
	
	@Override
	protected void setup(Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.setup(context);
		// setup()은 Mapper가 생성될 때 단 한 번만 실행
		workType = context.getConfiguration().get("workType");
	}


	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.map(key, value, context);
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		
		outputkey.set(parser.getYear() + "," + parser.getMonth());
		
		if(workType.equals("departure")) {
			if(parser.getDepartureDelayTime() > 0) {
				context.write(outputkey, outputValue);
			}
		} else if(workType.equals("arrival")) {
			if(parser.getArriveDelayTime() > 0) {
				context.write(outputkey, outputValue);
			}
		}
	}
	
	
}

```

```java
package com.adacho.hadoop.driver;


import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import com.adacho.hadoop.mapper.DelayCountMapper;
import com.adacho.hadoop.reducer.DelayCountReducer;


public class DelayCount extends Configured implements Tool {
	
	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new DelayCount(), args);
		System.out.println("Mr-Job Result: " + res);
	}

	
	// Tool의 추상메서드 구현
	@Override
	public int run(String[] args) throws Exception {
		// TODO Auto-generated method stub
		// GenericOptionsParser에서 제공하는 파라미터를 제외한 나머지 파라미터를 가져온다.
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();
		
		if(otherArgs.length != 2) {
			System.out.println("Usage: DelayCount <in> <out>");
			System.exit(2);
		}
		
		Job job = Job.getInstance(getConf(), "DelayCount");
		
		job.setJarByClass(DelayCount.class);
		job.setMapperClass(DelayCountMapper.class);
		job.setReducerClass(DelayCountReducer.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		
		job.waitForCompletion(true);
		return 0;
	}
	
	
}

```



- 메이븐 실행

프로젝트파일 우클릭 -> 런애즈 -> 메이븐 빌드 ... -> 골: clean install -> 런 -> BUILD SUCCESS 확인



- 하둡

D:\KHY\airline\target 확인

> 프로젝트파일 우클릭 - 프로퍼티 - 리소스 - 경로의 .jar를 share의 java 폴더에 복붙(기존에 있었으므로 덮어쓰기)

D:b\KHY_OracleVBox\VBoxShare\download에 airOT201201~12.csv 파일 넣기

> 이미 되어있으므로 패스

```
[bit44@hadoopnode01 ~/hadoop]$ bin/yarn jar /mnt/share/java/airline-0.0.1.jar com.adacho.hadoop.driver.DelayCount -D workType=departure airline_dep_delay_input dep_delay_output
.
.
.
                Physical memory (bytes) snapshot=3094564864
                Virtual memory (bytes) snapshot=27074772992
                Total committed heap usage (bytes)=2272722944
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=1364564442
        File Output Format Counters
                Bytes Written=171
Mr-Job Result: 0

```

> 파일 잘못 생성하여 아웃풋 디렉토리가 만들어졌을 경우
>
> bin/hdfs dfs -rm -r dep_delay_output
> 또는
> bin/hdfs dfs -rmdir dep_delay_output

> jps 확인해서 프로세스 죽이기
>
> kill 프로세스아이디

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls dep_delay_output
Found 2 items
-rw-r--r--   1 bit44 supergroup          0 2020-08-24 18:32 dep_delay_output/_SUCCESS
-rw-r--r--   1 bit44 supergroup        171 2020-08-24 18:32 dep_delay_output/part-r-00000
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -cat dep_delay_output/part-r-00000
2012,1  151762
2012,10 183397
2012,11 161643
2012,12 207041
2012,2  136280
2012,3  187828
2012,4  153453
2012,5  175546
2012,6  205523
2012,7  232461
2012,8  215957
2012,9  158063
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -cat airline_dep_delay_output/part-r-00000
2012,1  151762
2012,10 183397
2012,11 161643
2012,12 207041
2012,2  136280
2012,3  187828
2012,4  153453
2012,5  175546
2012,6  205523
2012,7  232461
2012,8  215957
2012,9  158063
[bit44@hadoopnode01 ~/hadoop]$

```





### Counter

1. 하둡은 맵리듀스 잡의 진행 상황을 모니터링 할 수 있게 Counter라는 API를 제공

2. 모든 잡은 다수의 내장 카운터를 가지고 있음

3. 내장 카운터는 맵 리듀스 콤바이너의 입출력 레코드 건수와 바이트를 확인하며, 몇 개의 맵 태스크와 리듀스 태스크가 실행되고 실패했는지, 파일 시스템에서는 얼마나 많은 바이트를 읽고 썼는지에 대한 정보를 제공

4. 이러한 내장 카운터의 값은 잡을 실행하면 콘솔 화면에 출력되는 로그에 나타남



- Enum 클래스

```java
package com.adacho.hadoop.counter;

public enum DelayCounters {

	// delay는 결과로 나오므로 코드를 작성하지 않음
	not_available_arrival, scheduled_arrival, early_arrival, not_available_departure, 
	scheduled_departure, early_departure
}

```

```java
package com.adacho.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.adacho.hadoop.common.AirlinePerformanceParser;
import com.adacho.hadoop.counter.DelayCounters;

public class DelayCountMapperWithCounter extends Mapper<LongWritable, Text, Text, IntWritable>{

	private String workType;
	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputKey = new Text();
	
	
	@Override
	protected void setup(Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.setup(context);
		workType = context.getConfiguration().get("workType");
	}


	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.map(key, value, context);
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		
		if(workType.equals("departure")) {
			if(parser.isDepartureDelayAvailable()) {
				if(parser.getDepartureDelayTime() > 0) {
					outputKey.set(parser.getYear() + "," + parser.getMonth());
					context.write(outputKey, outputValue);
				} else if(parser.getDepartureDelayTime() == 0) {
					context.getCounter(DelayCounters.scheduled_departure).increment(1);
				} else if(parser.getDepartureDelayTime() < 0) {
					context.getCounter(DelayCounters.early_departure).increment(1);
				}
			} else {
				context.getCounter(DelayCounters.not_available_departure).increment(1);
			}
		} else if(workType.equals("arrival")) {
			if(parser.isArriveDelayAvailable()) {
				if(parser.getArriveDelayTime() > 0) {
					outputKey.set(parser.getYear() + "," + parser.getMonth());
					context.write(outputKey, outputValue);
				} else if(parser.getArriveDelayTime() == 0) {
					context.getCounter(DelayCounters.scheduled_arrival).increment(1);
				} else if(parser.getArriveDelayTime() < 0) {
					context.getCounter(DelayCounters.early_arrival).increment(1);
				}
			} else {
				context.getCounter(DelayCounters.not_available_arrival).increment(1);
			}
		}
	}
	
	
	
	
}

```

```java
package com.adacho.hadoop.common;

import org.apache.hadoop.io.Text;

public class AirlinePerformanceParser {
	
	private int year;
	private int month;
	private int day;
	
	private int arriveDelayTime = 0;
	private int departureDelayTime = 0;
	private int distance = 0;
	
	private boolean arriveDelayAvailable = true;
	private boolean departureDelayAvailable = true;
	private boolean distanceAvailable = true;
	
	private String uniqueCarrier;
	
	
	public AirlinePerformanceParser(Text text) {
		try {
			String[] columns = text.toString().split(",");
			
			year = Integer.parseInt(columns[0]);
			month = Integer.parseInt(columns[1]);
			day = Integer.parseInt(columns[2]);
			uniqueCarrier = columns[5];
			
			if(!columns[16].equals("")) {
				departureDelayTime = (int)Float.parseFloat(columns[16]);
			} else {
				departureDelayAvailable = false;
			}
			
			if(!columns[26].equals("")) {
				arriveDelayTime = (int)Float.parseFloat(columns[26]);
			} else {
				arriveDelayAvailable = false;
			}
			
			if(!columns[37].equals("")) {
				distance = (int)Float.parseFloat(columns[37]);
			} else {
				distanceAvailable = false;
			}
			
		} catch (Exception e) {
			// TODO: handle exception
			System.out.println("Error parsing a record : " + e.getMessage());
		}
	}


	/**
	 * @return the year
	 */
	public int getYear() {
		return year;
	}


	/**
	 * @return the month
	 */
	public int getMonth() {
		return month;
	}


	/**
	 * @return the arriveDelayTime
	 */
	public int getArriveDelayTime() {
		return arriveDelayTime;
	}


	/**
	 * @return the departureDelayTime
	 */
	public int getDepartureDelayTime() {
		return departureDelayTime;
	}


	/**
	 * @return the distance
	 */
	public int getDistance() {
		return distance;
	}


	/**
	 * @return the arriveDelayAvailable
	 */
	public boolean isArriveDelayAvailable() {
		return arriveDelayAvailable;
	}


	/**
	 * @return the departureDelayAvailable
	 */
	public boolean isDepartureDelayAvailable() {
		return departureDelayAvailable;
	}


	/**
	 * @return the distanceAvailable
	 */
	public boolean isDistanceAvailable() {
		return distanceAvailable;
	}


	/**
	 * @return the uniqueCarrier
	 */
	public String getUniqueCarrier() {
		return uniqueCarrier;
	}

}

```

> 음수를 0으로 만드는 것을 방지하기 위해 인덱스 변경

```java
package com.adacho.hadoop.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import com.adacho.hadoop.mapper.DelayCountMapper;
import com.adacho.hadoop.reducer.DelayCountReducer;

public class DelayCountWithCounter extends Configured implements Tool {
	
	public static void main(String[] args) throws Exception {
		int res = ToolRunner.run(new Configuration(), new DelayCount(), args);
		System.out.println("Mr-Job Result: " + res);
	}

	
	// Tool의 추상메서드 구현
	@Override
	public int run(String[] args) throws Exception {
		// TODO Auto-generated method stub
		// GenericOptionsParser에서 제공하는 파라미터를 제외한 나머지 파라미터를 가져온다.
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();
		
		if(otherArgs.length != 2) {
			System.out.println("Usage: DelayCount <in> <out>");
			System.exit(2);
		}
		
		Job job = Job.getInstance(getConf(), "DelayCountWithCounter");
		
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		
		job.setJarByClass(DelayCount.class);
		job.setMapperClass(DelayCountMapper.class);
		job.setReducerClass(DelayCountReducer.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		job.waitForCompletion(true);
		return 0;
	}
}

```



- 메이븐 실행

프로젝트파일 우클릭 -> 런애즈 -> 메이븐 빌드 ... -> 골: clean install -> 런 -> BUILD SUCCESS 확인



- 하둡

D:\KHY\airline\target 확인

> 프로젝트파일 우클릭 - 프로퍼티 - 리소스 - 경로의 .jar를 share의 java 폴더에 복붙(기존에 있었으므로 덮어쓰기)

D:b\KHY_OracleVBox\VBoxShare\download에 airOT201201~12.csv 파일 넣기

> 이미 되어있으므로 패스

```
[bit44@hadoopnode01 ~/hadoop]$ bin/yarn jar /mnt/share/java/airline-0.0.1.jar com.adacho.hadoop.driver.DelayCountWithCounter -D workType=departure airline_dep_delay_input dep_delay_counter_output
.
.
.
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=1364564442
        File Output Format Counters
                Bytes Written=171
Mr-Job Result: 0

```

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls dep_delay_counter_output                                                                      Found 2 items
-rw-r--r--   1 bit44 supergroup          0 2020-08-24 18:47 dep_delay_counter_output/_SUCCESS
-rw-r--r--   1 bit44 supergroup        171 2020-08-24 18:47 dep_delay_counter_output/part-r-00000
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -cat dep_delay_counter_output/part-r-00000
2012,1  151762
2012,10 183397
2012,11 161643
2012,12 207041
2012,2  136280
2012,3  187828
2012,4  153453
2012,5  175546
2012,6  205523
2012,7  232461
2012,8  215957
2012,9  158063
[bit44@hadoopnode01 ~/hadoop]$

```





### MultipleOutputs

1. ~lib.output.MultipleOutputs는 여러개의 출력 데이터를 쉽게 생성할 수 있는 기능을 제공

2. MultipleOutputs는 여러개의 OutputCollectors를 만들고 각 OutputCollectors에 대한 출력 경로, 출력 포맷, 키와 값 유형을 설정

3. 이러한 패러미터 설정은 MultipleOutputs에서 제공하는 static 메서드 addNamedOutput을 호출해 설정

4. MultipleOutputs에서 출력하는 데이터는 기존 맵리듀스 잡에서 생성하는 데이터와는 별개로 생성

5. 맵리듀스 잡이 종료되면 리듀스 단계에서 part-r-xxxxx라는 출력 데이터를 생성함, 그런데 리듀스 단계에서 MultipleOutputs를 이용해 myfile이라는 디렉터리에 데이터를 생성할 경우 part-r-xxxxx와 myfile-r-xxxxx가 동시에 생성



```java
package com.adacho.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.adacho.hadoop.common.AirlinePerformanceParser;
import com.adacho.hadoop.counter.DelayCounters;

public class DelayCountMapperWithMultipleOutputs extends Mapper<LongWritable, Text, Text, IntWritable>{

	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputKey = new Text();
	
	
	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.map(key, value, context);
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		
		if(parser.isDepartureDelayAvailable()) {
			if(parser.getDepartureDelayTime() > 0) {
				outputKey.set("D," + parser.getYear() + "," + parser.getMonth());
				context.write(outputKey, outputValue);
			} else if(parser.getDepartureDelayTime() == 0) {
				context.getCounter(DelayCounters.scheduled_departure).increment(1);
			} else if(parser.getDepartureDelayTime() < 0) {
				context.getCounter(DelayCounters.early_departure).increment(1);
			}
		} else {
			context.getCounter(DelayCounters.not_available_departure).increment(1);
		}
		
		if(parser.isArriveDelayAvailable()) {
			if(parser.getArriveDelayTime() > 0) {
				outputKey.set("A," + parser.getYear() + "," + parser.getMonth());
				context.write(outputKey, outputValue);
			} else if(parser.getArriveDelayTime() == 0) {
				context.getCounter(DelayCounters.scheduled_arrival).increment(1);
			} else if(parser.getArriveDelayTime() < 0) {
				context.getCounter(DelayCounters.early_arrival).increment(1);
			}
		} else {
			context.getCounter(DelayCounters.not_available_arrival).increment(1);
		}
	}
	
	
}

```

> getCounter() : Enum 타입

```java
package com.adacho.hadoop.reducer;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;

public class DelayCountReducerWithMultipleOutputs extends Reducer<Text, IntWritable, Text, IntWritable>{

	private MultipleOutputs<Text, IntWritable> mos;
	private Text outputKey = new Text();
	private IntWritable result = new IntWritable();
	
	
	@Override
	protected void setup(Reducer<Text, IntWritable, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.setup(context);
		mos = new MultipleOutputs<Text, IntWritable>(context);
	}


	@Override
	protected void reduce(Text key, Iterable<IntWritable> values,
			Reducer<Text, IntWritable, Text, IntWritable>.Context context) throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.reduce(arg0, arg1, arg2);
		String[] columns = key.toString().split(",");
		
		outputKey.set(columns[1] + "," + columns[2]); // 년도, 월
		int sum = 0;
		for(IntWritable value : values) {
			sum += value.get();
		}
		result.set(sum);
		
		if(columns[0].equals("D")) { // D 혹은 A를 준 것
			mos.write("departure", outputKey, result); // 출력 디렉터리명 추가
		}
		else
			mos.write("arrival", outputKey, result);
	}
	
}

```

```java
package com.adacho.hadoop.mapper;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import com.adacho.hadoop.common.AirlinePerformanceParser;
import com.adacho.hadoop.counter.DelayCounters;

public class DelayCountMapperWithCounter extends Mapper<LongWritable, Text, Text, IntWritable>{

	private String workType;
	private final static IntWritable outputValue = new IntWritable(1);
	private Text outputKey = new Text();
	
	
	@Override
	protected void setup(Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.setup(context);
		workType = context.getConfiguration().get("workType");
	}


	@Override
	protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		// super.map(key, value, context);
		AirlinePerformanceParser parser = new AirlinePerformanceParser(value);
		
		if(workType.equals("departure")) {
			if(parser.isDepartureDelayAvailable()) {
				if(parser.getDepartureDelayTime() > 0) {
					outputKey.set(parser.getYear() + "," + parser.getMonth());
					context.write(outputKey, outputValue);
				} else if(parser.getDepartureDelayTime() == 0) {
					context.getCounter(DelayCounters.scheduled_departure).increment(1);
				} else if(parser.getDepartureDelayTime() < 0) {
					context.getCounter(DelayCounters.early_departure).increment(1);
				}
			} else {
				context.getCounter(DelayCounters.not_available_departure).increment(1);
			}
		} else if(workType.equals("arrival")) {
			if(parser.isArriveDelayAvailable()) {
				if(parser.getArriveDelayTime() > 0) {
					outputKey.set(parser.getYear() + "," + parser.getMonth());
					context.write(outputKey, outputValue);
				} else if(parser.getArriveDelayTime() == 0) {
					context.getCounter(DelayCounters.scheduled_arrival).increment(1);
				} else if(parser.getArriveDelayTime() < 0) {
					context.getCounter(DelayCounters.early_arrival).increment(1);
				}
			} else {
				context.getCounter(DelayCounters.not_available_arrival).increment(1);
			}
		}
	}
}

```

```java
package com.adacho.hadoop.driver;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import com.adacho.hadoop.mapper.DelayCountMapperWithMultipleOutputs;
import com.adacho.hadoop.reducer.DelayCountReducerWithMultipleOutputs;


public class DelayCountWithMultipleOutputs extends Configured implements Tool {
	public static void main(String[] args) throws Exception {
		
		int res = ToolRunner.run(new Configuration(), new DelayCountWithMultipleOutputs(), args);
		System.out.println("MR-Job Result: " + res);
	}

	
	@Override
	public int run(String[] args) throws Exception {
		// TODO Auto-generated method stub
		String[] otherArgs = new GenericOptionsParser(getConf(), args).getRemainingArgs();
		
		if(otherArgs.length != 2) {
			System.out.println("Usage: DelayCountWithMultipleOutputs <in> <out>");
			System.exit(2);
		}
		
		Job job = Job.getInstance(getConf(), "DelayCountWithMultipleOutputs");
		
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		
		job.setJarByClass(DelayCountWithMultipleOutputs.class);
		job.setMapperClass(DelayCountMapperWithMultipleOutputs.class);
		job.setReducerClass(DelayCountReducerWithMultipleOutputs.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		MultipleOutputs.addNamedOutput(job, "departure", TextOutputFormat.class,
				Text.class, IntWritable.class);
		MultipleOutputs.addNamedOutput(job, "arrival", TextOutputFormat.class,
				Text.class, IntWritable.class);
		
		
		job.waitForCompletion(true);
		return 0;
	}
	
	
	
}

```



- 메이븐 실행

프로젝트파일 우클릭 -> 런애즈 -> 메이븐 빌드 ... -> 골: clean install -> 런 -> BUILD SUCCESS 확인



- 하둡

D:\KHY\airline\target 확인

> 프로젝트파일 우클릭 - 프로퍼티 - 리소스 - 경로의 .jar를 share의 java 폴더에 복붙(기존에 있었으므로 덮어쓰기)

D:b\KHY_OracleVBox\VBoxShare\download에 airOT201201~12.csv 파일 넣기

> 이미 되어있으므로 패스

```
[bit44@hadoopnode01 ~/hadoop]$ bin/yarn jar /mnt/share/java/airline-0.0.1.jar com.adacho.hadoop.driver.DelayCountWithMultipleOutputs airline_dep_delay_input delay_counts_mos
.
.
.

```

> 뭔가 잘못 됐을 때, 컨트롤 + C로 빠져나와야 됨

```
[bit44@hadoopnode01 ~/hadoop]$ bin/hdfs dfs -ls delay_counts_mos
\Found 4 items
-rw-r--r--   1 bit44 supergroup          0 2020-08-24 16:52 delay_counts_mos/_SUCCESS
-rw-r--r--   1 bit44 supergroup        171 2020-08-24 16:52 delay_counts_mos/arrival-r-00000
-rw-r--r--   1 bit44 supergroup        171 2020-08-24 16:52 delay_counts_mos/departure-r-00000
-rw-r--r--   1 bit44 supergroup          0 2020-08-24 16:52 delay_counts_mos/part-r-00000

```

