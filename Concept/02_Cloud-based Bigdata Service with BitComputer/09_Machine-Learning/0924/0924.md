## 머신러닝 과정 돌아보기

1. 수집: 크롤링, 설문조사, 기존 DB

> 적재, 저장

2. 전처리: Dtype 지정, 토큰화, 이상치 파악, 결측치 파악

> 데이터마트 구축

3. 데이터 분석: info, summary, 비즈니스 파악(전문가 의견/특성조합), 산포도, 시퀀스(시간에 따른 데이터 변화), 상관관계 파악

4. 데이터 편집, 변환, 파이프라인 구축

> Train, Test 나누기(8:2 ~ 7:3 비율)
>
> Test는 6번 과정에서 사용

> 이상치 처리, 결측치 처리, 스케일링, 인코딩, 특성조합
>
> train set 기준 => 파이프라인으로 관리

5. 모델 선정 및 훈련: GridSearchCV
6. 테스트
7. 배포: 서비스
8. 유지 보수

> 4, 5, 6 단계를 반복하는 것





# 지도학습의 종류 - 분류와 회귀

https://drive.google.com/drive/folders/18ttswFBiwVTluWVlbO4-XaFeRL0EbMIU

- 이진 분류

```
y = f(x)

고양이 -> 0
강아지 -> 1
위 두 가지를 클래스라고 함
```



- 다중 분류

> 세 가지 이상 클래스



- 분류와 회귀는 카테고리 자체가 다른 것이라고 보면 됨

> 챗봇은 분류임





## 일반화

과대적합과 과소적합만 피하면 일반화가 됨

> 데이터를 복잡하게 분석하면 과대적합 일어남(복잡도 ↑)
>
> 데이터를 가볍게 분석하면 과소적합 일어남(복잡도 ↓)

> 복잡도가 중간인 지점을 찾는게 중요

※ 머신러닝은 예측을 통한 데이터 분석의 도구





## 코드

```python
!pip install mglearn # 코랩에서 패키지 설치 방법

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import mglearn
```

```python
mglearn.plots.plot_knn_classification(n_neighbors=26) # n_neighbors: 이웃의 개수

# 테스트용 데이터셋 가져오기
# y를 결정짓지 위한 x들의 항목들(행렬은 보통 대문자로 표기)
# 많은 데이터들을 위한 y 값 1개(소문자)
X, y = mglearn.datasets.make_forge()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # random_state=0: 똑같은 결과를 위해서(랜덤에 의존하지 않겠다)
```

```
/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function make_blobs is deprecated; Please import make_blobs directly from scikit-learn
  warnings.warn(msg, category=FutureWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function make_blobs is deprecated; Please import make_blobs directly from scikit-learn
  warnings.warn(msg, category=FutureWarning)

```



```python
# classification
# K-NN 모델에는 이웃의 개수가 들어간다(이웃의 개수가 하이퍼 파라미터로써 복잡도를 조절)
clf = KNeighborsClassifier(n_neighbors=3)
clf
```

```
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,
                     weights='uniform')
```



```python
# 모델 훈련 - fit 메서드 활용
# 지도학습 알고리즘 훈련시 X_tarin, y_train을 넣음
clf.fit(X_train, y_train)
```

```
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,
                     weights='uniform')
```



```python
# 예측은 predict 함수를 활용
# 예측할 데이터를 넣어줌
clf.predict(X_test)
```

```
array([1, 0, 1, 0, 1, 0, 0])
```



```python
# 훈련용 데이터와 테스트 데이터의 형상(shape)은 항상 일치해야 함
print("훈련 데이터의 shape: {}".format(X_train.shape))
print("테서트 데이터의 shape: {}".format(X_test.shape))
```

```
훈련 데이터의 shape: (19, 2)
테서트 데이터의 shape: (7, 2)
```

> 앞쪽: 데이터의 샘플 개수
>
> 뒷쪽: 특성의 개수



```python
score = clf.score(X_test, y_test)
print(score)
```

```
0.8571428571428571
```

> 85.7% 정도



- 

y = target(정답)

햇y = predict(예측)

바y = target mean(y들의 평균)



```python
mglearn.plots.plot_knn_regression(n_neighbors=3)

# 사이킷런의 특징상 알고리즘명 뒤에 Classifier, Regressor가 붙음
from sklearn.neighbors import KNeighborsRegressor

X, y = mglearn.datasets.make_wave(n_samples=40)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

reg_1 = KNeighborsRegressor(n_neighbors=1)
reg_3 = KNeighborsRegressor(n_neighbors=3)
```

```python
reg_1.fit(X_train, y_train)
reg_3.fit(X_train, y_train)

# 이웃의 개수가 1일 때와 3일 때의 결정 계수(점수) 확인하기
# 회귀에서의 score 함수는 결정 계수를 보여줌
print("이웃의 개수가 1개일 때: {:.2f}".format(reg_1.score(X_test, y_test)))
print("이웃의 개수가 3개일 때: {:.2f}".format(reg_3.score(X_test, y_test)))

# train셋
print("이웃의 개수가 1개일 때: {:.2f}".format(reg_1.score(X_train, y_train)))
print("이웃의 개수가 1개일 때: {:.2f}".format(reg_3.score(X_train, y_train)))
```

```
이웃의 개수가 1개일 때: 0.35
이웃의 개수가 3개일 때: 0.83
이웃의 개수가 1개일 때: 1.00
이웃의 개수가 1개일 때: 0.82
```





## 선형모델 알아보기

- y = wx + b

> w: weight
>
> b: bias



- MSE

둥근 포물선형으로 그래프가 그려짐

> y축은 오차
>
> x축은 w 또는 b



```python
# 선형 회귀
# 오차를 제일 줄일 수 있는(데이터를 가장 잘 표현하는) 직선을 긋기 위해 기울기와 절편을 구하는 알고리즘
from sklearn.linear_model import LinearRegression

X, y = mglearn.datasets.make_wave(n_samples=60)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

lin_reg = LinearRegression().fit(X_train, y_train) # 훈련

print("기울기(w값): {}".format(lin_reg.coef_))
print("절편(b값): {}".format(lin_reg.intercept_))
```

```
기울기(w값): [0.39390555]
절편(b값): -0.031804343026759746
```



```python
print("훈련 세트 점수: {:.2f}".format(lin_reg.score(X_train, y_train)))
print("테스트 세트 점수: {:.2f}".format(lin_reg.score(X_test, y_test)))
```

```
훈련 세트 점수: 0.67
테스트 세트 점수: 0.66
```

> 특성이 많으면 많을 수록 좋음





## 릿지 회귀, 라쏘

> 랏소는 이상치에 좋음(평가 방식이 MAE)
>
> 릿지는 MSE 방식

1. 가중치를 제어할 수 있는 방안들을 제공(학습 없이)

> 선형회귀의 복잡도는 가중치에 의해 제어됨

```
y = w1x1 + w2x2

x1, x2: 상수
w1=100, w2=10이면 w1에 과대적합이 되어있다고 볼 수 있음
하지만 ridge와 lasso는 패널티(알파)라는 개념이 있어 학습 없이 가중치를 제어할 수 있음(억제)
```



```python
from sklearn.linear_model import Ridge

# 보스턴 주택가격 데이터셋 불러오기
X, y = mglearn.datasets.load_extended_boston()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

lin_reg = LinearRegression().fit(X_train, y_train)
ridge = Ridge(alpha=0.1).fit(X_train, y_train)

# 일반 LinearRegression()과 비교
print("선현 회귀 모델: {:.2f} / {:.2f}".format(lin_reg.score(X_train, y_train), lin_reg.score(X_test, y_test)))
print("선현 회귀 모델: {:.2f} / {:.2f}".format(ridge.score(X_train, y_train), ridge.score(X_test, y_test)))

# print("훈련 세트 점수 : {:.2f}".format(ridge.score(X_train, y_train)))
# print("테스트 세트 점수 : {:.2f}".format(ridge.score(X_test, y_test)))
```

```
선현 회귀 모델: 0.95 / 0.61
선현 회귀 모델: 0.93 / 0.77
```

> 선형 과대적합이 되었다라는 것 확인
>
> 1. 이상치나 데이터를 수정
> 2. 다른 모델을 선택

> 릿지는 선형보다 일반화 되어있음을 확인



```python
alpha = 10

ridge10 = Ridge(alpha=alpha).fit(X_train, y_train)

print("Ridge10: {:.2f} / {:.2f}".format(ridge10.score(X_train, y_train), ridge10.score(X_test, y_test)))
```

```
Ridge10: 0.79 / 0.64
```

> 알파(패널티)를 높이면 가중치가 감소(모델의 복잡도 감소)
>
> 따라서 소수점 자리 정도는 넣어야 가중치를 증가시킴

> 과소적합이 되기 때문에 train과 test 모두 떨어짐



```python
alpha = 0.1

ridge01 = Ridge(alpha=alpha).fit(X_train, y_train)

print("Ridge10: {:.2f} / {:.2f}".format(ridge01.score(X_train, y_train), ridge01.score(X_test, y_test)))
```

```
Ridge10: 0.93 / 0.77
```

> 복잡도가 올라가면서 점수가 높아짐
>
> 사실상 과대적합이라고 볼 수도 있음

> 릿지는 가중치를 조금 높이게 괜찮을 수 있다(혹은 기본값으로)



- 릿지 회귀 특징

릿지회귀는 0에 가깝게 만들 뿐 실제 계수를 0으로 만들어버리지 않음

> 모델에 의한 특성 선택



- 라쏘 특징

라쏘 모델은 실제 0으로 만들어버리는 특성을 지님





## 복습할 것

1. 일반화의 정의
2. 선형회귀의 공식(y=WX+b)

3. 릿지, 라소