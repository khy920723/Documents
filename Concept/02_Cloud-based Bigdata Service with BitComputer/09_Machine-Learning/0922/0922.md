# 머신러닝을 쓰는 이유

https://drive.google.com/drive/folders/19nFSmk4W29ihs6MB2JCyL9sQt1y0TQrs 

> 네이버 메일로 공유해준 강의자료 링크

1. 예측(추론)
2. 데이터 분석(패턴찾기) -> 학습(트레이닝 과정)





## 데이터 구조

특성(feature)

레이블(label, target)

> x일 때 y가 얼마일 지 알아보는 관계식 정도의 정보
>
> 이런 데이터들을 학습 시키는 것





## COLAB

https://colab.research.google.com/notebooks/intro.ipynb

> 코랩 링크
>
> 구글의 GPU와 TPU를 사용할 수 있어서 좋음
>
> 노트북파일 작성하면 .ipynb 확장자로 저장 됨

> 과제 제출용 아이디: khy.bitcamp@gmail.com / rhwnsdlf12





### 사용법

파일 - 새노트 - 수정 - 노트설정 - GPU 선택

> 동작 없는 시간이 길면 자동으로 꺼지기 때문에 항상 저장하는 것이 좋음(대략 30분)
>
> 따라서 오랜 시간이 걸리는 작업은 코랩으로 하면 안됨

> 대용량 데이터 학습은 GPU를 선택하는 것이 좋음





## 05_머신러닝(머신이 배운다)

- GIGO: 가비지 인 가비지 아웃



- 조건문과 반복문 위주로 프로그래밍을 함

> 단, 조건문과 반복문이 많아질 수록 제대로 코드작성 하는건지 확인해야 함



- 머신러닝은 코딩이 아닌 학문분야



- 모델설정 - 하이퍼 파라미터 설정(사람이 직접 설정) - 훈련 - 머신러닝 모델 검증 - 서비스 런칭

> 평가 결과가 100프로(오차 0)일 경우 잘못된 것일 수도 있음



- 패턴을 찾는 과정 = 데이터 마이닝



- 준지도 학습: 비지도로 군집을 시키고 지도로 답을 알려줌



- 강화학습 = QnA





### 지도학습

- 정답은 언제나 순서가 있는 정수로 분류(0, 1, 2, 3 ...)

> 딥러닝은 one-hot Encoding 값 사용



- 회귀는 연속된 실수 값들로 이뤄짐

> 때문에 선으로 표현 가능





### 비지도학습





### 데이터의 일반화

- 새로운 데이터에도 머신이 잘 적응해야 하는 것

> 데이터를 일반화 시키지 않으면 학습이 안됨



- 공감 = 일반화(단순화)

> 모델 일반화 / 데이터 일반화
>
> 하이퍼 파라미터를 줘서 시키는 것이 모델일반화



- 특성의 개수에 따라 복잡도가 결정됨





## 실습

https://github.com/mhso-dev/bit-python/blob/master/02_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%9D%84_%EC%9C%84%ED%95%9C_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D%EA%B3%BC_%EC%B2%98%EB%A6%AC.ipynb

> 강사님 깃허브 링크

> 코랩 노트패드 작성 후 쉬프트+ 엔터 = 실행

> 02 부분 그대로 실습함





### 테스트 세트 만들기

- 정규분포를 따라가는 데이터를 가장 학습 잘 함

> 따라서 정규분포 생김새로 먼저 다듬는 것이 중요



- 데스트 세트 만들기 부분에서 train과 테스트로 나눈 이유

> 훈련 검증 세트로 학습, 테스트 세트로 검증
>
> 훈련 - 검증 - 테스트



- 데이터 스누핑 편향



- 일반화 오차





### 데이터 쪼개기

- 섞고 잘라서 테스트

> shuffle - split - test



- y축: 정확도(스코어) / x축: 복잡도

> x축의 왼쪽으로 갈 수록 과소적합
>
> x축의 오른쪽으로 갈 수록 과대적합
>
> 따라서 train 세트와 test세트의 중간 지점을 일반화 해야함

```python
import numpy as np

def split_train_test(data, test_ratio=0.2):
  shuffled_indices = np.random.permutation(len(data))  # 인덱스를 섞어 랜덤하게 보여줌
  test_set_size = int(len(data) * test_ratio)  # int를 사용하는 이유: 테스트 개수가 실수로 나오기 때문에 정수로 출력
  
  test_indices = shuffled_indices[:test_set_size] # 20퍼센트 비율의 랜덤 인덱스
  train_indices = shuffled_indices[test_set_size:] # 80퍼센트 비율의 랜덤 인덱스
  
  return data.iloc[train_indices], data.iloc[test_indices] # 인덱스로 되어있기 때문에 iloc 사용
```



- 제대로 된 머신러닝을 하려면 학습의 방법까지 알아야함

> 해시값 = id값

```python
from zlib import crc32 # 고유값을 구할 용도(id 생성기) = 파일의 지문을 구하는 것

def test_set_check(identifier, test_ratio):
  return crc32(np.int64(identifier)) & 0xffffffff < test_ration * 2 **32 # & 이후 코드: 32비트 os 지원하는 코드

def split_train_test_by_id(data, test_ratio, id_column): # 아이디 부여
  ids = data[id_columns] # id로 사용할 데이터 선정
  in_test_set = ids.apply(lambda id_ : test_set_check(id_, test_ratio))  # apply: 집계함수, 결과를 리턴해줘야 함
  return data.loc[~in_test_set], data.loc[in_test_set] # 20프로 마지막까지, 20프로 정도까지
```

> 신뢰할 수 있고 절대 바뀌지 않는 값을 선정
>
> 위도와 경도는 절대 바뀌지 않음

```python
df_train, df_test = split_train_test(housing, 0.2)
len(df_train), len(df_test)
```

> 인덱스로 스트레티지로 쓰려면 제약 조건이 필요함



- 샘플링 편향과 계층적 샘플링

> 비율에 맞게 샘플을 뽑기 위함
>
> 그래서 계층을 만듦(카테고리와 비슷)



- 하우징 데이터셋에서는?

```python
housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1,2,3,4,5]) # np.inf: 무한대

housing[["median_income", "income_cat"]].head()
```

```
	median_income	income_cat
0	8.3252	5
1	8.3014	5
2	7.2574	5
3	5.6431	4
4	3.8462	3
```

> 계층적 샘플링

```python
housing["income_cat"].hist()
```

```
<matplotlib.axes._subplots.AxesSubplot at 0x7f3dc1059668>
...
```



```python
# 사이킷런에는 계층을 활용한 샘플링을 지원하는 stratifiedShuffledSplit 있음
from sklearn.model_selection import  StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

for train_index, test_index in split.split(housing, housing["income_cat"]):
  strat_train_set = housing.loc[train_index]
  strat_test_set  = housing.loc[test_index]
```



```python
strat_test_set["income_cat"].value_counts() / len(strat_test_set) # 전체 데이터에서 각 샘플들이 차지하는 비율
```

```
3    0.350533
2    0.318798
4    0.176357
5    0.114583
1    0.039729
Name: income_cat, dtype: float64
```



```python

def income_cat_proportions(data):
    return data["income_cat"].value_counts() / len(data)

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

compare_props = pd.DataFrame({
    "전체": income_cat_proportions(housing),
    "계층 샘플링": income_cat_proportions(strat_test_set),
    "무작위 샘플링": income_cat_proportions(test_set),
}).sort_index()
compare_props["무작위 샘플링 오류율"] = 100 * compare_props["무작위 샘플링"] / compare_props["전체"] - 100
compare_props["계층 샘플링 오류율"] = 100 * compare_props["계층 샘플링"] / compare_props["전체"] - 100
compare_props
```

```

전체	계층 샘플링	무작위 샘플링	무작위 샘플링 오류율	계층 샘플링 오류율
1	0.039826	0.039729	0.040213	0.973236	-0.243309
2	0.318847	0.318798	0.324370	1.732260	-0.015195
3	0.350581	0.350533	0.358527	2.266446	-0.013820
4	0.176308	0.176357	0.167393	-5.056334	0.027480
5	0.114438	0.114583	0.109496	-4.318374	0.127011
```

> 절대값이 적을 수록 오차가 거의 없음
>
> 무작위 샘플링은 오차가 매우 큼

> 회귀에서 많이 사용
>
> 데이터가 한 쪽에 많이 쏠려있는 케이스의 경우 계층을 만들어주는 편이 좋음

```python
# 위치별 시각화
# housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1) # 알파: 불투명도
housing.plot(kind='scatter', x='longitude', y='latitude', alpha=.4,
             s=housing["population"] / 100,
             label="population",
             figsize=(10, 7),
             c='median_house_value',
             cmap=plt.get_cmap('jet'),
             colorbar=True) # s: 점의 크기, c: 컬러, 씨맵: 컬러

plt.legend()
```

```
<matplotlib.axes._subplots.AxesSubplot at 0x7f3dbf25eba8>

```

> 시각화로 집의 가격이 해안가에 가까울 수록 비싸진다는 것을 확인 가능



- 상관관계

> x와 y사이의 기울기 = y = ax +b
>
> a: 감마, 피어슨 상관계수(절대값을 씌움) / b: 편향, 민감도
>
> 양의 상관관계, 음의 상관관계, 0

```python
corr_matrix = housing.corr()

# 중간 주택 가격에 대한 다른 특성들과의 상관계수 구하기
corr_matrix['median_house_value'].sort_values(ascending=False)
```

```
median_house_value    1.000000
median_income         0.688075
total_rooms           0.134153
housing_median_age    0.105623
households            0.065843
total_bedrooms        0.049686
population           -0.024650
longitude            -0.045967
latitude             -0.144160
Name: median_house_value, dtype: float64
```

```python

from pandas.plotting import scatter_matrix

attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]

scatter_matrix(housing[attributes], figsize=(12, 8))
plt.show()
```

```python
# 하나만 확대해서 보기
housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)
```



- 특성 조합 보기

> 특성의 개수가 너무 없을 때 쓰는 방법
>
> 상관관계까 있어보인다 = 곱하기, 비율을 보고싶다 = 나누기
>
> 비즈니스적으로 생각을 해야됨

```python
housing["rooms_per_household"] = housing["total_rooms"] / housing["households"]
housing["bedrooms_per_room"]   = housing["total_bedrooms"] / housing["total_rooms"]
housing["population_per_household"] = housing["population"] / housing["households"]


corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)
```

```
median_house_value          1.000000
median_income               0.688075
rooms_per_household         0.151948
total_rooms                 0.134153
housing_median_age          0.105623
households                  0.065843
total_bedrooms              0.049686
population_per_household   -0.023737
population                 -0.024650
longitude                  -0.045967
latitude                   -0.144160
bedrooms_per_room          -0.255880
Name: median_house_value, dtype: float64
```





### 머신러닝을 위한 데이터 준비

```python
# 지도학습
# feature(문제)와 label(답) 나누기

housing = strat_train_set.drop("median_house_value", axis=1) # 훈련 데이터셋 (feature) 준비, axis: 축(0: 행, 1: 열)
housing_labels = strat_train_set["median_house_value"].copy() # gnsfus fpdlqmf (target, label) 준비
# 점-스칼라: 0차원
# 선-벡터: 1차원
# 면-매트릭스: 2차원
# 입체-텐서: 3차원

```



```python
# housing.dropna(subset=["total_bedrooms"]) # NaN값 삭제
# housing.drop("total_bedrooms", axis=1) # total_bedrooms 특성 삭제
median = housing["total_bedrooms"].median() # total_bedrooms의 중간값으로 채우기. 해당 값을 테스트 세트에도 채워야 합니다
housing["total_bedrooms"].fillna(median, inplace=True)
```

```python
# 파이프라인(작업들을 하나로)을 하기 위한 sklearn의 SimpleImputer
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")
```

```python
# 숫자 특성만 분리
housing_num = housing.drop("ocean_proximity", axis=1)

imputer.fit(housing_num) # fit: 적용, 근거에 대한 것을 넣는 부분
```

```
SimpleImputer(add_indicator=False, copy=True, fill_value=None,
              missing_values=nan, strategy='median', verbose=0)
```



```python
# 각 컬럼에 적용되는 미디안 값 확인
imputer.statistics_
```

```
array([-118.51  ,   34.26  ,   29.    , 2119.5   ,  433.    , 1164.    ,
        408.    ,    3.5409,    3.    ])
```

```python
# 각 컬럼에 적용되는 미디안 값 확인(판다스)
housing_num.median().values
```

```
array([-118.51  ,   34.26  ,   29.    , 2119.5   ,  433.    , 1164.    ,
        408.    ,    3.5409,    3.    ])
```



```python
# 변환 - transform
X = imputer.transform(housing_num) # 결측치가 채워진 numpy
```

```
	longitude	latitude	housing_median_age	total_rooms	total_bedrooms	population	households	median_income	income_cat
17606	-121.89	37.29	38.0	1568.0	351.0	710.0	339.0	2.7042	2.0
18632	-121.93	37.05	14.0	679.0	108.0	306.0	113.0	6.4214	5.0
14650	-117.20	32.77	31.0	1952.0	471.0	936.0	462.0	2.8621	2.0
3230	-119.61	36.31	25.0	1847.0	371.0	1460.0	353.0	1.8839	2.0
3555	-118.59	34.23	17.0	6592.0	1525.0	4459.0	1463.0	3.0347	3.0
```

```python
housing_tr.info()
```

```
<class 'pandas.core.frame.DataFrame'>
Int64Index: 16512 entries, 17606 to 15775
Data columns (total 9 columns):
 #   Column              Non-Null Count  Dtype  
---  ------              --------------  -----  
 0   longitude           16512 non-null  float64
 1   latitude            16512 non-null  float64
 2   housing_median_age  16512 non-null  float64
 3   total_rooms         16512 non-null  float64
 4   total_bedrooms      16512 non-null  float64
 5   population          16512 non-null  float64
 6   households          16512 non-null  float64
 7   median_income       16512 non-null  float64
 8   income_cat          16512 non-null  float64
dtypes: float64(9)
memory usage: 1.3 MB
```



```python

housing_cat = housing[["ocean_proximity"]]
housing_cat.head(10)
```

```

ocean_proximity
17606	<1H OCEAN
18632	<1H OCEAN
14650	NEAR OCEAN
3230	INLAND
3555	<1H OCEAN
19480	INLAND
8879	<1H OCEAN
13685	INLAND
4937	<1H OCEAN
4861	<1H OCEAN
```



```python
from sklearn.preprocessing import OrdinalEncoder  # 순서가 있는 인코더
ordinal_encoder = OrdinalEncoder()

housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat.values)
housing_cat_encoded[:10]
```

```
array([[0.],
       [0.],
       [4.],
       [1.],
       [0.],
       [1.],
       [0.],
       [1.],
       [0.],
       [0.]])
```



```python
ordinal_encoder.categories_ # 순서만 가져다 놓은 것
```

```
[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
       dtype=object)]
```

```python
# 카테고리 형식의 데이터는 위 식으로 쓰면 안되기 때문에 원핫인코더 사용
from sklearn.preprocessing import OneHotEncoder
cat_encoder = OneHotEncoder()

housing_cat_one_hot = cat_encoder.fit_transform(housing_cat) # 데이터 적용과 변화를 동시에 함
housing_cat_one_hot
```

```
<16512x5 sparse matrix of type '<class 'numpy.float64'>'
	with 16512 stored elements in Compressed Sparse Row format>
```

> 희소행렬: 메모리 절약을 해결하기 위해 사용되는 행렬

```python
housing_cat_one_hot.toarray() # 5개의 컬럼이 만들어진 것을 확인
```

```
array([[1., 0., 0., 0., 0.],
       [1., 0., 0., 0., 0.],
       [0., 0., 0., 0., 1.],
       ...,
       [0., 1., 0., 0., 0.],
       [1., 0., 0., 0., 0.],
       [0., 0., 0., 1., 0.]])
```

