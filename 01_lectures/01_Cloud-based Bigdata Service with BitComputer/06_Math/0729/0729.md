# 데이터 과학 





## 머신러닝(기계학습) 입문

```
Input -> Program(Algorithm) -> Output

​                ↓

Program: ML(Machine Learning) -> 학습(Input)
```

```
Classification : 남/녀, 개/고양이 ...

Regression(회귀): 연속적인 값, 30.5, 3.5 ...
```



### 레이블과 샘플

1. 레이블: y =f(x)에서 y변수에 해당하는 것

2. 샘플(예제): 기계 학습에 주어지는 특정한 예(feature와 같음)



### 학습 데이터와 테스트 데이터

```
어떤 데이터들 → 학습 알고리즘 → 모델 생성
```



### 학습과 예측

1. 학습: 모델을 만들거나 배우는 것
2. 예측(predict): 학습된 모델을 레이블이 없는 샘플에 적용하는 것

> 학습된 모델을 사용하여 유용한 예측(y')를 해내는 것



### 지도학습

1. 회귀(regression): 실수 입력(x)과 실수 출력(y)이 주어질 때 입력에서 출력으로의 매핑 함수를 학습하는 것
2. 분류: y=f(x)에서 출력 y가 이산적(discrete)인 경우에 이것을 분류문제라고 함



### 비지도 학습

1. 컴퓨터 스스로 입력들을 분류하는 것을 의미

2. 식 y=f(x)에서 레이블 y가 주어지지 않는 것

3. 데이터들의 상관도를 분석하여 유사한 데이더틀을 모을 수 있음(군집화)

- 클러스터링(군집화): 데이터간 거리를 계산하여 입력을 몇 개의 그룹으로 나누는 방법



### 강화학습



### 기계학습의 가치

1. 프로그래밍 시간을 줄일 수 있음

> 많은 예제만 있다면 학습 시켜서 빠른 시간 안에 신뢰성 있는 프로그램 완성 가능

2. 알고리즘이 떠오르지 않는 문제들 해결 시도 가능

> 많은 예제만 있으면 학습하기 때문



- Linear regression

```python
# 기존 프로그램 방식

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def celsius_to_faherenheit(x):
    return x * 1.8 + 35

data_C = np.array(range(0, 100),)
data_F = celsius_to_faherenheit(data_C)
print(data_C, "\n")
print(data_F)

inp = int(input("섭씨 온도 입력:"))
print("화씨 온도로", celsius_to_faherenheit(inp), "입니다.")
```

```
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
 96 97 98 99] 

[ 35.   36.8  38.6  40.4  42.2  44.   45.8  47.6  49.4  51.2  53.   54.8
  56.6  58.4  60.2  62.   63.8  65.6  67.4  69.2  71.   72.8  74.6  76.4
  78.2  80.   81.8  83.6  85.4  87.2  89.   90.8  92.6  94.4  96.2  98.
  99.8 101.6 103.4 105.2 107.  108.8 110.6 112.4 114.2 116.  117.8 119.6
 121.4 123.2 125.  126.8 128.6 130.4 132.2 134.  135.8 137.6 139.4 141.2
 143.  144.8 146.6 148.4 150.2 152.  153.8 155.6 157.4 159.2 161.  162.8
 164.6 166.4 168.2 170.  171.8 173.6 175.4 177.2 179.  180.8 182.6 184.4
 186.2 188.  189.8 191.6 193.4 195.2 197.  198.8 200.6 202.4 204.2 206.
 207.8 209.6 211.4 213.2]
섭씨 온도 입력:30
화씨 온도로 89.0 입니다.

Process finished with exit code 0

```

> 기존 프로그램 방식





## 알고리즘

1. KNN(k-Nearest Neibor) 알고리즘: 머신러닝 알고리즘 중 가장 간단하고 이해하기 쉬운 분류 알고리즘

> 데이터 관찰 -> 거리 계산 -> 이웃 찾음 -> 새로운 데이터에 대해 투표

> 장점: 어떤 종류의 학습이나 준비 시간이 필요 없음
>
> 단점: 데이터와 클래스가 많이 있다면 많은 메모리 공간과 계산 시간 필요(특징 공간에 있는 모든 데이터들에 대한 정보 필요)

```python
import pandas as pd
import pickle  # 파이썬 객체를 파일로 저장하는 패키지
import matplotlib.pyplot as plt
import seaborn as sns  # 시각화 함수 패키지
import numpy as np

# 그래프 출력을 위한 선언
# get_ipython().run_line_magic('matplotlib', 'inline')  # 주피터에서 사용하기 위한 코드, 파이참에선 plt.show()
# sklearn 모델의 동일한 결과 출력을 위해 선언
np.random.seed(5)


# 데이터 불러오기(학습 데이터, 테스트 데이터)
# 데이터 분석 단계(4.2_농구선수_데이터분석.ipynb)에서 생성한 농구 선수 포지션 예측하기의 학습 데이터 및 테스트 데이터 로드
train = pd.read_csv("C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_train.csv")
print(train.head())
print("----------------------")

test = pd.read_csv("C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_test.csv")
print(test.head())
print("----------------------")


# 최적의 K 찾기(교차검증 cross validation)
# import kNN library
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score  # 교차검증 시켜주는 패키지

# find best k, range from 3 to half of the number of data
max_k_range = train.shape[0] // 2
k_list = []
for i in range(3, max_k_range, 2):
    k_list.append(i)

cross_validation_scores = []
x_train = train[['3P', 'BLK', 'TRB']]
y_train = train[['Pos']]

# 10-fold cross validation
for k in k_list:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, x_train, y_train.values.ravel(), cv=10, scoring='accuracy')
    cross_validation_scores.append(scores.mean())

print(cross_validation_scores)
print("----------------------")

# visuallize accuracy acoodrding to k
plt.plot(k_list, cross_validation_scores)
plt.xlabel('the number of k')
plt.ylabel('Accuracy')
plt.show()

# find best k
cvs = cross_validation_scores
k = k_list[cvs.index(max(cross_validation_scores))]
print('The best number of k:' + str(k))
print("----------------------")


# 학습
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

knn = KNeighborsClassifier(n_neighbors=k)

# select data features
x_train = train[['3P', 'BLK']]
# select target value
y_train = train[['Pos']]

# setup knn using train data
knn.fit(x_train, y_train.values.ravel())

# select data feature to be used for prediction
x_test = test[['3P', 'BLK']]

# select target value
y_test = test[['Pos']]

# test
pred = knn.predict(x_test)

# check ground_truth with knn prediction
comparison = pd.DataFrame({'prediction': pred,
                           'ground_truth': y_test.values.ravel()})
print(comparison)  # 실제 데이터(test data)
print("----------------------")

# chek accuracy
print("accuracy: " + str(accuracy_score(y_test.values.ravel(), pred)))  # 예측값, 1.0 이상의 값이 나올 순 없음
print("----------------------")


# 데이터 시각화
# 데이터를 특징을 바탕으로 한 공간에 시각화 함으로서 머신러닝 학습에 필요한 특징과
# 불필요한
# get_ipython().run_line_magic('matplotlib', 'inline')
df = pd.read_csv("C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_stat.csv")
print(df.head())
print("----------------------")
print(df.Pos.value_counts())
print("----------------------")

# 스틸, 2점슛 데이터 시각화
sns.lmplot('STL', '2P', data=df, fit_reg=False,  # x축, y축, 데이터, 라인없음
           scatter_kws={'s': 150},  # 좌표상 점의 크기
           markers=['o', 'x'],
           hue='Pos')  # 예측값
# title
plt.title('STL and 2P in 2d plane')
plt.show()

# 어시스트, 2점슛 데이터 시각화
sns.lmplot('AST', '2P', data=df, fit_reg=False,
           scatter_kws={'s': 150},
           markers=['o', 'x'],
           hue='Pos'
           )
# title
plt.title('AST and 2P in 2d plane')
plt.show()

# 블로킹 3점슛 데이터 시각화
sns.lmplot('BLK', '3P', data=df, fit_reg=False,
           scatter_kws={'s': 150},
           markers=['o', 'x'],
           hue='Pos')
# title
plt.title('BLK and 3P in 2d plane')
plt.show()

# 리바운드, 3점슛 데이터 시각화
sns.lmplot('TRB', '3P', data=df, fit_reg=False,
           scatter_kws={'s': 150},
           markers=['o', 'x'],
           hue='Pos'
           )
# title
plt.title('TLB and 3P in 2d plane')
plt.show()


# 데이터 다듬기
# 분별력이 없는 특징(feature)을 데이터에서 제거
df.drop(['2P', 'AST', 'STL'], axis=1, inplace=True)
print(df.head())
print("----------------------")


# 데이터 나누기(학습 데이터, 테스트 데이터)
# sklearn의 train_test_split을 사용하면 라인한 줄로 손쉽게 데이터를 나눌 수 있음
from sklearn.model_selection import train_test_split

# 다듬어진 데이터에서 20%를 테스트 데이터로 분류(트레인 80%, 테스트 20%)
train, test = train_test_split(df, test_size=0.2)

# 학습 데이터의 갯수 확인(80개 데이터 존재)
print(train.shape[0])
print("----------------------")

# 테스트 데이터의 갯수 확인(20개 데이터 존재)
print(test.shape[0])


# 파일로 저장
train.to_csv("C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_train_after.csv", index=None)
test.to_csv('C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_test_after.csv', index=None)

```

```
             Player Pos   3P  TRB  BLK
0  Denzel Valentine  SG  1.3  2.6  0.1
1       Kyle Korver  SG  2.4  2.8  0.3
2      Troy Daniels  SG  2.1  1.5  0.1
3      Tim Hardaway  SG  1.9  2.8  0.2
4    Dewayne Dedmon   C  0.0  6.5  0.8
----------------------
                 Player Pos   3P  TRB  BLK
0          JaVale McGee   C  0.0  3.2  0.9
1         Manu Ginobili  SG  1.3  2.3  0.2
2          Nene Hilario   C  0.0  4.2  0.6
3         Evan Fournier  SG  1.9  3.1  0.1
4  Georgios Papagiannis   C  0.0  3.9  0.8
----------------------
[0.8875, 0.875, 0.875, 0.8625, 0.875, 0.8625, 0.8625, 0.8625, 0.8625, 0.875, 0.875, 0.875, 0.8625, 0.8625, 0.85, 0.85, 0.825, 0.8, 0.8]
----------------------
The best number of k:3
----------------------
   prediction ground_truth
0           C            C
1          SG           SG
2           C            C
3          SG           SG
4           C            C
5           C            C
6           C            C
7          SG           SG
8          SG           SG
9           C            C
10         SG           SG
11          C            C
12         SG           SG
13          C            C
14          C            C
15         SG           SG
16         SG           SG
17          C            C
18         SG           SG
19          C            C
----------------------
accuracy: 1.0
----------------------
           Player Pos   3P   2P  TRB  AST  STL  BLK
0    Alex Abrines  SG  1.4  0.6  1.3  0.6  0.5  0.1
1    Steven Adams   C  0.0  4.7  7.7  1.1  1.1  1.0
2   Alexis Ajinca   C  0.0  2.3  4.5  0.3  0.5  0.6
3  Chris Andersen   C  0.0  0.8  2.6  0.4  0.4  0.6
4     Will Barton  SG  1.5  3.5  4.3  3.4  0.8  0.5
----------------------
SG    50
C     50
Name: Pos, dtype: int64
----------------------
           Player Pos   3P  TRB  BLK
0    Alex Abrines  SG  1.4  1.3  0.1
1    Steven Adams   C  0.0  7.7  1.0
2   Alexis Ajinca   C  0.0  4.5  0.6
3  Chris Andersen   C  0.0  2.6  0.6
4     Will Barton  SG  1.5  4.3  0.5
----------------------
80
----------------------
20

```

> k-fold에 따라 accuracy 값이 다를 수 있음



```python
import pandas as pd
import pickle  # 파이썬 객체를 파일로 저장하는 패키지
import matplotlib.pyplot as plt
import seaborn as sns  # 시각화 함수 패키지
import numpy as np

# 그래프 출력을 위한 선언
# get_ipython().run_line_magic('matplotlib', 'inline')  # 주피터에서 사용하기 위한 코드, 파이참에선 plt.show()
# sklearn 모델의 동일한 결과 출력을 위해 선언
np.random.seed(5)


# 데이터 불러오기(학습 데이터, 테스트 데이터)
# 데이터 분석 단계(4.2_농구선수_데이터분석.ipynb)에서 생성한 농구 선수 포지션 예측하기의 학습 데이터 및 테스트 데이터 로드
train = pd.read_csv("C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_train.csv")
print(train.head())
print("----------------------")

test = pd.read_csv("C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_test.csv")
print(test.head())
print("----------------------")


# 최적의 K 찾기(교차검증 cross validation)
# import kNN library
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score  # 교차검증 시켜주는 패키지

# find best k, range from 3 to half of the number of data
max_k_range = train.shape[0] // 2
k_list = []
for i in range(3, max_k_range, 2):
    k_list.append(i)

cross_validation_scores = []
x_train = train[['3P', 'BLK', 'TRB']]
y_train = train[['Pos']]

# 10-fold cross validation
for k in k_list:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, x_train, y_train.values.ravel(), cv=10, scoring='accuracy')
    cross_validation_scores.append(scores.mean())

print(cross_validation_scores)
print("----------------------")

# visuallize accuracy acoodrding to k
plt.plot(k_list, cross_validation_scores)
plt.xlabel('the number of k')
plt.ylabel('Accuracy')
plt.show()

# find best k
cvs = cross_validation_scores
k = k_list[cvs.index(max(cross_validation_scores))]
print('The best number of k:' + str(k))
print("----------------------")


# 학습
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

knn = KNeighborsClassifier(n_neighbors=k)

# select data features
x_train = train[['3P', 'BLK', 'TRB']] # 리바운드를 포함시킴(3개 특징)
# select target value
y_train = train[['Pos']]

# setup knn using train data
knn.fit(x_train, y_train.values.ravel())

# select data feature to be used for prediction
x_test = test[['3P', 'BLK', 'TRB']]  # 리바운드를 포함시킴(3개 특징)

# select target value
y_test = test[['Pos']]

# test
pred = knn.predict(x_test)

# check ground_truth with knn prediction
comparison = pd.DataFrame({'prediction': pred,
                           'ground_truth': y_test.values.ravel()})
print(comparison)  # 실제 데이터(test data)
print("----------------------")

# chek accuracy
print("accuracy: " + str(accuracy_score(y_test.values.ravel(), pred)))  # 예측값, 1.0 이상의 값이 나올 순 없음
print("----------------------")


# 데이터 시각화
# 데이터를 특징을 바탕으로 한 공간에 시각화 함으로서 머신러닝 학습에 필요한 특징과
# 불필요한
# get_ipython().run_line_magic('matplotlib', 'inline')
df = pd.read_csv("C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_stat.csv")
print(df.head())
print("----------------------")
print(df.Pos.value_counts())
print("----------------------")

# 스틸, 2점슛 데이터 시각화
sns.lmplot('STL', '2P', data=df, fit_reg=False,  # x축, y축, 데이터, 라인없음
           scatter_kws={'s': 150},  # 좌표상 점의 크기
           markers=['o', 'x'],
           hue='Pos')  # 예측값
# title
plt.title('STL and 2P in 2d plane')
plt.show()

# 어시스트, 2점슛 데이터 시각화
sns.lmplot('AST', '2P', data=df, fit_reg=False,
           scatter_kws={'s': 150},
           markers=['o', 'x'],
           hue='Pos'
           )
# title
plt.title('AST and 2P in 2d plane')
plt.show()

# 블로킹 3점슛 데이터 시각화
sns.lmplot('BLK', '3P', data=df, fit_reg=False,
           scatter_kws={'s': 150},
           markers=['o', 'x'],
           hue='Pos')
# title
plt.title('BLK and 3P in 2d plane')
plt.show()

# 리바운드, 3점슛 데이터 시각화
sns.lmplot('TRB', '3P', data=df, fit_reg=False,
           scatter_kws={'s': 150},
           markers=['o', 'x'],
           hue='Pos'
           )
# title
plt.title('TLB and 3P in 2d plane')
plt.show()


# 데이터 다듬기
# 분별력이 없는 특징(feature)을 데이터에서 제거
df.drop(['2P', 'AST', 'STL'], axis=1, inplace=True)
print(df.head())
print("----------------------")


# 데이터 나누기(학습 데이터, 테스트 데이터)
# sklearn의 train_test_split을 사용하면 라인한 줄로 손쉽게 데이터를 나눌 수 있음
from sklearn.model_selection import train_test_split

# 다듬어진 데이터에서 20%를 테스트 데이터로 분류(트레인 80%, 테스트 20%)
train, test = train_test_split(df, test_size=0.2)

# 학습 데이터의 갯수 확인(80개 데이터 존재)
print(train.shape[0])
print("----------------------")

# 테스트 데이터의 갯수 확인(20개 데이터 존재)
print(test.shape[0])


# 파일로 저장
train.to_csv("C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_train_after.csv", index=None)
test.to_csv('C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_test_after.csv', index=None)
```

```
             Player Pos   3P  TRB  BLK
0  Denzel Valentine  SG  1.3  2.6  0.1
1       Kyle Korver  SG  2.4  2.8  0.3
2      Troy Daniels  SG  2.1  1.5  0.1
3      Tim Hardaway  SG  1.9  2.8  0.2
4    Dewayne Dedmon   C  0.0  6.5  0.8
----------------------
                 Player Pos   3P  TRB  BLK
0          JaVale McGee   C  0.0  3.2  0.9
1         Manu Ginobili  SG  1.3  2.3  0.2
2          Nene Hilario   C  0.0  4.2  0.6
3         Evan Fournier  SG  1.9  3.1  0.1
4  Georgios Papagiannis   C  0.0  3.9  0.8
----------------------
[0.8875, 0.875, 0.875, 0.8625, 0.875, 0.8625, 0.8625, 0.8625, 0.8625, 0.875, 0.875, 0.875, 0.8625, 0.8625, 0.85, 0.85, 0.825, 0.8, 0.8]
----------------------
The best number of k:3
----------------------
   prediction ground_truth
0           C            C
1          SG           SG
2           C            C
3          SG           SG
4           C            C
5           C            C
6           C            C
7          SG           SG
8          SG           SG
9           C            C
10         SG           SG
11         SG            C
12         SG           SG
13          C            C
14          C            C
15         SG           SG
16         SG           SG
17          C            C
18         SG           SG
19          C            C
----------------------
accuracy: 0.95
----------------------
           Player Pos   3P   2P  TRB  AST  STL  BLK
0    Alex Abrines  SG  1.4  0.6  1.3  0.6  0.5  0.1
1    Steven Adams   C  0.0  4.7  7.7  1.1  1.1  1.0
2   Alexis Ajinca   C  0.0  2.3  4.5  0.3  0.5  0.6
3  Chris Andersen   C  0.0  0.8  2.6  0.4  0.4  0.6
4     Will Barton  SG  1.5  3.5  4.3  3.4  0.8  0.5
----------------------
C     50
SG    50
Name: Pos, dtype: int64
----------------------
           Player Pos   3P  TRB  BLK
0    Alex Abrines  SG  1.4  1.3  0.1
1    Steven Adams   C  0.0  7.7  1.0
2   Alexis Ajinca   C  0.0  4.5  0.6
3  Chris Andersen   C  0.0  2.6  0.6
4     Will Barton  SG  1.5  4.3  0.5
----------------------
80
----------------------
20

Process finished with exit code 0
```

> 3개 feature로 데이터를 설정 및 학습



- iris 데이터셋 이용

```python
from sklearn import datasets, cluster, metrics
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import pandas as pd


from sklearn.datasets import load_iris
iris = datasets.load_iris()

# 입력과 출력 설정
X = iris.data
y = iris.target

# 전체 데이터를 학습 데이터와 테스트 데이터 비율 80:20으로 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
print(X_train.shape)
print('--------------')

from sklearn.neighbors import KNeighborsClassifier  # kNN알고리즘으로 분류
from sklearn import metrics

# 학습단계
knn = KNeighborsClassifier(n_neighbors=5)  # 숫자를 바꿔가며 실행 가능
knn.fit(X, y)

# 테스트 단계
y_pred = knn.predict(X_test)

# 정확도 점수 출력
scores = metrics.accuracy_score(y_test, y_pred)
print(scores)
```

```
(120, 4)
--------------
0.9666666666666667
```



- SVM (support vector machine)

1. 분류 알고리즘 중 한 가지

2. SVM의 파라미터 

   - C (cost): 비용, 결정경계선의 마진을 결정하는 파라미터
   - gamma: 커널의 데이터포인트의 표준편차를 결정하는 파라미터

   > C가 클 수록 결정경계선과 서포트 벡터의 간격(마진)이 작아짐
   >
   > gamma가 클 수록 결정경계선이 데이터포인트와 더욱 가까워짐

```python
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
from sklearn.svm import SVC
import numpy as np

# sklearn 모델의 동일한 결과 출력을 위해 선언
np.random.seed(5)

train = pd.read_csv("C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_train.csv")
test = pd.read_csv("C:\\Users\\BIT\\Desktop\\KHY\\6_Math\\0729\\basketball_test.csv")

# sklearn에서 제공하는 gridsearch를 사용하면 손쉽게 최적의 C, gamma를 구할 수 있음
def svc_param_selection(X, y, nfolds):
    svm_parameters = [{'kernel': ['rbf'],  # 2차원을 3차원으로 변형시켜주는 알고리즘
                       'gamma': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1],
                       'C': [0.01, 0.1, 1, 10, 100, 1000]}]
    clf = GridSearchCV(SVC(), svm_parameters, cv=10)  # 분류선 생성
    clf.fit(X_train, y_train.values.ravel())
    print(clf.best_params_)
    return clf

X_train = train[['3P', 'BLK']]
y_train = train[['Pos']]
# 최적의 파라미터를 sklearn의 gridsearch를 통해 구함
clf = svc_param_selection(X_train, y_train.values.ravel(), 10)


# 시각화
# 최적의 파라미터일 때의 결정경계선과 다른 파라미터들일 때의 결정 경계선을 비교

# 시각화를 하기 위해 최적의 C 후보 지정
C_canditates = []
C_canditates.append(clf.best_params_['C'] * 0.01)
C_canditates.append(clf.best_params_['C'])
C_canditates.append(clf.best_params_['C'] * 100)

# 시각화를 위해 최적의 gamma 후보 지정
gamma_canditates = []
gamma_canditates.append(clf.best_params_['gamma'] * 0.01)
gamma_canditates.append(clf.best_params_['gamma'])
gamma_canditates.append(clf.best_params_['gamma'] * 100)

X = train[['3P', 'BLK']]
Y = train['Pos'].tolist()

# 포지션에 해당하는 문자열 SG와 C를 벡터화
position = []
for gt in Y:
    if gt == 'C':
        position.append(0)
    else:  # SG일 때
        position.append(1)

# 각각의 파라미터에 해당하는 SVM 모델을 만들어 classfires에 저장
classifiers = []
for C in C_canditates:
    for gamma in gamma_canditates:
        clf = SVC(C=C, gamma=gamma)
        clf.fit(X, Y)
        classifiers.append((C, gamma, clf))

# 18, 18 사이즈의 챠트 구성
plt.figure(figsize=(18, 18))
xx, yy = np.meshgrid(np.linspace(0, 4, 100), np.linspace(0, 4, 100))

# 각각의 모델들에 대한 결정경계 함수를 적용하여 시각화
for (k, (C, gamma, clf)) in enumerate(classifiers):
    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # 최적의 모델을 포함한 다른 파라미터로 학습된 모델들을 함께 시각화
    plt.subplot(len(C_canditates), len(gamma_canditates), k+1)  # 챠트를 구성하는 부분
    plt.title("gamma=10^%d, C=10^%d" % (np.log10(gamma), np.log10(C)), size='medium')
    # ^: 캐럿, 제곱

    # 서포트 벡터와 결정경계선을 시각화
    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
    plt.scatter(X['3P'], X['BLK'], c=position, cmap=plt.cm.RdBu_r, edgecolors='k')

plt.show()  # 기본적으로 MatplotlibDeprecationWarning 에러가 뜸(정확한 이유는 모름)


# 테스트
# sklearn의 gridsearch로 얻어진 최적의 파라미터로 학습된 clf를 이용하여 테스트 진행

# 테스트에 사용될 특징 지정
X_test = test[['3P', 'BLK']]

# 특징으로 예측할 값(농구선수 포지션)을 지정
y_test = test[['Pos']]

# 최적의 파라미터로 완성된 SVM에 테스트 데이터를 주입하여 실제값과 예측값 얻음
y_true, y_pred = y_test, clf.predict(X_test)

print('-----------------------------')
print(classification_report(y_true, y_pred))
print('-----------------------------')
print("accuracy: " + str(accuracy_score(y_true, y_pred)))

# 실제값(ground_truth)과 예측값(prediction)이 어느 정도 일치하는 눈으로 직접 비교
comparison = pd.DataFrame({'prediction': y_pred,
                           'ground_truth': y_true.values.ravel()})
print('-----------------------------')
print(comparison)
```

```
{'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}
D:/KHY/pycharm_workspace/exam01/0729.py:81: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
D:/KHY/pycharm_workspace/exam01/0729.py:81: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
D:/KHY/pycharm_workspace/exam01/0729.py:81: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
D:/KHY/pycharm_workspace/exam01/0729.py:81: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
D:/KHY/pycharm_workspace/exam01/0729.py:81: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
D:/KHY/pycharm_workspace/exam01/0729.py:81: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
D:/KHY/pycharm_workspace/exam01/0729.py:81: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
D:/KHY/pycharm_workspace/exam01/0729.py:81: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
D:/KHY/pycharm_workspace/exam01/0729.py:81: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.
  plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
-----------------------------
              precision    recall  f1-score   support

           C       1.00      0.91      0.95        11
          SG       0.90      1.00      0.95         9

    accuracy                           0.95        20
   macro avg       0.95      0.95      0.95        20
weighted avg       0.96      0.95      0.95        20

-----------------------------
accuracy: 0.95
-----------------------------
   prediction ground_truth
0           C            C
1          SG           SG
2           C            C
3          SG           SG
4           C            C
5           C            C
6           C            C
7          SG           SG
8          SG           SG
9           C            C
10         SG           SG
11         SG            C
12         SG           SG
13          C            C
14          C            C
15         SG           SG
16         SG           SG
17          C            C
18         SG           SG
19          C            C

Process finished with exit code 0
```

