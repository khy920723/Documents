# 딥러닝

## 딥러닝 기초/03_신경망_학습

1. 신경망의 목적은 오차를 최소화 할 수 있는 가중치화 편향을 구하는 것

> 예측값(y)에 의해 오차량(가중치, 편향)이 바뀜
>
> y = w1x1 + w2x2 + b





### 경사법 원리

```python
import numpy as np
# 미분 코드
def _numerical_gradient_no_batch(f, x):
    h = 1e-4 # 0.0001
    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성
    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h) 계산
        x[idx] = float(tmp_val) + h
        fxh1 = f(x)
        # f(x-h) 계산
        x[idx] = tmp_val - h 
        fxh2 = f(x) 
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val # 값 복원
    return grad
def numerical_gradient(f, X):
    if X.ndim == 1:
        return _numerical_gradient_no_batch(f, X)
    else:
        grad = np.zeros_like(X)
        for idx, x in enumerate(X):
            grad[idx] = _numerical_gradient_no_batch(f, x)
        return grad

"""
f: 비용 함수 또는 손실 함수(cost function, loss function)
init_x: 초깃값
lr: 학습률
step_num: 갱신할 횟수
"""
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    
    # step_num 만큼 반복 학습
    for i in range(step_num):
        grad = numerical_gradient(f, x) # x를 미분하고
        x -= lr * grad # 학습률을 곱해서 x를 update
    return x
```

```python
def function_2(x):
  return x[0] ** 2 + x[1] ** 2

# x0: -3.0, x1 = 4.0일 때
init_x = np.array([-3.0, 4.0])
gradient_descent(function_2, init_x, lr=0.1, step_num=100)
```

```
array([-6.11110793e-10,  8.14814391e-10])
```



```python
# coding: utf-8
import numpy as np
import matplotlib.pylab as plt


def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x
    x_history = []

    for i in range(step_num):
        x_history.append( x.copy() )

        grad = numerical_gradient(f, x)
        x -= lr * grad

    return x, np.array(x_history)

init_x = np.array([-3.0, 4.0])    

lr = 0.1
step_num = 20
x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)

plt.plot( [-5, 5], [0,0], '--b')
plt.plot( [0,0], [-5, 5], '--b')
plt.plot(x_history[:,0], x_history[:,1], 'o')

plt.xlim(-3.5, 3.5)
plt.ylim(-4.5, 4.5)
plt.xlabel("X0")
plt.ylabel("X1")
plt.show()
```

```

```



```python
# 학습률이 너무 큰 예. lr = 10.0
init_x = np.array([-3.0, 4.0])
result, _ = gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)
print("Learning Rate 10.0 : {}".format(result))


# 학습률이 너무 작은 예. lr = 1e-10
init_x = np.array([-3.0, 4.0])
result, _ = gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)
print("Learning Rate 1e-10 : {}".format(result))
```

```
Learning Rate 10.0 : [-2.58983747e+13 -1.29524862e+12]
Learning Rate 1e-10 : [-2.99999994  3.99999992]
```





## 강사님 딥러닝 예시 코드

> 코랩에서 새폴더 생성 후 common.zip 파일을 넣어줌

```python
%cd common
!unzip common.zip
```

```
/content/common
Archive:  common.zip
  inflating: functions.py            
  inflating: gradient.py             
  inflating: layers.py               
  inflating: multi_layer_net.py      
  inflating: multi_layer_net_extend.py  
  inflating: optimizer.py            
  inflating: trainer.py              
  inflating: util.py                 
 extracting: __init__.py             
```



```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.functions import softmax, cross_entropy_error
from common.gradient import numerical_gradient

# 신경망 클래스
class SimpleNet:
  def __init__(self):
    self.W = np.random.randn(2, 3) # 2 x 3 형태로 정규분포로 이루어진 가중치의 초깃값 생성

  def predict(self, x):
    return np.dot(x, self.W)

  def loss(self, x, t):
    z = self.predict(x)
    y = softmax(z)
    loss = cross_entropy_error(y, t)
    return loss
```

```python
net = SimpleNet()
print(net.W)

x = np.array([0.6, 0.9])
p = net.predict(x)
print(p)
print(np.argmax(p))

t = np.array([0, 0, 1]) # 잘못된 값을 넣게 되면 loss가 커짐
print(net.loss(x, t))
```

```
[[-0.16465003  1.37271586  0.81078875]
 [-0.59474692 -0.74124992  2.08852494]]
[-0.63406224  0.15650459  2.36614569]
2
0.14800320227507802
```



```python
def f(w):
  return net.loss(x, t)

dw = numerical_gradient(f, net.W)
print(dw)  
```

```
[[ 0.02575731  0.05678573 -0.08254304]
 [ 0.03863596  0.0851786  -0.12381456]]
```





## 신경망 만들어보기

```python
# MNIST 데이터셋 분류 신경망 만들기
sys.path.append(os.pardir)
from common.functions import *
from common.gradient import numerical_gradient

class TwoLayerNet:

  # 신경망 초기화
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
      # 사용할 매개변수를 준비(매개변수에 대한 초기 설정)
      # 가중치는 정규분포로 이루어진 랜덤값을 사용
      # 편향은 0으로 초기화 하는 것이 일반적

        # 가중치를 담아놓을 딕셔너리
        self.params = {}

        # 1층(은닉층)의 매개변수 설정
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) # 784개의 입력을 받는 100개의 뉴런 생성
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        
        # 2층(출력층)의 매개변수 설정
        self.params['b1'] = np.zeros(hidden_size)
        self.params['b2'] = np.zeros(output_size)
    
    # 예측(소프트맥스를 사용할 예정)
    def predict(self, x):
      # 가중치, 편향 불러오기
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        # 1-1 1층의 내적 구하기
        a1 = np.dot(x, W1) + b1
        # 1-2 내적에 대한 활성화 함수 적용
        z1 = sigmoid(a1)
        
        # 1-1
        a2 = np.dot(z1, W2) + b2
        # 2-2 2층의 출력함수 적용하기
        y  = softmax(a2)
        
        return y
    
    # 비용함수
    # x : 입력 데이터, t : 정답 레이블
    def loss(self, x, t):
      # 예측
        y = self.predict(x)
        # 예측한 결과물, 정답에 대한 loss를 구하면 됨
        return cross_entropy_error(y, t)
    
    # 정확도 확인하기
    def accuracy(self, x, t):
        y = self.predict(x) # 소프트 맥스의 결과
        y = np.argmax(y, axis=1) # axis=1을 준 이유: 각각의 행에서 인덱스를 추출하기 위함
        t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
    
    # TwolayerNet의 미분 수행 함수
    # x : 입력 데이터, t : 정답 레이블
    def numerical_gradient(self, x, t):
        loss_W = lambda W : self.loss(x, t)
        
        # 각 층에서 구해지는 기울기를 저장할 딕셔너리
        grads = {}
        # 1창의 가중치들의 기울기를 구함(loss에 대한 w1의 기울기)
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        
        # 2창의 가중치들의 기울기를 구함
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads


# 신경망 생성하기
# MNIST 처리를 위한 TwoLayerNet 객체 생성
net = TwoLayerNet(input_size=28*28, hidden_size=100, output_size=10) # 이미지 데이터의 픽셀, 은닉층 개수 100, 출력층 개수 10
print(net.params['W1'].shape)
print(net.params['b1'].shape)
print(net.params['W2'].shape)
print(net.params['b2'].shape)

# 100개의 데이터를 임의로 준비
# 예측 처리 예시
x = np.random.rand(100, 784) # 784개의 feature를 가진 100개의 데이터를 랜덤으로 생성 (100장의 미니 배치 )
y = net.predict(x) # 
print(x.shape, y.shape)

# grad 변수 확인 예시
x = np.random.rand(100, 784) # 더미 입력 데이터 (100장)
t = np.random.rand(100, 10) # 더미 정답 레이블 생성하기 (100장에 대한 정답 레이블 )

grads = net.numerical_gradient(x, t) # 기울기 계산

print(grads['W1'].shape)
print(grads['b1'].shape)
print(grads['W2'].shape)
print(grads['b2'].shape)
```

```
(784, 100)
(100,)
(100, 10)
(10,)
(100, 784) (100, 10)
```

